{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MaskRCNN.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1QAad03Vhd3xys9z_WCrzuVbJUL6U47Wn","authorship_tag":"ABX9TyNKYrLhikvd4o52UHmuyasm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SStS257HZBwJ"},"source":["출처: https://dacon.io/codeshare/3725?dtype=recent"]},{"cell_type":"code","metadata":{"id":"zcQm2fkiZGNw","executionInfo":{"status":"ok","timestamp":1638981209349,"user_tz":-540,"elapsed":514,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":["import os\n","import glob\n","import json\n","from glob import glob\n","from tqdm import tqdm\n","import time\n","import datetime\n","import math\n","\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from pycocotools.coco import COCO\n","\n","import torch\n","import torch.optim as optim\n","from torch  import nn, Tensor\n","\n","import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","from collections import defaultdict, deque\n","\n","import base64\n","import torch.distributed as dist\n","from torch.utils.data import Dataset"],"execution_count":132,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aa_53qNpSC29","executionInfo":{"status":"ok","timestamp":1638981211042,"user_tz":-540,"elapsed":1263,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"20fb5091-3364-4dcd-c22f-5075b59f6b4c"},"source":["!ls"],"execution_count":133,"outputs":[{"output_type":"stream","name":"stdout","text":["coco_eval.py   drive\t  __pycache__  transforms.py  vision\n","coco_utils.py  engine.py  sample_data  utils.py\n"]}]},{"cell_type":"markdown","metadata":{"id":"UPMPCDkZhuZ3"},"source":["## 버전 확인"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-xdZkYLLhvnN","executionInfo":{"status":"ok","timestamp":1638981211043,"user_tz":-540,"elapsed":183,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"1275f979-f835-4ba4-8c22-941a4c0761cd"},"source":["print('torch :', torch.__version__)\n","print('torchvision : ', torchvision.__version__)\n","print('cv2 :', cv2.__version__)"],"execution_count":134,"outputs":[{"output_type":"stream","name":"stdout","text":["torch : 1.10.0+cu111\n","torchvision :  0.11.1+cu111\n","cv2 : 4.1.2\n"]}]},{"cell_type":"markdown","metadata":{"id":"EvwpoiUJdKhA"},"source":["## 전체 데이터 json 만들기"]},{"cell_type":"code","metadata":{"id":"rIBJ0FI2UIvj","executionInfo":{"status":"ok","timestamp":1638981211045,"user_tz":-540,"elapsed":174,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":["# # # 이건 저의 개인적인 코랩이기 때문에 다르신 분들은 다시 변경하시기 바랍니다.\n","# # ## train_PATH와 test_PATH만 변경하면 됩니다.\n","# # # json 파일로 만들어서 분석이 더 용이하다고 생각되어 만든 것이니 더 좋은 의견 있으면 말씀해주시기 바랍니다.\n","\n","# ## Train\n","# train_PATH = '/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Data/train/annotation'\n","# file_list = os.listdir(train_PATH)\n","\n","# train_files = []\n","# for file in tqdm(file_list):\n","#     dir = train_PATH + '/' + file\n","#     json_list = glob(dir + '/' +'*.json')\n","#     train_files.append(json_list)\n","\n","# train_json_list = []\n","# for files in tqdm(train_files):\n","#     for json_file in tqdm(files):\n","#         train_json_list.append(json_file)\n","\n","# ## Test\n","# test_PATH = '/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Data/test/annotations'\n","# file_list = os.listdir(test_PATH)\n","\n","# test_files = []\n","# for file in tqdm(file_list):\n","#     dir = test_PATH + '/' + file\n","#     json_list = glob(dir + '/' + '*.json')\n","#     test_files.append(json_list)\n","\n","# test_json_list = []\n","# for files in tqdm(test_files):\n","#     for json_file in tqdm(files):\n","#         test_json_list.append(json_file)"],"execution_count":135,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DU7KIN18dNHy"},"source":["## Sample data에서 json 만들기"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZYC1fY7fdCwt","executionInfo":{"status":"ok","timestamp":1638981211052,"user_tz":-540,"elapsed":178,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"7c33f656-b3bb-418c-f509-d34d3d331be3"},"source":["## Train\n","train_PATH = '/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Sample_data/train/annotation'\n","file_list = os.listdir(train_PATH)\n","\n","train_files = []\n","for file in tqdm(file_list):\n","    dir = train_PATH + '/' + file\n","    json_list = glob(dir + '/' +'*.json')\n","    train_files.append(json_list)\n","\n","train_json_list = []\n","for files in tqdm(train_files):\n","    for json_file in tqdm(files):\n","        train_json_list.append(json_file)\n","\n","\n","test_PATH = '/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Sample_data/test/annotations'\n","file_list = os.listdir(test_PATH)\n","test_files = []\n","for file in tqdm(file_list):\n","    dir = test_PATH + '/' + file\n","    json_list = glob(dir + '/' +'*.json')\n","    test_files.append(json_list)\n","\n","test_json_list = []\n","for files in tqdm(test_files):\n","    for json_file in tqdm(files):\n","        test_json_list.append(json_file)"],"execution_count":136,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:00<00:00, 759.36it/s]\n","  0%|          | 0/4 [00:00<?, ?it/s]\n","100%|██████████| 56/56 [00:00<00:00, 335065.65it/s]\n","\n","100%|██████████| 56/56 [00:00<00:00, 483294.29it/s]\n","\n","100%|██████████| 56/56 [00:00<00:00, 392121.91it/s]\n","\n","100%|██████████| 56/56 [00:00<00:00, 383166.43it/s]\n","100%|██████████| 4/4 [00:00<00:00, 171.32it/s]\n","100%|██████████| 4/4 [00:00<00:00, 588.55it/s]\n","  0%|          | 0/4 [00:00<?, ?it/s]\n","100%|██████████| 14/14 [00:00<00:00, 117675.86it/s]\n","\n","100%|██████████| 14/14 [00:00<00:00, 175284.35it/s]\n","\n","100%|██████████| 14/14 [00:00<00:00, 41469.11it/s]\n","\n","100%|██████████| 14/14 [00:00<00:00, 27223.11it/s]\n","100%|██████████| 4/4 [00:00<00:00, 168.51it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"AD3TaUlo1gZY","executionInfo":{"status":"ok","timestamp":1638981211053,"user_tz":-540,"elapsed":46,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":["import os\n","import numpy as np\n","import torch\n","from PIL import Image\n","\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, transforms = None, mode = 'train', json_file = None):\n","        self.mode = mode\n","        self.root = root\n","        self.transforms = transforms\n","        self.json_file = json_file\n","\n","    def __getitem__(self, idx):\n","        if self.mode == 'train':\n","\n","            annot = json_file[idx]\n","            # Train의 경로로 이동한다.\n","            # 나중에 진짜 데이터를 다룰 때 사용한다.\n","            # PATH = root + '/' + 'train'\n","\n","            with open(annot, 'r') as f:\n","                annot = json.loads(f.read())\n","                \n","            image_name = annot['images'][0]['file_name']\n","            dir = image_name.split('_')[0]\n","\n","            image_PATH = self.root + '/' + 'train' + '/' +'image' +'/' + dir + '/' + image_name\n","            \n","            image = Image.open(image_PATH).convert('RGB')\n","\n","            boxes = []\n","            segmentations = []\n","            labels = []\n","\n","            for i in range(len(annot['annotations'])):\n","\n","                segmentation = annot['annotations'][i]['segmentation'][0]\n","                bbox = annot['annotations'][i]['bbox']\n","                label = torch.tensor(annot['annotations'][i]['category_id'], dtype = np.uint8)\n","                xmin, ymin, width, height = bbox[0],bbox[1],bbox[2],bbox[3]\n","                xmin, ymin, xmax, ymax = xmin, ymin, xmin + width, ymax + height\n","                \n","                boxes.append(torch.as_tensor([xmin, ymin, xmax, ymax], dtype = torch.float32))\n","                segmentations.append([segmentation])\n","                labels.append(label)\n","\n","            target = {}\n","            target['boxes'] = boxes\n","            target['labels'] = labels\n","            target['segmentation'] = segmentations\n","            target['image_id'] = torch.tensor(idx)\n","\n","            if self.transforms is not None:\n","                img, target = self.transforms(img, target)\n","\n","            return img, target\n","\n","        if self.mode == 'test':\n","            # Test의 경로로 이동한다.\n","            # PATH = root + '/' + 'test'\n","            # 나중에 진짜 데이터를 다룰 때 사용한다.\n","            annot = json_file[idx]\n","\n","            ##### 이 부분은 추후에 작성하기로 하자.\n","            with open(annot, 'r') as f:\n","                annot = json.loads(f.read())\n","\n","            image_name = annot['images'][0]['file_name']\n","            dir = image_name[:2]\n","\n","            image_PATH = PATH + '/' + dir + '/' + image_name\n","            image = Image.open(image_PATH).convert('RGB')\n","            target = {}\n","\n","            target['image_id'] = image_name\n","\n","            return image, target\n","\n","    def __len__(self):\n","        return len(self.json_file)"],"execution_count":137,"outputs":[]},{"cell_type":"code","metadata":{"id":"ypiLsatqlf2G","executionInfo":{"status":"ok","timestamp":1638981211066,"user_tz":-540,"elapsed":50,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":["root = '/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Sample_data'\n","train_dataset = CustomDataset(root = root,json_file = train_json_list, mode = 'train')\n","test_dataset = CustomDataset(root = root, json_file = test_json_list, mode = 'test')"],"execution_count":138,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uxxSEsZVTOeu"},"source":["## Pytorch Tutorial을 가지고 분석을 진행해 보기\n","\n","https://tutorials.pytorch.kr/intermediate/torchvision_tutorial.html\n","\n","PET를 먼저 진행해 보고 추후에 나머지도 진행해 볼 예정입니다."]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Sample_data/train/annotation/PP/PP_050_1520.json', 'r') as f:\n","    data = json.loads(f.read())\n","\n","print(data['images'][0]['file_name'].split('_')[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lundwtUhWjj7","executionInfo":{"status":"ok","timestamp":1638981211067,"user_tz":-540,"elapsed":50,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"d26e955f-7a84-4ccd-cf3a-dfd8786cba74"},"execution_count":139,"outputs":[{"output_type":"stream","name":"stdout","text":["050\n"]}]},{"cell_type":"code","metadata":{"id":"w4TgIuqmTNmX","executionInfo":{"status":"ok","timestamp":1638981211468,"user_tz":-540,"elapsed":446,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":["import torch\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, transforms):\n","        self.root = root\n","        self.transforms = transforms\n","        self.categories = {1 : 'pet', 2 : 'ps', 3 : 'pp', 4 : 'pe'}\n","\n","        mode = self.root.split('/')[-1]\n","        self.mode = mode\n","\n","        self.imgs = list(sorted(os.listdir(os.path.join(root, 'image', 'PET'))))\n","        if self.mode == 'train':\n","            self.annot = list(sorted(os.listdir(os.path.join(root, 'annotation', 'PET'))))\n","        else:\n","            self.annot = list(sorted(os.listdir(os.path.join(root, 'annotations', 'PET'))))\n","\n","    def __getitem__(self, idx):\n","        if self.mode == 'train':\n","            img_path = os.path.join(self.root, 'image','PET', self.imgs[idx])\n","\n","            annot_path = os.path.join(self.root, 'annotation','PET',self.annot[idx])\n","\n","            with open(annot_path, 'r') as f:\n","                annot = json.loads(f.read())\n","            \n","            # 이 부분은 그냥 숫자를 idx를 넣는 부분과 숫자를 넣는 부분이 있고 저는 아무것도 안할 것 이기 때문에 데이터에 그냥 뽑아온 값입니다~\n","            image_id = int(annot_path.split('/')[-1].split('_')[1])\n","\n","            img = Image.open(img_path).convert('RGB')\n","\n","            # width, height로 0인 이미지 생성하여 segmentation 넣어주기\n","            width = annot['images'][0]['width']\n","            height = annot['images'][0]['height']\n","\n","            #각 마스크를 target으로 사용해야함\n","            annot = annot['annotations']\n","\n","            masked_image = np.zeros((width, height), dtype = np.uint8)\n","            num_objs = len(annot)\n","\n","            mask = []\n","            boxes = []\n","            labels = []\n","            masked_image = np.zeros((width, height), dtype = np.uint8)\n","\n","            for i in range(len(annot)):\n","                segmentation = annot[i]['segmentation'][0]\n","                xmin, ymin, width, height = annot[i]['bbox'][0], annot[i]['bbox'][1], annot[i]['bbox'][2], annot[i]['bbox'][3]\n","                xmax = xmin + width\n","                ymax = ymin + height\n","\n","                label = annot[i]['category_id']\n","\n","                all_points_x = []\n","                all_points_y = []\n","\n","                for i in range(len(segmentation)):\n","                    if i%2 == 0:\n","                        all_points_x.append(segmentation[i])\n","                    else:\n","                        all_points_y.append(segmentation[i])\n","\n","                polygon_xy = np.array([(x,y) for (x,y) in zip(all_points_x, all_points_y)])\n","\n","                cv2.fillPoly(masked_image, np.uint([polygon_xy]), i)\n","\n","                mask.append(masked_image)\n","                boxes.append([xmin, ymin, xmax, ymax])\n","                labels.append(label)\n","\n","            mask = np.array(mask)\n","\n","            boxes = torch.as_tensor(boxes, dtype = torch.float32)\n","            labels = torch.ones((num_objs,), dtype = torch.int64)\n","            mask = torch.as_tensor(mask, dtype = torch.uint8)\n","            image_id = torch.tensor([image_id])\n","\n","            target = {}\n","            target['boxes'] = boxes\n","            target['labels'] = labels\n","            target['masks'] = mask\n","            target['image_id'] = image_id\n","\n","            if self.transforms is not None:\n","                img, target = self.transforms(img, target)\n","\n","            return img, target\n","            \n","        if self.mode == 'test':\n","            img_path = os.path.join(self.root, 'image','PET', self.imgs[idx])\n","\n","            annot_path = os.path.join(self.root, 'annotations','PET',self.annot[idx])\n","\n","            with open(annot_path, 'r') as f:\n","                annot = json.loads(f.read())\n","            \n","            # 이 부분은 그냥 숫자를 idx를 넣는 부분과 숫자를 넣는 부분이 있고 저는 아무것도 안할 것 이기 때문에 데이터에 그냥 뽑아온 값입니다~\n","            image_id = int(annot_path.split('/')[-1].split('_')[1])\n","\n","            img = Image.open(img_path).convert('RGB')\n","\n","            # width, height로 0인 이미지 생성하여 segmentation 넣어주기\n","            width = annot['images'][0]['width']\n","            height = annot['images'][0]['height']\n","\n","            #각 마스크를 target으로 사용해야함\n","            annot = annot['annotations']\n","\n","            masked_image = np.zeros((width, height), dtype = np.uint8)\n","            num_objs = len(annot)\n","\n","            mask = []\n","            boxes = []\n","            labels = []\n","            area = []\n","            masked_image = np.zeros((width, height), dtype = np.uint8)\n","\n","            for i in range(len(annot)):\n","                segmentation = annot[i]['segmentation'][0]\n","                xmin, ymin, width, height = annot[i]['bbox'][0], annot[i]['bbox'][1], annot[i]['bbox'][2], annot[i]['bbox'][3]\n","                xmax = xmin + width\n","                ymax = ymin + height\n","\n","                label = annot[i]['category_id']\n","\n","                all_points_x = []\n","                all_points_y = []\n","\n","                for i in range(len(segmentation)):\n","                    if i%2 == 0:\n","                        all_points_x.append(segmentation[i])\n","                    else:\n","                        all_points_y.append(segmentation[i])\n","\n","                polygon_xy = np.array([(x,y) for (x,y) in zip(all_points_x, all_points_y)])\n","\n","                cv2.fillPoly(masked_image, np.uint([polygon_xy]), i)\n","\n","                mask.append(masked_image)\n","                boxes.append([xmin, ymin, xmax, ymax])\n","                labels.append(label)\n","                area.append((xmax - xmin) * (ymax - xmin))\n","\n","            mask = np.array(mask)\n","\n","            boxes = torch.as_tensor(boxes, dtype = torch.float32)\n","            labels = torch.ones((num_objs,), dtype = torch.int64)\n","            mask = torch.as_tensor(mask, dtype = torch.uint8)\n","            image_id = torch.tensor([image_id])\n","\n","            target = {}\n","            target['boxes'] = boxes\n","            target['labels'] = labels\n","            target['masks'] = mask\n","            target['image_id'] = image_id\n","            target['area'] = area\n","\n","            if self.transforms is not None:\n","                img, target = self.transforms(img, target)\n","\n","            return img, target\n","\n","            if self.transforms is not None:\n","                img, target = self.transforms(img, target)\n","\n","            return img, target\n","\n","    def __len__(self):\n","        return len(self.imgs)"],"execution_count":140,"outputs":[]},{"cell_type":"code","metadata":{"id":"DvoXdQtCCEzH","executionInfo":{"status":"ok","timestamp":1638981211470,"user_tz":-540,"elapsed":17,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True)\n","\n","num_classes = 1 + 1 # 원래 클래스 + 1\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"],"execution_count":141,"outputs":[]},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","# 분류 목적으로 미리 학습된 모델을 로드하고 특징들만을 리턴하도록 합니다\n","backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n","# Faster RCNN은 백본의 출력 채널 수를 알아야 합니다.\n","# mobilenetV2의 경우 1280이므로 여기에 추가해야 합니다.\n","backbone.out_channels = 1280\n","\n","# RPN(Region Proposal Network)이 5개의 서로 다른 크기와 3개의 다른 측면 비율(Aspect ratio)을 가진\n","# 5 x 3개의 앵커를 공간 위치마다 생성하도록 합니다.\n","# 각 특징 맵이 잠재적으로 다른 사이즈와 측면 비율을 가질 수 있기 때문에 Tuple[Tuple[int]] 타입을 가지도록 합니다.\n","\n","anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n","                                   aspect_ratios=((0.5, 1.0, 2.0),))\n","\n","# 관심 영역의 자르기 및 재할당 후 자르기 크기를 수행하는 데 사용할 피쳐 맵을 정의합니다.\n","# 만약 백본이 텐서를 리턴할때, featmap_names 는 [0] 이 될 것이라고 예상합니다.\n","# 일반적으로 백본은 OrderedDict[Tensor] 타입을 리턴해야 합니다.\n","# 그리고 특징맵에서 사용할 featmap_names 값을 정할 수 있습니다.\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n","                                                output_size=7,\n","                                                sampling_ratio=2)\n","\n","# 조각들을 Faster RCNN 모델로 합칩니다.\n","model = FasterRCNN(backbone,\n","                   num_classes=2,\n","                   rpn_anchor_generator=anchor_generator,\n","                   box_roi_pool=roi_pooler)"],"metadata":{"id":"-QDlR2nwSs48","executionInfo":{"status":"ok","timestamp":1638981212331,"user_tz":-540,"elapsed":877,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"execution_count":142,"outputs":[]},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","\n","\n","def get_model_instance_segmentation(num_classes):\n","    # COCO 에서 미리 학습된 인스턴스 분할 모델을 읽어옵니다\n","    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n","\n","    # 분류를 위한 입력 특징 차원을 얻습니다\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    # 미리 학습된 헤더를 새로운 것으로 바꿉니다\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    # 마스크 분류기를 위한 입력 특징들의 차원을 얻습니다\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","    # 마스크 예측기를 새로운 것으로 바꿉니다\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n","                                                       hidden_layer,\n","                                                       num_classes)\n","\n","    return model"],"metadata":{"id":"DkqNzf-ESzNp","executionInfo":{"status":"ok","timestamp":1638981212333,"user_tz":-540,"elapsed":31,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"execution_count":143,"outputs":[]},{"cell_type":"code","source":["%%shell\n","\n","# Download TorchVision repo to use some files from\n","# references/detection\n","git clone https://github.com/pytorch/vision.git\n","cd vision\n","git checkout v0.8.2\n","\n","cp references/detection/utils.py ../\n","cp references/detection/transforms.py ../\n","cp references/detection/coco_eval.py ../\n","cp references/detection/engine.py ../\n","cp references/detection/coco_utils.py ../"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5krPiMPeTbMi","executionInfo":{"status":"ok","timestamp":1638981212334,"user_tz":-540,"elapsed":31,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"fd06035e-da67-43f6-a95d-a275f77c1bd2"},"execution_count":144,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'vision' already exists and is not an empty directory.\n","HEAD is now at 2f40a483 [v0.8.X] .circleci: Add Python 3.9 to CI (#3063)\n"]},{"output_type":"execute_result","data":{"text/plain":[""]},"metadata":{},"execution_count":144}]},{"cell_type":"code","source":["from engine import train_one_epoch, evaluate\n","import utils\n","import transforms as T\n","\n","def get_transform(train):\n","    transforms = []\n","    transforms.append(T.ToTensor())\n","    if train:\n","        # (역자주: 학습시 50% 확률로 학습 영상을 좌우 반전 변환합니다)\n","        transforms.append(T.RandomHorizontalFlip(0.5))\n","    return T.Compose(transforms)"],"metadata":{"id":"7AKNiQ2xS1mj","executionInfo":{"status":"ok","timestamp":1638981212336,"user_tz":-540,"elapsed":20,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"execution_count":145,"outputs":[]},{"cell_type":"code","source":["model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","dataset = CustomDataset('/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Sample_data/train', get_transform(train=True))\n","data_loader = torch.utils.data.DataLoader(\n","    dataset, batch_size=2, shuffle=True, num_workers=4,\n","    collate_fn=utils.collate_fn\n",")\n","# For Training\n","images,targets = next(iter(data_loader))\n","images = list(image for image in images)\n","targets = [{k: v for k, v in t.items()} for t in targets]\n","\n","\n","output = model(images,targets)   # Returns losses and detections\n","# For inference\n","model.eval()\n","x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n","predictions = model(x)           # Returns predictions"],"metadata":{"id":"1muiZL-cS3HO","executionInfo":{"status":"ok","timestamp":1638981232944,"user_tz":-540,"elapsed":20627,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"execution_count":146,"outputs":[]},{"cell_type":"code","source":["# use our dataset and defined transformations\n","dataset = CustomDataset('/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Sample_data/train', get_transform(train=True))\n","dataset_test = CustomDataset('/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Sample_data/test', get_transform(train=False))\n","\n","# split the dataset in train and test set\n","torch.manual_seed(1)\n","indices = torch.randperm(len(dataset)).tolist()\n","dataset = torch.utils.data.Subset(dataset, indices[:-50])\n","dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n","\n","# define training and validation data loaders\n","data_loader = torch.utils.data.DataLoader(\n","    dataset, batch_size=2, shuffle=True, num_workers=4,\n","    collate_fn=utils.collate_fn)\n","\n","data_loader_test = torch.utils.data.DataLoader(\n","    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n","    collate_fn=utils.collate_fn)"],"metadata":{"id":"5Jv_6FdUTnGl","executionInfo":{"status":"ok","timestamp":1638981232955,"user_tz":-540,"elapsed":153,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"execution_count":147,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# our dataset has two classes only - background and person\n","num_classes = 2\n","\n","# get the model using our helper function\n","model = get_model_instance_segmentation(num_classes)\n","# move model to the right device\n","model.to(device)\n","\n","# construct an optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005,\n","                            momentum=0.9, weight_decay=0.0005)\n","\n","# and a learning rate scheduler which decreases the learning rate by\n","# 10x every 3 epochs\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                               step_size=3, gamma=0.1)\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# our dataset has two classes only - background and person\n","num_classes = 2\n","\n","# get the model using our helper function\n","model = get_model_instance_segmentation(num_classes)\n","# move model to the right device\n","model.to(device)\n","\n","# construct an optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005,\n","                            momentum=0.9, weight_decay=0.0005)\n","\n","# and a learning rate scheduler which decreases the learning rate by\n","# 10x every 3 epochs\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                               step_size=3,\n","                                               gamma=0.1)"],"metadata":{"id":"iidKJ_43UDzm","executionInfo":{"status":"ok","timestamp":1638981234734,"user_tz":-540,"elapsed":1930,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"execution_count":148,"outputs":[]},{"cell_type":"code","source":["targets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LNk_elFNzwSE","executionInfo":{"status":"ok","timestamp":1638981234744,"user_tz":-540,"elapsed":122,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"d74afa6c-f843-4187-b8d1-151ccbc90ca5"},"execution_count":149,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'boxes': tensor([[ 703.,  677.,  991.,  945.],\n","          [ 704., 1246., 1022., 1637.],\n","          [1355.,  498., 1568.,  698.]]),\n","  'image_id': tensor([18]),\n","  'labels': tensor([1, 1, 1]),\n","  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           ...,\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0]],\n","  \n","          [[0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           ...,\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0]],\n","  \n","          [[0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           ...,\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)},\n"," {'boxes': tensor([[1331.,  537., 1567.,  803.],\n","          [1197., 1300., 1508., 1530.],\n","          [ 823.,  763., 1202., 1098.]]),\n","  'image_id': tensor([26]),\n","  'labels': tensor([1, 1, 1]),\n","  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           ...,\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0]],\n","  \n","          [[0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           ...,\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0]],\n","  \n","          [[0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           ...,\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)}]"]},"metadata":{},"execution_count":149}]},{"cell_type":"code","source":["targets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0e9KRtZF1uM7","executionInfo":{"status":"ok","timestamp":1638981234750,"user_tz":-540,"elapsed":69,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"22f5e490-7c18-4893-b8d5-19f1b499bd21"},"execution_count":150,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'boxes': tensor([[ 703.,  677.,  991.,  945.],\n","          [ 704., 1246., 1022., 1637.],\n","          [1355.,  498., 1568.,  698.]]),\n","  'image_id': tensor([18]),\n","  'labels': tensor([1, 1, 1]),\n","  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           ...,\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0]],\n","  \n","          [[0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           ...,\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0]],\n","  \n","          [[0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           ...,\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)},\n"," {'boxes': tensor([[1331.,  537., 1567.,  803.],\n","          [1197., 1300., 1508., 1530.],\n","          [ 823.,  763., 1202., 1098.]]),\n","  'image_id': tensor([26]),\n","  'labels': tensor([1, 1, 1]),\n","  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           ...,\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0]],\n","  \n","          [[0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           ...,\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0]],\n","  \n","          [[0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           ...,\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)}]"]},"metadata":{},"execution_count":150}]},{"cell_type":"code","source":["# let's train it for 10 epochs\n","from torch.optim.lr_scheduler import StepLR\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    # train for one epoch, printing every 10 iterations\n","    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n","    # update the learning rate\n","    lr_scheduler.step()\n","    # evaluate on the test dataset\n","    evaluate(model, data_loader_test, device=device)# let's train it for 10 epochs"],"metadata":{"id":"lcLBMHIXURWG","executionInfo":{"status":"error","timestamp":1638981236770,"user_tz":-540,"elapsed":2082,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"colab":{"base_uri":"https://localhost:8080/","height":440},"outputId":"48ef753c-765c-45cc-da10-266bab401510"},"execution_count":151,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: [0]  [0/3]  eta: 0:00:03  lr: 0.002502  loss: 1299.1433 (1299.1433)  loss_classifier: 0.8822 (0.8822)  loss_box_reg: 0.4372 (0.4372)  loss_mask: 1297.8226 (1297.8226)  loss_objectness: 0.0003 (0.0003)  loss_rpn_box_reg: 0.0010 (0.0010)  time: 1.1978  data: 0.9793  max mem: 3706\n","Epoch: [0]  [2/3]  eta: 0:00:00  lr: 0.005000  loss: 129.5469 (-103212960633221872908712552693760.0000)  loss_classifier: 0.8822 (41839380289394149687296.0000)  loss_box_reg: 0.5319 (53956456199622796247040.0000)  loss_mask: 128.1711 (-103212960633221872908712552693760.0000)  loss_objectness: 0.0028 (34456248115477247688704.0000)  loss_rpn_box_reg: 0.0020 (1329771763349190868992.0000)  time: 0.5336  data: 0.3358  max mem: 3916\n","Epoch: [0] Total time: 0:00:01 (0.5723 s / it)\n"]},{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-151-b1118cdc63f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# evaluate on the test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# let's train it for 10 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/engine.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Test:'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mcoco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coco_api_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0miou_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_iou_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mcoco_evaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCocoEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/coco_utils.py\u001b[0m in \u001b[0;36mget_coco_api_from_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCocoDetection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_coco_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/coco_utils.py\u001b[0m in \u001b[0;36mconvert_to_coco_api\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mbboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mareas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'area'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0miscrowd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iscrowd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'masks'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'area'"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"AHCPYQ43z58T","executionInfo":{"status":"aborted","timestamp":1638981236767,"user_tz":-540,"elapsed":86,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"execution_count":null,"outputs":[]}]}