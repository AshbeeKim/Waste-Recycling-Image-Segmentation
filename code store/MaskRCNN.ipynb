{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MaskRCNN.ipynb","provenance":[],"collapsed_sections":["UPMPCDkZhuZ3","EvwpoiUJdKhA","DU7KIN18dNHy"],"machine_shape":"hm","mount_file_id":"1QAad03Vhd3xys9z_WCrzuVbJUL6U47Wn","authorship_tag":"ABX9TyP6+fSM8j/AKYhxl6qrY1Im"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SStS257HZBwJ"},"source":["출처: https://dacon.io/codeshare/3725?dtype=recent"]},{"cell_type":"code","metadata":{"id":"zcQm2fkiZGNw","executionInfo":{"status":"ok","timestamp":1639047823063,"user_tz":-540,"elapsed":870,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":["import os\n","import glob\n","import json\n","from glob import glob\n","from tqdm import tqdm\n","import time\n","import datetime\n","import math\n","\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from pycocotools.coco import COCO\n","\n","import torch\n","import torch.optim as optim\n","from torch  import nn, Tensor\n","\n","import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","from collections import defaultdict, deque\n","\n","import base64\n","import torch.distributed as dist\n","from torch.utils.data import Dataset"],"execution_count":109,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aa_53qNpSC29","executionInfo":{"status":"ok","timestamp":1639047823473,"user_tz":-540,"elapsed":15,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"e6be5a98-6f95-48af-93ba-70e94723d2e0"},"source":["!ls"],"execution_count":110,"outputs":[{"output_type":"stream","name":"stdout","text":["coco_eval.py   drive\t  __pycache__  transforms.py  vision\n","coco_utils.py  engine.py  sample_data  utils.py\n"]}]},{"cell_type":"markdown","metadata":{"id":"UPMPCDkZhuZ3"},"source":["## 버전 확인"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-xdZkYLLhvnN","executionInfo":{"status":"ok","timestamp":1639047823474,"user_tz":-540,"elapsed":10,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"460b9ddb-3470-41bc-a018-32d40f94f17d"},"source":["print('torch :', torch.__version__)\n","print('torchvision : ', torchvision.__version__)\n","print('cv2 :', cv2.__version__)"],"execution_count":111,"outputs":[{"output_type":"stream","name":"stdout","text":["torch : 1.10.0+cu111\n","torchvision :  0.11.1+cu111\n","cv2 : 4.1.2\n"]}]},{"cell_type":"markdown","metadata":{"id":"EvwpoiUJdKhA"},"source":["## 전체 데이터 json 만들기"]},{"cell_type":"code","metadata":{"id":"rIBJ0FI2UIvj","executionInfo":{"status":"ok","timestamp":1639047823475,"user_tz":-540,"elapsed":8,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":["# # # 이건 저의 개인적인 코랩이기 때문에 다르신 분들은 다시 변경하시기 바랍니다.\n","# # ## train_PATH와 test_PATH만 변경하면 됩니다.\n","# # # json 파일로 만들어서 분석이 더 용이하다고 생각되어 만든 것이니 더 좋은 의견 있으면 말씀해주시기 바랍니다.\n","\n","# ## Train\n","# train_PATH = '/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Data/train/annotation'\n","# file_list = os.listdir(train_PATH)\n","\n","# train_files = []\n","# for file in tqdm(file_list):\n","#     dir = train_PATH + '/' + file\n","#     json_list = glob(dir + '/' +'*.json')\n","#     train_files.append(json_list)\n","\n","# train_json_list = []\n","# for files in tqdm(train_files):\n","#     for json_file in tqdm(files):\n","#         train_json_list.append(json_file)\n","\n","# ## Test\n","# test_PATH = '/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Data/test/annotations'\n","# file_list = os.listdir(test_PATH)\n","\n","# test_files = []\n","# for file in tqdm(file_list):\n","#     dir = test_PATH + '/' + file\n","#     json_list = glob(dir + '/' + '*.json')\n","#     test_files.append(json_list)\n","\n","# test_json_list = []\n","# for files in tqdm(test_files):\n","#     for json_file in tqdm(files):\n","#         test_json_list.append(json_file)"],"execution_count":112,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DU7KIN18dNHy"},"source":["## Sample data에서 json 만들기"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZYC1fY7fdCwt","executionInfo":{"status":"ok","timestamp":1639047823900,"user_tz":-540,"elapsed":433,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"e5762e37-68d7-4de4-dcd6-5e6b716bf2bd"},"source":["## Train\n","train_PATH = '/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Sample_data/train/annotation'\n","file_list = os.listdir(train_PATH)\n","\n","train_files = []\n","for file in tqdm(file_list):\n","    dir = train_PATH + '/' + file\n","    json_list = glob(dir + '/' +'*.json')\n","    train_files.append(json_list)\n","\n","train_json_list = []\n","for files in tqdm(train_files):\n","    for json_file in tqdm(files):\n","        train_json_list.append(json_file)\n","\n","\n","test_PATH = '/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Sample_data/test/annotations'\n","file_list = os.listdir(test_PATH)\n","test_files = []\n","for file in tqdm(file_list):\n","    dir = test_PATH + '/' + file\n","    json_list = glob(dir + '/' +'*.json')\n","    test_files.append(json_list)\n","\n","test_json_list = []\n","for files in tqdm(test_files):\n","    for json_file in tqdm(files):\n","        test_json_list.append(json_file)"],"execution_count":113,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:00<00:00, 711.80it/s]\n","  0%|          | 0/4 [00:00<?, ?it/s]\n","100%|██████████| 56/56 [00:00<00:00, 446541.87it/s]\n","\n","100%|██████████| 56/56 [00:00<00:00, 434160.86it/s]\n","\n","100%|██████████| 56/56 [00:00<00:00, 435771.84it/s]\n","\n","100%|██████████| 56/56 [00:00<00:00, 438210.87it/s]\n","100%|██████████| 4/4 [00:00<00:00, 180.82it/s]\n","100%|██████████| 4/4 [00:00<00:00, 1175.62it/s]\n","  0%|          | 0/4 [00:00<?, ?it/s]\n","100%|██████████| 14/14 [00:00<00:00, 142179.80it/s]\n","\n","100%|██████████| 14/14 [00:00<00:00, 123882.40it/s]\n","\n","100%|██████████| 14/14 [00:00<00:00, 112923.57it/s]\n","\n","100%|██████████| 14/14 [00:00<00:00, 134679.49it/s]\n","100%|██████████| 4/4 [00:00<00:00, 161.87it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"AD3TaUlo1gZY","executionInfo":{"status":"ok","timestamp":1639047823900,"user_tz":-540,"elapsed":9,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":["import os\n","import numpy as np\n","import torch\n","from PIL import Image\n","\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, transforms = None, mode = 'train', json_file = None):\n","        self.mode = mode\n","        self.root = root\n","        self.transforms = transforms\n","        self.json_file = json_file\n","\n","    def __getitem__(self, idx):\n","        if self.mode == 'train':\n","\n","            annot = json_file[idx]\n","            # Train의 경로로 이동한다.\n","            # 나중에 진짜 데이터를 다룰 때 사용한다.\n","            # PATH = root + '/' + 'train'\n","\n","            with open(annot, 'r') as f:\n","                annot = json.loads(f.read())\n","                \n","            image_name = annot['images'][0]['file_name']\n","            dir = image_name.split('_')[0]\n","\n","            image_PATH = self.root + '/' + 'train' + '/' +'image' +'/' + dir + '/' + image_name\n","            \n","            img = Image.open(image_PATH).convert('RGB')\n","\n","            boxes = []\n","            segmentations = []\n","            labels = []\n","\n","            for i in range(len(annot['annotations'])):\n","\n","                segmentation = annot['annotations'][i]['segmentation'][0]\n","                bbox = annot['annotations'][i]['bbox']\n","                label = torch.tensor(annot['annotations'][i]['category_id'], dtype = np.uint8)\n","                xmin, ymin, width, height = bbox[0],bbox[1],bbox[2],bbox[3]\n","                xmin, ymin, xmax, ymax = xmin, ymin, xmin + width, ymax + height\n","                \n","                boxes.append(torch.as_tensor([xmin, ymin, xmax, ymax], dtype = torch.float32))\n","                segmentations.append([segmentation])\n","                labels.append(label)\n","\n","            target = {}\n","            target['boxes'] = boxes\n","            target['labels'] = labels\n","            target['segmentation'] = segmentations\n","            target['image_id'] = torch.tensor(idx)\n","\n","            if self.transforms is not None:\n","                img, target = self.transforms(img, target)\n","\n","            return img, target\n","\n","        if self.mode == 'test':\n","            # Test의 경로로 이동한다.\n","            # PATH = root + '/' + 'test'\n","            # 나중에 진짜 데이터를 다룰 때 사용한다.\n","            annot = json_file[idx]\n","\n","            ##### 이 부분은 추후에 작성하기로 하자.\n","            with open(annot, 'r') as f:\n","                annot = json.loads(f.read())\n","\n","            image_name = annot['images'][0]['file_name']\n","            dir = image_name[:2]\n","\n","            image_PATH = PATH + '/' + dir + '/' + image_name\n","            image = Image.open(image_PATH).convert('RGB')\n","            target = {}\n","\n","            target['image_id'] = image_name\n","\n","            return image, target\n","\n","    def __len__(self):\n","        return len(self.json_file)"],"execution_count":114,"outputs":[]},{"cell_type":"code","metadata":{"id":"ypiLsatqlf2G","executionInfo":{"status":"ok","timestamp":1639047823901,"user_tz":-540,"elapsed":9,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":["root = '/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Sample_data'\n","train_dataset = CustomDataset(root = root,json_file = train_json_list, mode = 'train')\n","test_dataset = CustomDataset(root = root, json_file = test_json_list, mode = 'test')"],"execution_count":115,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uxxSEsZVTOeu"},"source":["## Pytorch Tutorial을 가지고 분석을 진행해 보기\n","\n","https://tutorials.pytorch.kr/intermediate/torchvision_tutorial.html\n","\n","PET를 먼저 진행해 보고 추후에 나머지도 진행해 볼 예정입니다."]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Sample_data/train/annotation/PP/PP_050_1520.json', 'r') as f:\n","    data = json.loads(f.read())\n","\n","print(data['images'][0]['file_name'].split('_')[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lundwtUhWjj7","executionInfo":{"status":"ok","timestamp":1639047823901,"user_tz":-540,"elapsed":8,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"52e6dd58-1668-4cf5-da40-38476d184455"},"execution_count":116,"outputs":[{"output_type":"stream","name":"stdout","text":["050\n"]}]},{"cell_type":"code","metadata":{"id":"w4TgIuqmTNmX","executionInfo":{"status":"ok","timestamp":1639047824330,"user_tz":-540,"elapsed":434,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":["import torch\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, transforms):\n","        self.root = root\n","        self.transforms = transforms\n","        self.categories = {1 : 'pet', 2 : 'ps', 3 : 'pp', 4 : 'pe'}\n","\n","        mode = self.root.split('/')[-1]\n","        self.mode = mode\n","\n","        self.imgs = list(sorted(os.listdir(os.path.join(root, 'image', 'PET'))))\n","        if self.mode == 'train':\n","            self.annot = list(sorted(os.listdir(os.path.join(root, 'annotation', 'PET'))))\n","        else:\n","            self.annot = list(sorted(os.listdir(os.path.join(root, 'annotations', 'PET'))))\n","\n","    def __getitem__(self, idx):\n","        if self.mode == 'train':\n","            img_path = os.path.join(self.root, 'image','PET', self.imgs[idx])\n","\n","            annot_path = os.path.join(self.root, 'annotation','PET',self.annot[idx])\n","\n","            with open(annot_path, 'r') as f:\n","                annot = json.loads(f.read())\n","            \n","            # 이 부분은 그냥 숫자를 idx를 넣는 부분과 숫자를 넣는 부분이 있고 저는 아무것도 안할 것 이기 때문에 데이터에 그냥 뽑아온 값입니다~\n","            image_id = int(annot_path.split('/')[-1].split('_')[1])\n","\n","            img = Image.open(img_path).convert('RGB')\n","\n","            # width, height로 0인 이미지 생성하여 segmentation 넣어주기\n","            x = annot['images'][0]['width']\n","            y = annot['images'][0]['height']\n","\n","            #각 마스크를 target으로 사용해야함\n","            annot = annot['annotations']\n","\n","            masked_image = np.zeros((x, y), dtype = np.uint8)\n","\n","            # area, iscrowd는 채울 방법이 없어서 그냥 bbox를 채워넣기로 하자.\n","            mask = []\n","            boxes = []\n","            labels = []\n","            iscrowd = []\n","            area = []\n","            masked_image = np.zeros((x, y), dtype = np.uint8)\n","\n","            for i in range(len(annot)):\n","                segmentation = annot[i]['segmentation'][0]\n","                areas = annot[i]['area']\n","                iscrowds = annot[i]['iscrowd']\n","                xmin, ymin, width, height = annot[i]['bbox'][0], annot[i]['bbox'][1], annot[i]['bbox'][2], annot[i]['bbox'][3]\n","                xmax = xmin + width\n","                ymax = ymin + height\n","\n","                label = annot[i]['category_id']\n","\n","                all_points_x = []\n","                all_points_y = []\n","\n","                for j in range(len(segmentation)):\n","                    if j%2 == 0:\n","                        all_points_x.append(segmentation[j])\n","                    else:\n","                        all_points_y.append(segmentation[j])\n","\n","                polygon_xy = np.array([(x,y) for (x,y) in zip(all_points_x, all_points_y)])\n","\n","                cv2.fillPoly(masked_image, np.uint([polygon_xy]), i+1)\n","\n","                mask.append(masked_image)\n","                boxes.append([xmin, ymin, xmax, ymax])\n","                labels.append(label)\n","                area.append(areas)\n","                iscrowd.append(iscrowds)\n","\n","            mask = np.array(mask)\n","            obj_ids = np.unique(mask)\n","            obj_ids = obj_ids[1:]\n","            masks = mask == obj_ids[:, None, None]\n","            num_objs = len(obj_ids)\n","            \n","\n","            boxes = torch.as_tensor(boxes, dtype = torch.float32)\n","            labels = torch.ones((num_objs,), dtype = torch.int64)\n","            mask = torch.as_tensor(mask, dtype = torch.uint8)\n","            image_id = torch.tensor([image_id])\n","            area = torch.as_tensor(area, dtype = torch.float32)\n","            iscrowd = torch.as_tensor(iscrowd, dtype = torch.int32)\n","\n","            target = {}\n","            target['boxes'] = boxes\n","            target['labels'] = labels\n","            target['masks'] = mask\n","            target['image_id'] = image_id\n","            target['area'] = area\n","            target['iscrowd'] = iscrowd\n","\n","            img = self.preprocess(img)\n","\n","            return img, target\n","            \n","        if self.mode == 'test':\n","            img_path = os.path.join(self.root, 'image','PET', self.imgs[idx])\n","\n","            annot_path = os.path.join(self.root, 'annotations','PET',self.annot[idx])\n","\n","            with open(annot_path, 'r') as f:\n","                annot = json.loads(f.read())\n","            \n","            # 이 부분은 그냥 숫자를 idx를 넣는 부분과 숫자를 넣는 부분이 있고 저는 아무것도 안할 것 이기 때문에 데이터에 그냥 뽑아온 값입니다~\n","            image_id = int(annot_path.split('/')[-1].split('_')[1])\n","\n","            img = Image.open(img_path).convert('RGB')\n","\n","            # width, height로 0인 이미지 생성하여 segmentation 넣어주기\n","            x = annot['images'][0]['width']\n","            y = annot['images'][0]['height']\n","\n","            #각 마스크를 target으로 사용해야함\n","            annot = annot['annotations']\n","\n","            masked_image = np.zeros((x, y), dtype = np.uint8)\n","\n","            # area, iscrowd는 채울 방법이 없어서 그냥 bbox를 채워넣기로 하자.\n","            mask = []\n","            boxes = []\n","            labels = []\n","            iscrowd = []\n","            area = []\n","            masked_image = np.zeros((x, y), dtype = np.uint8)\n","\n","            for i in range(len(annot)):\n","                segmentation = annot[i]['segmentation'][0]\n","                areas = annot[i]['area']\n","                iscrowds = annot[i]['iscrowd']\n","                xmin, ymin, width, height = annot[i]['bbox'][0], annot[i]['bbox'][1], annot[i]['bbox'][2], annot[i]['bbox'][3]\n","                xmax = xmin + width\n","                ymax = ymin + height\n","\n","                label = annot[i]['category_id']\n","\n","                all_points_x = []\n","                all_points_y = []\n","\n","                for j in range(len(segmentation)):\n","                    if j%2 == 0:\n","                        all_points_x.append(segmentation[j])\n","                    else:\n","                        all_points_y.append(segmentation[j])\n","\n","                polygon_xy = np.array([(x,y) for (x,y) in zip(all_points_x, all_points_y)])\n","\n","                cv2.fillPoly(masked_image, np.uint([polygon_xy]), i+1)\n","\n","                mask.append(masked_image)\n","                boxes.append([xmin, ymin, xmax, ymax])\n","                labels.append(label)\n","                area.append(areas)\n","                iscrowd.append(iscrowds)\n","\n","            mask = np.array(mask)\n","            obj_ids = np.unique(mask)\n","            obj_ids = obj_ids[1:]\n","            masks = mask == obj_ids[:, None, None]\n","            num_objs = len(obj_ids)\n","            \n","\n","            boxes = torch.as_tensor(boxes, dtype = torch.float32)\n","            labels = torch.ones((num_objs,), dtype = torch.int64)\n","            mask = torch.as_tensor(mask, dtype = torch.uint8)\n","            image_id = torch.tensor([image_id])\n","            area = torch.as_tensor(area, dtype = torch.float32)\n","            iscrowd = torch.as_tensor(iscrowd, dtype = torch.int32)\n","\n","            target = {}\n","            target['boxes'] = boxes\n","            target['labels'] = labels\n","            target['masks'] = mask\n","            target['image_id'] = image_id\n","            target['area'] = area\n","            target['iscrowd'] = iscrowd\n","\n","            img = self.preprocess(img)\n","\n","            return img, target\n","\n","\n","\n","    def preprocess(self, img):\n","        image = img\n","        import torchvision.transforms as T\n","        m, s = np.mean(image, axis = (0, 1)), np.std(image, axis = (0, 1))        \n","        if self.mode == 'train':\n","            transform = T.Compose([\n","                                   T.ToTensor(),\n","                                   T.Normalize(mean = m, std = s),\n","            ])\n","            image = transform(image)\n","        else:\n","            transform = T.Compose([\n","                                   T.Resize(256),\n","                                   T.ToTensor(),\n","                                   T.Normalize(mean = m, std = s),\n","            ])\n","            image = transform(image)\n","        return image\n","\n","\n","    def __len__(self):\n","        return len(self.imgs)"],"execution_count":117,"outputs":[]},{"cell_type":"code","metadata":{"id":"DvoXdQtCCEzH","executionInfo":{"status":"ok","timestamp":1639047824821,"user_tz":-540,"elapsed":493,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True)\n","\n","num_classes = 1 + 1 # 원래 클래스 + 1\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"],"execution_count":118,"outputs":[]},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","# 분류 목적으로 미리 학습된 모델을 로드하고 특징들만을 리턴하도록 합니다\n","backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n","# Faster RCNN은 백본의 출력 채널 수를 알아야 합니다.\n","# mobilenetV2의 경우 1280이므로 여기에 추가해야 합니다.\n","backbone.out_channels = 1280\n","\n","# RPN(Region Proposal Network)이 5개의 서로 다른 크기와 3개의 다른 측면 비율(Aspect ratio)을 가진\n","# 5 x 3개의 앵커를 공간 위치마다 생성하도록 합니다.\n","# 각 특징 맵이 잠재적으로 다른 사이즈와 측면 비율을 가질 수 있기 때문에 Tuple[Tuple[int]] 타입을 가지도록 합니다.\n","\n","anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n","                                   aspect_ratios=((0.5, 1.0, 2.0),))\n","\n","# 관심 영역의 자르기 및 재할당 후 자르기 크기를 수행하는 데 사용할 피쳐 맵을 정의합니다.\n","# 만약 백본이 텐서를 리턴할때, featmap_names 는 [0] 이 될 것이라고 예상합니다.\n","# 일반적으로 백본은 OrderedDict[Tensor] 타입을 리턴해야 합니다.\n","# 그리고 특징맵에서 사용할 featmap_names 값을 정할 수 있습니다.\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n","                                                output_size=7,\n","                                                sampling_ratio=2)\n","\n","# 조각들을 Faster RCNN 모델로 합칩니다.\n","model = FasterRCNN(backbone,\n","                   num_classes=2,\n","                   rpn_anchor_generator=anchor_generator,\n","                   box_roi_pool=roi_pooler)"],"metadata":{"id":"-QDlR2nwSs48","executionInfo":{"status":"ok","timestamp":1639047825970,"user_tz":-540,"elapsed":1152,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"execution_count":119,"outputs":[]},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","\n","\n","def get_model_instance_segmentation(num_classes):\n","    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n","\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n","                                                       hidden_layer,\n","                                                       num_classes)\n","\n","    return model"],"metadata":{"id":"DkqNzf-ESzNp","executionInfo":{"status":"ok","timestamp":1639047825971,"user_tz":-540,"elapsed":5,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"execution_count":120,"outputs":[]},{"cell_type":"code","source":["%%shell\n","\n","# Download TorchVision repo to use some files from\n","# references/detection\n","git clone https://github.com/pytorch/vision.git\n","cd vision\n","git checkout v0.8.2\n","\n","cp references/detection/utils.py ../\n","cp references/detection/transforms.py ../\n","cp references/detection/coco_eval.py ../\n","cp references/detection/engine.py ../\n","cp references/detection/coco_utils.py ../"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5krPiMPeTbMi","executionInfo":{"status":"ok","timestamp":1639047826711,"user_tz":-540,"elapsed":744,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"f54c0b60-d688-40e5-8c88-356a0c2040c2"},"execution_count":121,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'vision' already exists and is not an empty directory.\n","HEAD is now at 2f40a483 [v0.8.X] .circleci: Add Python 3.9 to CI (#3063)\n"]},{"output_type":"execute_result","data":{"text/plain":[""]},"metadata":{},"execution_count":121}]},{"cell_type":"code","source":["from engine import train_one_epoch, evaluate\n","import utils\n","import transforms as T\n","# from torchvision import transforms as T\n","\n","def get_transform(train):\n","    transforms = []\n","    # transforms.append(T.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)))\n","    transforms.append(T.ToTensor())\n","    # transforms.append(T.Resize(256))\n","\n","    # transforms.append(T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))\n","    if train:\n","        # (역자주: 학습시 50% 확률로 학습 영상을 좌우 반전 변환합니다)\n","        transforms.append(T.RandomHorizontalFlip(0.5))\n","    return T.Compose(transforms)"],"metadata":{"id":"7AKNiQ2xS1mj","executionInfo":{"status":"ok","timestamp":1639047826711,"user_tz":-540,"elapsed":13,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"execution_count":122,"outputs":[]},{"cell_type":"code","source":["np.unique(targets[0]['masks'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kZmcDoqvz3Im","executionInfo":{"status":"ok","timestamp":1639047826711,"user_tz":-540,"elapsed":13,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"9bcc3132-46de-45fb-bb56-361b144605e7"},"execution_count":123,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 2, 3], dtype=uint8)"]},"metadata":{},"execution_count":123}]},{"cell_type":"code","source":["np.unique(targets[0]['masks'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fAfwNrzF1Dg5","executionInfo":{"status":"ok","timestamp":1639047826712,"user_tz":-540,"elapsed":11,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"e1fd8c37-0524-44fd-fd99-b684d09d0404"},"execution_count":124,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 2, 3], dtype=uint8)"]},"metadata":{},"execution_count":124}]},{"cell_type":"code","source":["targets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pJfocShK1-Pu","executionInfo":{"status":"ok","timestamp":1639047826712,"user_tz":-540,"elapsed":8,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"ce4887c6-a2df-445e-8a97-55c5680ce94f"},"execution_count":125,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'area': tensor([105837.0000, 127080.0000, 115652.5000]),\n","  'boxes': tensor([[ 466.,  544.,  931.,  958.],\n","          [ 596., 1117.,  942., 1692.],\n","          [ 886.,  741., 1214., 1284.]]),\n","  'image_id': tensor([16]),\n","  'iscrowd': tensor([0, 0, 0], dtype=torch.int32),\n","  'labels': tensor([1, 1, 1]),\n","  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           ...,\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0]],\n","  \n","          [[0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           ...,\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0]],\n","  \n","          [[0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           ...,\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)},\n"," {'area': tensor([ 89264.5000, 107719.5000, 116722.0000]),\n","  'boxes': tensor([[ 330., 1067.,  869., 1463.],\n","          [ 838.,  969., 1161., 1595.],\n","          [ 638.,  571., 1257.,  838.]]),\n","  'image_id': tensor([14]),\n","  'iscrowd': tensor([0, 0, 0], dtype=torch.int32),\n","  'labels': tensor([1, 1, 1]),\n","  'masks': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           ...,\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0]],\n","  \n","          [[0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           ...,\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0]],\n","  \n","          [[0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           ...,\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0],\n","           [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)}]"]},"metadata":{},"execution_count":125}]},{"cell_type":"code","source":["model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","dataset = CustomDataset('/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Sample_data/train', get_transform(train=True))\n","data_loader = torch.utils.data.DataLoader(\n","    dataset, batch_size=2, shuffle=True, num_workers=4,\n","    collate_fn=utils.collate_fn\n",")\n","# For Training\n","images,targets = next(iter(data_loader))\n","images = list(image for image in images)\n","targets = [{k: v for k, v in t.items()} for t in targets]\n","\n","output = model(images,targets)   # Returns losses and detections\n","# For inference\n","model.eval()\n","x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n","predictions = model(x)           # Returns predictions"],"metadata":{"id":"1muiZL-cS3HO","executionInfo":{"status":"ok","timestamp":1639047837160,"user_tz":-540,"elapsed":10454,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"execution_count":126,"outputs":[]},{"cell_type":"code","source":["# use our dataset and defined transformations\n","dataset = CustomDataset('/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Sample_data/train', get_transform(train=True))\n","dataset_test = CustomDataset('/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Sample_data/test', get_transform(train=False))\n","\n","# # split the dataset in train and test set\n","# torch.manual_seed(1)\n","# indices = torch.randperm(len(dataset)).tolist()\n","# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n","# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n","\n","# define training and validation data loaders\n","data_loader = torch.utils.data.DataLoader(\n","    dataset, batch_size=2, shuffle=True, num_workers=4,\n","    collate_fn=utils.collate_fn)\n","\n","data_loader_test = torch.utils.data.DataLoader(\n","    dataset_test, batch_size=2, shuffle=False, num_workers=4,\n","    collate_fn=utils.collate_fn)"],"metadata":{"id":"5Jv_6FdUTnGl","executionInfo":{"status":"ok","timestamp":1639047838063,"user_tz":-540,"elapsed":908,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"execution_count":127,"outputs":[]},{"cell_type":"code","source":["print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TRwhQL2pjKkK","executionInfo":{"status":"ok","timestamp":1639047838063,"user_tz":-540,"elapsed":7,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"910fdbf9-c8d9-4def-877f-fdcb67ef4da7"},"execution_count":128,"outputs":[{"output_type":"stream","name":"stdout","text":["FasterRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n","  )\n","  (backbone): BackboneWithFPN(\n","    (body): IntermediateLayerGetter(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): FrozenBatchNorm2d(256, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(512, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(1024, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(2048, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","      )\n","      (layer_blocks): ModuleList(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (extra_blocks): LastLevelMaxPool()\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n","    )\n","  )\n",")\n"]}]},{"cell_type":"code","source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# our dataset has two classes only - background and person\n","num_classes = 2\n","\n","# get the model using our helper function\n","model = get_model_instance_segmentation(num_classes)\n","# move model to the right device\n","model.to(device)\n","\n","# construct an optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.0005\n","                            , weight_decay=0.005)\n","\n","# and a learning rate scheduler which decreases the learning rate by\n","# 10x every 3 epochs\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                               step_size=3, gamma=0.1)"],"metadata":{"id":"iidKJ_43UDzm","executionInfo":{"status":"ok","timestamp":1639047838642,"user_tz":-540,"elapsed":583,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"execution_count":129,"outputs":[]},{"cell_type":"code","source":["np.unique(targets[1]['masks'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pcYXidweYE2w","executionInfo":{"status":"ok","timestamp":1639047838642,"user_tz":-540,"elapsed":14,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"033e69b7-b454-4a4d-f9f4-06d720841506"},"execution_count":130,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 1, 2, 3], dtype=uint8)"]},"metadata":{},"execution_count":130}]},{"cell_type":"code","source":["images[0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-RVIbdpHfEv4","executionInfo":{"status":"ok","timestamp":1639047838643,"user_tz":-540,"elapsed":14,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"6f1eeefb-098c-44cc-831c-b932db4a159e"},"execution_count":131,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 2024, 2024])"]},"metadata":{},"execution_count":131}]},{"cell_type":"code","source":["# 데이터는 제대로 들어가 있다.\n","print('train의 길이 :', len(data_loader.dataset))\n","print('test의 길이 :', len(data_loader_test.dataset))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sfyf8VT_sXw_","executionInfo":{"status":"ok","timestamp":1639047838643,"user_tz":-540,"elapsed":12,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"57e6e699-a173-4396-cfff-68239671dcb1"},"execution_count":132,"outputs":[{"output_type":"stream","name":"stdout","text":["train의 길이 : 56\n","test의 길이 : 14\n"]}]},{"cell_type":"code","source":["# 이 친구는 왜 loss가 마이너스가 나와서 사람을 힘들게 하는지 알아봐야겠다~\n","\n","# let's train it for 10 epochs\n","from torch.optim.lr_scheduler import StepLR\n","num_epochs = 3\n","\n","for epoch in range(num_epochs):\n","    # train for one epoch, printing every 10 iterations\n","    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n","    # update the learning rate\n","    lr_scheduler.step()\n","    # evaluate on the test dataset\n","    evaluate(model, data_loader_test, device=device)# let's train it for 10 epochs"],"metadata":{"id":"lcLBMHIXURWG","executionInfo":{"status":"error","timestamp":1639047884996,"user_tz":-540,"elapsed":46362,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"colab":{"base_uri":"https://localhost:8080/","height":987},"outputId":"70b685a9-8d1f-4765-cfa3-5c94349e3e92"},"execution_count":133,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: [0]  [ 0/28]  eta: 0:01:08  lr: 0.000019  loss: 7.4629 (7.4629)  loss_classifier: 0.8145 (0.8145)  loss_box_reg: 0.0479 (0.0479)  loss_mask: 6.3127 (6.3127)  loss_objectness: 0.2627 (0.2627)  loss_rpn_box_reg: 0.0252 (0.0252)  time: 2.4356  data: 2.2172  max mem: 5882\n","Epoch: [0]  [10/28]  eta: 0:00:10  lr: 0.000204  loss: 4.9525 (7.1851)  loss_classifier: 0.4372 (0.5156)  loss_box_reg: 0.0352 (0.0483)  loss_mask: 3.9826 (6.1383)  loss_objectness: 0.4900 (0.4530)  loss_rpn_box_reg: 0.0252 (0.0300)  time: 0.5777  data: 0.3818  max mem: 5992\n","Epoch: [0]  [20/28]  eta: 0:00:04  lr: 0.000389  loss: 1.5521 (4.0817)  loss_classifier: 0.2774 (0.3610)  loss_box_reg: 0.0698 (0.0627)  loss_mask: 1.1929 (3.3191)  loss_objectness: 0.2157 (0.3138)  loss_rpn_box_reg: 0.0250 (0.0251)  time: 0.4211  data: 0.2247  max mem: 5994\n","Epoch: [0]  [27/28]  eta: 0:00:00  lr: 0.000500  loss: 0.4394 (2.7850)  loss_classifier: 0.1680 (0.3150)  loss_box_reg: 0.0920 (0.0782)  loss_mask: 0.0430 (2.0866)  loss_objectness: 0.1674 (0.2807)  loss_rpn_box_reg: 0.0172 (0.0246)  time: 0.4209  data: 0.2220  max mem: 5994\n","Epoch: [0] Total time: 0:00:13 (0.4797 s / it)\n","creating index...\n","index created!\n","Test:  [0/7]  eta: 0:00:15  model_time: 0.1646 (0.1646)  evaluator_time: 0.0622 (0.0622)  time: 2.2533  data: 2.0251  max mem: 5994\n","Test:  [6/7]  eta: 0:00:00  model_time: 0.1324 (0.1296)  evaluator_time: 0.0513 (0.0516)  time: 0.5480  data: 0.3637  max mem: 5994\n","Test: Total time: 0:00:03 (0.5662 s / it)\n","Averaged stats: model_time: 0.1324 (0.1296)  evaluator_time: 0.0513 (0.0516)\n","Accumulating evaluation results...\n","DONE (t=0.00s).\n","Accumulating evaluation results...\n","DONE (t=0.00s).\n","IoU metric: bbox\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n","IoU metric: segm\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n","Epoch: [1]  [ 0/28]  eta: 0:01:07  lr: 0.000500  loss: -7.4803 (-7.4803)  loss_classifier: 0.1899 (0.1899)  loss_box_reg: 0.2063 (0.2063)  loss_mask: -8.2719 (-8.2719)  loss_objectness: 0.3489 (0.3489)  loss_rpn_box_reg: 0.0464 (0.0464)  time: 2.4012  data: 2.1968  max mem: 5994\n","Loss is nan, stopping training\n","{'loss_classifier': tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>), 'loss_mask': tensor(nan, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_objectness': tensor(nan, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(nan, device='cuda:0', grad_fn=<DivBackward0>)}\n"]},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"]}]},{"cell_type":"code","source":["len(data_loader_test.dataset)"],"metadata":{"id":"x8KuoepiGdw9","executionInfo":{"status":"aborted","timestamp":1639047884988,"user_tz":-540,"elapsed":18,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(data_loader.dataset)"],"metadata":{"id":"iUzsx3WLHL_C","executionInfo":{"status":"aborted","timestamp":1639047884989,"user_tz":-540,"elapsed":19,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","sys.exit('exit')"],"metadata":{"id":"53IsuM61HVjp","executionInfo":{"status":"aborted","timestamp":1639047884992,"user_tz":-540,"elapsed":22,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"hagH0mTDPVvn","executionInfo":{"status":"aborted","timestamp":1639047884994,"user_tz":-540,"elapsed":25,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"execution_count":null,"outputs":[]}]}