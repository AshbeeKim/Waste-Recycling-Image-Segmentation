{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MaskRCNN.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1QAad03Vhd3xys9z_WCrzuVbJUL6U47Wn","authorship_tag":"ABX9TyNip6w5UsxUy2lmG3DgQ9lc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SStS257HZBwJ"},"source":["출처: https://dacon.io/codeshare/3725?dtype=recent"]},{"cell_type":"code","metadata":{"id":"zcQm2fkiZGNw","executionInfo":{"status":"ok","timestamp":1638810033153,"user_tz":-540,"elapsed":552,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":["import os\n","import glob\n","import json\n","from glob import glob\n","from tqdm import tqdm\n","import time\n","import datetime\n","import math\n","\n","\n","import cv2\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from pycocotools.coco import COCO\n","\n","import torch\n","import torch.optim as optim\n","from torch  import nn, Tensor\n","\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","from collections import defaultdict, deque\n","\n","\n","import base64\n","import torch.distributed as dist\n","from torch.utils.data import Dataset"],"execution_count":178,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aa_53qNpSC29","executionInfo":{"status":"ok","timestamp":1638810033761,"user_tz":-540,"elapsed":79,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"9aeb6a0a-f79f-48fa-ae03-31a4f2ea8e6b"},"source":["!ls"],"execution_count":179,"outputs":[{"output_type":"stream","name":"stdout","text":["drive  sample_data\n"]}]},{"cell_type":"markdown","metadata":{"id":"UPMPCDkZhuZ3"},"source":["## 버전 확인"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-xdZkYLLhvnN","executionInfo":{"status":"ok","timestamp":1638810033761,"user_tz":-540,"elapsed":69,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"da903666-757b-4620-f50a-8d90160093ca"},"source":["print('torch :', torch.__version__)\n","print('torchvision : ', torchvision.__version__)\n","print('cv2 :', cv2.__version__)"],"execution_count":180,"outputs":[{"output_type":"stream","name":"stdout","text":["torch : 1.10.0+cu111\n","torchvision :  0.11.1+cu111\n","cv2 : 4.1.2\n"]}]},{"cell_type":"markdown","metadata":{"id":"EvwpoiUJdKhA"},"source":["## 전체 데이터 json 만들기"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rIBJ0FI2UIvj","executionInfo":{"status":"ok","timestamp":1638810034633,"user_tz":-540,"elapsed":920,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"ca68fe87-1c19-4d2b-ec55-200559a3b3f8"},"source":["# # 이건 저의 개인적인 코랩이기 때문에 다르신 분들은 다시 변경하시기 바랍니다.\n","# ## train_PATH와 test_PATH만 변경하면 됩니다.\n","# # json 파일로 만들어서 분석이 더 용이하다고 생각되어 만든 것이니 더 좋은 의견 있으면 말씀해주시기 바랍니다.\n","\n","## Train\n","train_PATH = '/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Data/train/annotation'\n","file_list = os.listdir(train_PATH)\n","\n","train_files = []\n","for file in tqdm(file_list):\n","    dir = train_PATH + '/' + file\n","    json_list = glob(dir + '/' +'*.json')\n","    train_files.append(json_list)\n","\n","train_json_list = []\n","for files in tqdm(train_files):\n","    for json_file in tqdm(files):\n","        train_json_list.append(json_file)\n","\n","## Test\n","test_PATH = '/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Data/test/annotations'\n","file_list = os.listdir(test_PATH)\n","\n","test_files = []\n","for file in tqdm(file_list):\n","    dir = test_PATH + '/' + file\n","    json_list = glob(dir + '/' + '*.json')\n","    test_files.append(json_list)\n","\n","test_json_list = []\n","for files in tqdm(test_files):\n","    for json_file in tqdm(files):\n","        test_json_list.append(json_file)"],"execution_count":181,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:00<00:00, 71.83it/s]\n","  0%|          | 0/4 [00:00<?, ?it/s]\n","100%|██████████| 1000/1000 [00:00<00:00, 1019767.57it/s]\n","\n","100%|██████████| 1000/1000 [00:00<00:00, 294792.24it/s]\n","\n","100%|██████████| 1000/1000 [00:00<00:00, 912003.48it/s]\n","\n","100%|██████████| 1000/1000 [00:00<00:00, 966429.49it/s]\n","100%|██████████| 4/4 [00:00<00:00, 75.78it/s]\n","100%|██████████| 4/4 [00:00<00:00, 162.67it/s]\n","  0%|          | 0/4 [00:00<?, ?it/s]\n","100%|██████████| 100/100 [00:00<00:00, 493447.53it/s]\n","\n","100%|██████████| 100/100 [00:00<00:00, 535671.01it/s]\n","\n","100%|██████████| 100/100 [00:00<00:00, 511500.49it/s]\n","\n","100%|██████████| 100/100 [00:00<00:00, 509635.97it/s]\n","100%|██████████| 4/4 [00:00<00:00, 143.83it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"DU7KIN18dNHy"},"source":["## Sample data에서 json 만들기"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZYC1fY7fdCwt","executionInfo":{"status":"ok","timestamp":1638810117661,"user_tz":-540,"elapsed":83042,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"488250c0-4366-47bc-9e42-f9d31d80b904"},"source":["## Train\n","train_PATH = '/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Sample_data/annotation'\n","file_list = os.listdir(train_PATH)\n","\n","train_files = []\n","for file in tqdm(file_list):\n","    dir = train_PATH + '/' + file\n","    json_list = glob(dir + '/' +'*.json')\n","    train_files.append(json_list)\n","\n","train_json_list = []\n","for files in tqdm(train_files):\n","    for json_file in tqdm(files):\n","        train_json_list.append(json_file)\n","\n","# 225로 한 이유? -> 252개 중에 0.8로 곱하거나 0.9로 곱하면 소숫점이 나오기 때문에 안됩니다 ㅜㅜ\n","# 그래서 임의로 한 것이기 때문에 더 완벽한 분석법이 있으면 알려주세요ㅜㅜ\n","train = train_json_list[:225]\n","test = train_json_list[225:]"],"execution_count":182,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4/4 [00:00<00:00, 225.47it/s]\n","  0%|          | 0/4 [00:00<?, ?it/s]\n","  0%|          | 0/63 [00:00<?, ?it/s]\u001b[AException ignored in: <function WeakValueDictionary.__init__.<locals>.remove at 0x7fb3fdf2cd40>\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.7/weakref.py\", line 109, in remove\n","    def remove(wr, selfref=ref(self), _atomic_removal=_remove_dead_weakref):\n","KeyboardInterrupt\n","100%|██████████| 63/63 [01:20<00:00,  1.28s/it]\n"," 25%|██▌       | 1/4 [01:20<04:02, 80.78s/it]\n","100%|██████████| 63/63 [00:00<00:00, 249991.63it/s]\n"," 50%|█████     | 2/4 [01:21<01:07, 33.70s/it]\n","100%|██████████| 63/63 [00:00<00:00, 275251.20it/s]\n"," 75%|███████▌  | 3/4 [01:23<00:19, 19.24s/it]\n","100%|██████████| 63/63 [00:00<00:00, 94170.05it/s]\n","100%|██████████| 4/4 [01:23<00:00, 20.93s/it]\n"]}]},{"cell_type":"code","metadata":{"id":"AD3TaUlo1gZY","executionInfo":{"status":"ok","timestamp":1638810118219,"user_tz":-540,"elapsed":599,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":["import os\n","import numpy as np\n","import torch\n","from PIL import Image\n","\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, transforms = None, mode = 'train', json_file = None):\n","        self.mode = mode\n","        self.root = root\n","        self.transforms = transforms\n","        self.json_file = json_file\n","        \n","    def __getitem__(self, idx):\n","        if self.mode == 'train':\n","\n","            annot = json_file[idx]\n","            # Train의 경로로 이동한다.\n","            # 나중에 진짜 데이터를 다룰 때 사용한다.\n","            # PATH = root + '/' + 'train'\n","\n","            with open(annot, 'r') as f:\n","                annot = json.loads(f.read())\n","                \n","            image_name = annot['images'][0]['file_name']\n","            dir = image_name.split('_')[0]\n","\n","            image_PATH = PATH + '/' + dir + '/' + image_name\n","            \n","            image = Image.open(image_PATH).convert('RGB')\n","\n","            boxes = []\n","            segmentations = []\n","            labels = []\n","\n","            for i in range(len(annot['annotations'])):\n","\n","                segmentation = annot['annotations'][i]['segmentation'][0]\n","                bbox = annot['annotations'][i]['bbox']\n","                label = torch.tensor(annot['annotations'][i]['category_id'], dtype = np.uint8)\n","                xmin, ymin, width, height = bbox[0],bbox[1],bbox[2],bbox[3]\n","                xmin, ymin, xmax, ymax = xmin, ymin, xmin + width, ymax + height\n","                \n","                boxes.append([xmin, ymin, xmax, ymax])\n","                segmentations.append([segmentation])\n","                labels.append(label)\n","\n","            target = {}\n","            target['boxes'] = boxes\n","            target['labels'] = labels\n","            target['segmentation'] = segmentations\n","            target['image_id'] = image_name\n","\n","            if self.transforms is not None:\n","                img, target = self.transforms(img, target)\n","\n","            return img, target\n","\n","        if self.mode == 'test':\n","            # Test의 경로로 이동한다.\n","            # PATH = root + '/' + 'test'\n","            # 나중에 진짜 데이터를 다룰 때 사용한다.\n","\n","            ##### 이 부분은 추후에 작성하기로 하자.\n","            with open(annot, 'r') as f:\n","                annot = json.loads(f.read())\n","\n","            image_name = annot['images'][0]['file_name']\n","            dir = image_name[:2]\n","\n","            image_PATH = PATH + '/' + dir + '/' + image_name\n","            image = Image.open(image_PATH).convert('RGB')\n","            target = {}\n","\n","            target['image_id'] = image_name\n","\n","            return image, target\n","\n","    def __len__(self):\n","        return len(self.json_file)"],"execution_count":183,"outputs":[]},{"cell_type":"code","metadata":{"id":"ypiLsatqlf2G","executionInfo":{"status":"ok","timestamp":1638810118230,"user_tz":-540,"elapsed":45,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":["root = '/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Sample_data'\n","train_dataset = CustomDataset(root = root,json_file = train, mode = 'train')\n","test_dataset = CustomDataset(root = root, json_file = test, mode = 'test')"],"execution_count":184,"outputs":[]},{"cell_type":"code","metadata":{"id":"LJ3DsAk1z8BE","executionInfo":{"status":"ok","timestamp":1638810118235,"user_tz":-540,"elapsed":49,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":["def collate_fn(batch):\n","    return tuple(zip(*batch))"],"execution_count":185,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WYX4u3AIe55I","executionInfo":{"status":"ok","timestamp":1638810118236,"user_tz":-540,"elapsed":47,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"9e8c4c10-ba01-431b-bd08-57ca49707d21"},"source":["train_data_loader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size = 5, shuffle = True, num_workers = 16, collate_fn = collate_fn\n",")"],"execution_count":186,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","metadata":{"id":"erABr7XNezob","executionInfo":{"status":"ok","timestamp":1638810118238,"user_tz":-540,"elapsed":28,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":["def get_instance_segmentation_model(num_classes):\n","\n","    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained = True)\n","\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n","\n","    return model"],"execution_count":187,"outputs":[]},{"cell_type":"code","metadata":{"id":"AN4mA14Igja6","executionInfo":{"status":"ok","timestamp":1638810120622,"user_tz":-540,"elapsed":2411,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# PP, PE, PET, PS, background\n","num_classes = 4 + 1\n","\n","model = get_instance_segmentation_model(num_classes)\n","\n","model.to(device)\n","\n","params = [p for p in model.parameters() if p.requires_grad]\n","\n","model.to(device)\n","optimizer = torch.optim.SGD(params, lr = 0.001, momentum = 0.9, weight_decay = 0.0005)\n","\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 3, gamma = 0.001)"],"execution_count":188,"outputs":[]},{"cell_type":"code","metadata":{"id":"FCLnggpzhTK6","executionInfo":{"status":"ok","timestamp":1638810121409,"user_tz":-540,"elapsed":823,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":["class SmoothedValue(object):\n","    \"\"\"Track a series of values and provide access to smoothed values over a\n","    window or the global series average.\n","    \"\"\"\n","\n","    def __init__(self, window_size=20, fmt=None):\n","        if fmt is None:\n","            fmt = \"{median:.4f} ({global_avg:.4f})\"\n","        self.deque = deque(maxlen=window_size)\n","        self.total = 0.0\n","        self.count = 0\n","        self.fmt = fmt\n","\n","    def update(self, value, n=1):\n","        self.deque.append(value)\n","        self.count += n\n","        self.total += value * n\n","\n","    def synchronize_between_processes(self):\n","        \"\"\"\n","        Warning: does not synchronize the deque!\n","        \"\"\"\n","        if not is_dist_avail_and_initialized():\n","            return\n","        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n","        dist.barrier()\n","        dist.all_reduce(t)\n","        t = t.tolist()\n","        self.count = int(t[0])\n","        self.total = t[1]\n","\n","    @property\n","    def median(self):\n","        d = torch.tensor(list(self.deque))\n","        return d.median().item()\n","\n","    @property\n","    def avg(self):\n","        d = torch.tensor(list(self.deque), dtype=torch.float32)\n","        return d.mean().item()\n","\n","    @property\n","    def global_avg(self):\n","        return self.total / self.count\n","\n","    @property\n","    def max(self):\n","        return max(self.deque)\n","\n","    @property\n","    def value(self):\n","        return self.deque[-1]\n","\n","    def __str__(self):\n","        return self.fmt.format(\n","            median=self.median,\n","            avg=self.avg,\n","            global_avg=self.global_avg,\n","            max=self.max,\n","            value=self.value)\n","    \n","class MetricLogger(object):\n","    def __init__(self, delimiter=\"\\t\"):\n","        self.meters = defaultdict(SmoothedValue)\n","        self.delimiter = delimiter\n","\n","    def update(self, **kwargs):\n","        for k, v in kwargs.items():\n","            if isinstance(v, torch.Tensor):\n","                v = v.item()\n","            assert isinstance(v, (float, int))\n","            self.meters[k].update(v)\n","\n","    def __getattr__(self, attr):\n","        if attr in self.meters:\n","            return self.meters[attr]\n","        if attr in self.__dict__:\n","            return self.__dict__[attr]\n","        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n","            type(self).__name__, attr))\n","\n","    def __str__(self):\n","        loss_str = []\n","        for name, meter in self.meters.items():\n","            loss_str.append(\n","                \"{}: {}\".format(name, str(meter))\n","            )\n","        return self.delimiter.join(loss_str)\n","\n","    def synchronize_between_processes(self):\n","        for meter in self.meters.values():\n","            meter.synchronize_between_processes()\n","\n","    def add_meter(self, name, meter):\n","        self.meters[name] = meter\n","\n","    def log_every(self, iterable, print_freq, header=None):\n","        i = 0\n","        if not header:\n","            header = ''\n","        start_time = time.time()\n","        end = time.time()\n","        iter_time = SmoothedValue(fmt='{avg:.4f}')\n","        data_time = SmoothedValue(fmt='{avg:.4f}')\n","        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n","        if torch.cuda.is_available():\n","            log_msg = self.delimiter.join([\n","                header,\n","                '[{0' + space_fmt + '}/{1}]',\n","                'eta: {eta}',\n","                '{meters}',\n","                'time: {time}',\n","                'data: {data}',\n","                'max mem: {memory:.0f}'\n","            ])\n","        else:\n","            log_msg = self.delimiter.join([\n","                header,\n","                '[{0' + space_fmt + '}/{1}]',\n","                'eta: {eta}',\n","                '{meters}',\n","                'time: {time}',\n","                'data: {data}'\n","            ])\n","        MB = 1024.0 * 1024.0\n","        for obj in iterable:\n","            data_time.update(time.time() - end)\n","            yield obj\n","            iter_time.update(time.time() - end)\n","            if i % print_freq == 0 or i == len(iterable) - 1:\n","                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n","                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n","                if torch.cuda.is_available():\n","                    print(log_msg.format(\n","                        i, len(iterable), eta=eta_string,\n","                        meters=str(self),\n","                        time=str(iter_time), data=str(data_time),\n","                        memory=torch.cuda.max_memory_allocated() / MB))\n","                else:\n","                    print(log_msg.format(\n","                        i, len(iterable), eta=eta_string,\n","                        meters=str(self),\n","                        time=str(iter_time), data=str(data_time)))\n","            i += 1\n","            end = time.time()\n","        total_time = time.time() - start_time\n","        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n","        print('{} Total time: {} ({:.4f} s / it)'.format(\n","            header, total_time_str, total_time / len(iterable)))\n","        \n","def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n","    def f(x):\n","        if x >= warmup_iters:\n","            return 1\n","        alpha = float(x) / warmup_iters\n","        return warmup_factor * (1 - alpha) + alpha\n","\n","    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n","\n","def reduce_dict(input_dict, average=True):\n","    \"\"\"\n","    Args:\n","        input_dict (dict): all the values will be reduced\n","        average (bool): whether to do average or sum\n","    Reduce the values in the dictionary from all processes so that all processes\n","    have the averaged results. Returns a dict with the same fields as\n","    input_dict, after reduction.\n","    \"\"\"\n","    world_size = get_world_size()\n","    if world_size < 2:\n","        return input_dict\n","    with torch.no_grad():\n","        names = []\n","        values = []\n","        # sort the keys so that they are consistent across processes\n","        for k in sorted(input_dict.keys()):\n","            names.append(k)\n","            values.append(input_dict[k])\n","        values = torch.stack(values, dim=0)\n","        dist.all_reduce(values)\n","        if average:\n","            values /= world_size\n","        reduced_dict = {k: v for k, v in zip(names, values)}\n","    return reduced_dict\n","\n","def get_world_size():\n","    if not is_dist_avail_and_initialized():\n","        return 1\n","    return dist.get_world_size()\n","\n","def is_dist_avail_and_initialized():\n","    if not dist.is_available():\n","        return False\n","    if not dist.is_initialized():\n","        return False\n","    return True"],"execution_count":189,"outputs":[]},{"cell_type":"code","metadata":{"id":"0W8F-hd2iZDL","executionInfo":{"status":"ok","timestamp":1638810121830,"user_tz":-540,"elapsed":15,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":["def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n","    model.train()\n","    metric_logger = MetricLogger(delimiter=\"  \")\n","    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n","    header = 'Epoch: [{}]'.format(epoch)\n","\n","    lr_scheduler = None\n","    if epoch == 0:\n","        warmup_factor = 1. / 1000\n","        warmup_iters = min(1000, len(data_loader) - 1)\n","\n","        lr_scheduler = warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n","\n","    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        loss_dict = model(images, targets)\n","\n","        losses = sum(loss for loss in loss_dict.values())\n","\n","        # reduce losses over all GPUs for logging purposes\n","        loss_dict_reduced = reduce_dict(loss_dict)\n","        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n","\n","        loss_value = losses_reduced.item()\n","\n","        if not math.isfinite(loss_value):\n","            print(\"Loss is {}, stopping training\".format(loss_value))\n","            print(loss_dict_reduced)\n","            sys.exit(1)\n","\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","        if lr_scheduler is not None:\n","            lr_scheduler.step()\n","\n","        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n","        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n","\n","    return metric_logger\n","\n","def _get_iou_types(model):\n","    model_without_ddp = model\n","    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n","        model_without_ddp = model.module\n","    iou_types = [\"bbox\"]\n","    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n","        iou_types.append(\"segm\")\n","    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n","        iou_types.append(\"keypoints\")\n","    return iou_types"],"execution_count":190,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":642},"id":"KoJmUo4YicGM","executionInfo":{"status":"error","timestamp":1638810123807,"user_tz":-540,"elapsed":1990,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}},"outputId":"80363f6e-1448-4078-89fb-f41dba23035a"},"source":["# let's train it for 10 epochs\n","from torch.optim.lr_scheduler import StepLR\n","num_epochs = 1\n","\n","for epoch in range(num_epochs):\n","    # train for one epoch, printing every 10 iterations\n","    train_one_epoch(model, optimizer, train_data_loader, device, epoch, print_freq=100)\n","    # update the learning rate\n","    lr_scheduler.step()\n","    # evaluate on the test dataset"],"execution_count":191,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-191-1bb4c3b80670>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-190-2c1ce05b943d>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwarmup_lr_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetric_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_every\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-189-4189ca09be52>\u001b[0m in \u001b[0;36mlog_every\u001b[0;34m(self, iterable, print_freq, header)\u001b[0m\n\u001b[1;32m    124\u001b[0m             ])\n\u001b[1;32m    125\u001b[0m         \u001b[0mMB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mdata_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-183-7e6a9d5f9128>\", line 16, in __getitem__\n    annot = json_file[idx]\nIndexError: string index out of range\n"]}]},{"cell_type":"code","metadata":{"id":"BCZ5eCUAidWZ","executionInfo":{"status":"aborted","timestamp":1638810123803,"user_tz":-540,"elapsed":537,"user":{"displayName":"박정열","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02719436629239662639"}}},"source":[""],"execution_count":null,"outputs":[]}]}