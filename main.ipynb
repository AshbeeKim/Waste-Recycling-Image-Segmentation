{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Packages Version Infos :\n",
            "\tjson\t:\t2.0.9\n",
            "\ttqdm\t:\t4.62.3\n",
            "\tnumpy\t:\t1.21.1\n",
            "\tcv2.cv2\t:\t4.5.3\n",
            "\tPIL.Image\t:\t8.4.0\n",
            "\tmatplotlib\t:\t3.5.0\n",
            "\tpycocotools.coco\t:\t2.0\n",
            "\ttorch\t:\t1.10.0\n",
            "\ttorchvision\t:\t0.11.1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# from google.colab import drive\n",
        "# from os.path import join as jn\n",
        "\n",
        "# # 구글 드라이브 접근\n",
        "# ROOT = \"/content/drive\"\n",
        "# try:\n",
        "#   drive.mount(ROOT, force_remount=True)\n",
        "# except:\n",
        "#   drive.mount(ROOT)\n",
        "ROOT = os.getcwd()\n",
        "\n",
        "import json\n",
        "import tqdm as root_tqdm\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import pycocotools.coco as pycocos\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch  import nn, Tensor\n",
        "\n",
        "import torchvision\n",
        "# 본인 경로에 맞게 수정하면 됨.\n",
        "# PATH = jn(ROOT, \"MyDrive/Task/plastic-segmentation/Data\")\n",
        "# PATH = jn(ROOT, \"MyDrive/Task/plastic-segmentation/Sample_data\")\n",
        "PATH = os.path.join(ROOT, \"assets/data\")\n",
        "# # 구글 드라이브 경로에서 '/content/sample_data/data\"로 복사 시\n",
        "# import shutil\n",
        "# shutil.copytree(PATH +\"/train\", \"./sample_data/data/train\")\n",
        "\n",
        "\n",
        "basic_packages = [json, root_tqdm, np, cv, Image, mpl, pycocos, torch, torchvision]\n",
        "print(\"Packages Version Infos :\")\n",
        "for pname in basic_packages:\n",
        "    try:\n",
        "        print(f\"\\t{pname.__name__}\\t:\\t{pname.__version__}\")\n",
        "    except:\n",
        "        print(f\"\\tmodule '{pname.__name__}' has no attribute '__version__'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# general flow\n",
        "\n",
        "* Dataset\n",
        "    * CustomDataset\n",
        "    * DataLoaader\n",
        "* Model\n",
        "    * CustomModel\n",
        "    * train\n",
        "    * inference\n",
        "* VSL\n",
        "    * predict(inference)\n",
        "    * output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "MCgcWgvEINNr"
      },
      "outputs": [],
      "source": [
        "# Mask RCNN 공식 홈페이지 Tutorial을 사용하여, 필요한 코드를 불러옵니다.\n",
        "# 해당 튜토리얼과 동일하게 상위 폴더로 가져와도 되고, 특정 폴더로 옮겨도 되지만 일단은 py를 ipynb에서 읽을 수 있도록 호출\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6RrxAFLKzKP"
      },
      "source": [
        "# check shape of datasets(\"PennFudanPed\")\n",
        "\n",
        "* tutorial까지 돌아간 코드에서 shape 확인하고 customize...근데 사용한 metric?module?이 다른 걸 보면 아예 데이터 입력 방식이 다른 것일 수도..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* torch.uint8 -> torch.ByteTensor\n",
        "* torch.float16 -> torch.HalfTensor\n",
        "* torch.float, torch.float32 -> torch.FloatTensor\n",
        "* torch.int16 -> torch.ShortTensor\n",
        "* torch.int, torch.int32 -> torch.IntTensor\n",
        "* torch.int64 -> torch.LongTensor\n",
        "* torch.float64 -> torch.DoubleTensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgGnwpYuSE9v",
        "outputId": "5d03949f-53e3-4e7c-ebfb-baa891f92fc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0번째 train 이미지\n",
            "\t 이미지 shape : torch.Size([3, 321, 398])\n",
            "\t 이미지 type : torch.FloatTensor\n",
            "\t\tdict_keys == 'boxes' :  \n",
            "\t\t\tshape : torch.Size([1, 4])\n",
            "\t\t\ttype : torch.FloatTensor\n",
            "\t\t\ttensor([[170.,  21., 319., 311.]])\n",
            "\t\tdict_keys == 'labels' :  \n",
            "\t\t\tshape : torch.Size([1])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([1])\n",
            "\t\tdict_keys == 'masks' :  \n",
            "\t\t\tshape : torch.Size([1, 321, 398])\n",
            "\t\t\ttype : torch.ByteTensor\n",
            "\t\tdict_keys == 'image_id' :  \n",
            "\t\t\tshape : torch.Size([1])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([155])\n",
            "\t\tdict_keys == 'area' :  \n",
            "\t\t\tshape : torch.Size([1])\n",
            "\t\t\ttype : torch.FloatTensor\n",
            "\t\t\ttensor([43210.])\n",
            "\t\tdict_keys == 'iscrowd' :  \n",
            "\t\t\tshape : torch.Size([1])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([0])\n"
          ]
        }
      ],
      "source": [
        "# print(f\"0번째 train 이미지\")\n",
        "# print(f\"\\t 이미지 shape : {dataset[0][0].shape}\\n\\t 이미지 type : {dataset[0][0].type()}\")\n",
        "# for k in list(dataset[0][1].keys()):\n",
        "#      print(f\"\\t\\tdict_keys == '{k}' :  \\n\\t\\t\\tshape : {dataset[0][1][k].shape}\\n\\t\\t\\ttype : {dataset[0][1][k].type()}\")\n",
        "#      if k is not \"masks\":\n",
        "#          print(f\"\\t\\t\\t{dataset[0][1][k]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nebLcBgLXPxL"
      },
      "source": [
        "masks를 확인하고, 다른 값들을 확인한 결과,,\n",
        "    * mask 생성 무조건 해야함!!!\n",
        "    * 형태(모델을 돌리는 방법)는 자유겠지만, 그 각각을 맞추려면 CustomDataset(default)에서 CustomDataset(multi class or process)로 진행되어야 함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OTzjuUAJ6vZ"
      },
      "source": [
        "* DataLoader\n",
        "\n",
        "operation이 어떻게 작용되는지 확인하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wbbcgOl0hLG"
      },
      "source": [
        "# check sorting path process\n",
        "\n",
        "```python\n",
        "# author : qkrwjdduf159 <pde159@naver.com>\n",
        "# version 1\n",
        "## train\n",
        "trn_path = f\"{PATH}/train/annotation\"\n",
        "file_list = os.listdir(trn_path)\n",
        "\n",
        "train_files = []\n",
        "for file in tqdm(file_list):\n",
        "    dir = trn_path + '/' + file\n",
        "    json_list = glob(dir + '/' + '*.json')\n",
        "    train_files.append(json_list)\n",
        "\n",
        "train_json_list = []\n",
        "for files in tqdm(train_files):\n",
        "    for json_file in tqdm(files):\n",
        "        train_json_list.append(json_file)\n",
        "\n",
        "## test\n",
        "tst_PATH = f\"{PATH}/test/annotations\"\n",
        "file_list = os.listdir(tst_PATH)\n",
        "\n",
        "test_files = []\n",
        "for file in tqdm(file_list):\n",
        "    dir = tst_PATH + '/' + file\n",
        "    json_list = glob(dir + '/' + '*.json')\n",
        "    test_files.append(json_list)\n",
        "\n",
        "test_json_list = []\n",
        "for files in tqdm(test_files):\n",
        "    for json_file in tqdm(files):\n",
        "        test_json_list.append(json_file)\n",
        "```\n",
        "\n",
        "\n",
        "```python\n",
        "# author : AshbeeKim <ksb.forest@gmail.com>\n",
        "# version 2\n",
        "trn = os.path.join(PATH, \"train\")\n",
        "tst = os.path.join(PATH, \"test\")\n",
        "\n",
        "baseDF = {\"kind\" : [], \"label\" : [], \"metainfo_id\" : [], \"feature\" : [], \"image_path\" : [], \"annot_path\": []}\n",
        "trn_num, tst_num = 0, 0\n",
        "for fpath in tqdm([trn, tst]):\n",
        "    kind = os.path.basename(fpath)\n",
        "    BDIR = sorted(os.listdir(fpath))    # image, annotation(s)\n",
        "    for bdir in BDIR:\n",
        "        for dirs in sorted(os.listdir(jn(fpath, bdir))):\n",
        "            dpath = jn(fpath, bdir, dirs)\n",
        "            if bdir.lower()=='image':\n",
        "                paths = sorted(glob(dpath + \"/*.jpg\"))\n",
        "                baseDF['image_path'].extend(paths)\n",
        "                kinds = [kind for cnt in range(len(paths))]\n",
        "                baseDF['kind'].extend(kinds)\n",
        "                if kind=='train' : trn_num += len(paths)\n",
        "                elif kind=='test' : tst_num += len(paths)\n",
        "                fnames = [os.path.basename(fname) for fname in paths]\n",
        "                labels = list(map(lambda x: (x.split('_')[0]), fnames))\n",
        "                baseDF['label'].extend(labels)\n",
        "                metaIds = list(map(lambda x: int(x.split('_')[1]), fnames))\n",
        "                baseDF['metainfo_id'].extend(metaIds)\n",
        "                feats = list(map(lambda x: int(x.split('_')[-1][:-4]), fnames))\n",
        "                baseDF['feature'].extend(feats)\n",
        "            else:\n",
        "                paths = sorted(glob(dpath + \"/*.json\"))\n",
        "                baseDF['annot_path'].extend(paths)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awosI9-Sm-Oz"
      },
      "source": [
        "## Fix Seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ggw2ucDIe7CU"
      },
      "outputs": [],
      "source": [
        "# cotributed by @qkrwjdduf156, @AshbeeKim\n",
        "# import libraries\n",
        "\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "from glob import glob\n",
        "# from tqdm.notebook import tqdm\n",
        "from tqdm import tqdm\n",
        "\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torchvision.transforms import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yPHxCQsQsF6_"
      },
      "outputs": [],
      "source": [
        "# Fix randomness\n",
        "# * ref : https://www.kaggle.com/rluethy/sartorius-torch-mask-r-cnn\n",
        "\n",
        "def fix_all_seeds(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "fix_all_seeds(513610)\n",
        "\n",
        "data_directory = PATH\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "BATCH_SIZE = 1\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Reduced the train dataset to 5000 rows\n",
        "TEST = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcor3OIe0kUX"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "masks를 확인하고, 다른 값들을 확인한 결과,, \n",
        "* mask 생성 무조건 해야함!!! \n",
        "* 형태(모델을 돌리는 방법)는 자유겠지만, 그 각각을 맞추려면 CustomDataset(default)에서 CustomDataset(multi class or process)로 진행되어야 함.\n",
        "\n",
        "```python\n",
        "0번째 train 이미지\n",
        "\t 이미지 shape : torch.Size([3, 321, 398])\n",
        "\t 이미지 type : torch.FloatTensor\n",
        "\t\tdict_keys == 'boxes' :  \n",
        "\t\t\tshape : torch.Size([1, 4])\n",
        "\t\t\ttype : torch.FloatTensor\n",
        "\t\t\ttensor([[170.,  21., 319., 311.]])\n",
        "\t\tdict_keys == 'labels' :  \n",
        "\t\t\tshape : torch.Size([1])\n",
        "\t\t\ttype : torch.LongTensor\n",
        "\t\t\ttensor([1])\n",
        "\t\tdict_keys == 'masks' :  \n",
        "\t\t\tshape : torch.Size([1, 321, 398])\n",
        "\t\t\ttype : torch.ByteTensor\n",
        "\t\tdict_keys == 'image_id' :  \n",
        "\t\t\tshape : torch.Size([1])\n",
        "\t\t\ttype : torch.LongTensor\n",
        "\t\t\ttensor([155])\n",
        "\t\tdict_keys == 'area' :  \n",
        "\t\t\tshape : torch.Size([1])\n",
        "\t\t\ttype : torch.FloatTensor\n",
        "\t\t\ttensor([43210.])\n",
        "\t\tdict_keys == 'iscrowd' :  \n",
        "\t\t\tshape : torch.Size([1])\n",
        "\t\t\ttype : torch.LongTensor\n",
        "\t\t\ttensor([0])\n",
        "\n",
        "# torch.uint8 -> torch.ByteTensor\n",
        "# torch.float16 -> torch.HalfTensor\n",
        "# torch.float, torch.float32 -> torch.FloatTensor\n",
        "# torch.int16 -> torch.ShortTensor\n",
        "# torch.int, torch.int32 -> torch.IntTensor\n",
        "# torch.int64 -> torch.LongTensor\n",
        "# torch.float64 -> torch.DoubleTensor\n",
        "\n",
        "# list -> tensor ; torch.as_tensor\n",
        "# list\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smHx_S4Km28-"
      },
      "source": [
        "## CustomDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOLenWki-BzB"
      },
      "source": [
        "image segmentation을 위해선 target[boxes, labels, masks] 필수\n",
        "- 통상 \"image_id\", 마스크가 속한 이미지 id\n",
        "- \"area\", box 면적, pred box, act box IoU 구할 때 필요\n",
        "- \"iscrowd\", 작은 물체를 하나의 군집으로 박스처리하여 레이블링 했는지에 관한 여부, 물체가 숨어져있거나, 가려진 경우 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "OYMFOhWxz2_O"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root, transforms = None, target_transforms = None, mode = 'train'):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.target_transforms = target_transforms\n",
        "        self.mode = mode.lower()\n",
        "        self.infos = self.baseInfos()\n",
        "        self.imgs, self.annots = self.infos['image_path'], self.infos['annot_path']\n",
        "        # if self.mode == 'train':\n",
        "        #     self.imgs, self.masks = blahblah\n",
        "        self.masks = None\n",
        "\n",
        "    def __getitem__(self, idx=None):\n",
        "        '''return tuple(image, target)'''\n",
        "        annot_path = self.annots[idx]\n",
        "        mask_shape, target = self.json2annots(annot_path, idx)\n",
        "        # target : dict\n",
        "\n",
        "        image_path = self.imgs[idx]\n",
        "        img, background = self.imageNmask(image_path, mask_shape)\n",
        "        \n",
        "        if self.mode == \"train\":\n",
        "            # train인 경우에만 target['masks'] 처리할 함수가 들어가야 함.\n",
        "            target['masks'] = self.createMask(background, target['masks'])\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            try:\n",
        "                data = self.transforms(img, target)\n",
        "                return data\n",
        "            except:\n",
        "                if self.transforms is not None:\n",
        "                    img = self.transforms(img)\n",
        "                elif self.target_transforms is not None:\n",
        "                    target = self.target_transforms(target)\n",
        "                return img, target\n",
        "        else:\n",
        "            img = self.preprocess(img)\n",
        "            return img, target\n",
        "\n",
        "    def baseInfos(self):\n",
        "        baseinfos = {\"label\" : [], \"metainfo_id\" : [], \"feature\" : [], \"image_path\" : [], \"annot_path\": []}\n",
        "        \n",
        "        bpath = os.path.join(self.root, self.mode)\n",
        "        bdirs = sorted(os.listdir(bpath))   # image, annotation(s)\n",
        "        \n",
        "        for bdir in tqdm(bdirs):\n",
        "            for dirs in sorted(os.listdir(os.path.join(bpath, bdir))):\n",
        "                dpath = os.path.join(bpath, bdir, dirs)\n",
        "                \n",
        "                if bdir.lower()=='image':\n",
        "                    paths = sorted(glob(dpath + \"/*.jpg\"))\n",
        "                    baseinfos['image_path'].extend(paths)\n",
        "                    fnames = [os.path.basename(fname) for fname in paths]\n",
        "                    labels = list(map(lambda x: (x.split('_')[0]), fnames))\n",
        "                    baseinfos['label'].extend(labels)\n",
        "                    metaIds = list(map(lambda x: int(x.split('_')[1]), fnames))\n",
        "                    baseinfos['metainfo_id'].extend(metaIds)\n",
        "                    feats = list(map(lambda x: int(x.split('_')[-1][:-4]), fnames))\n",
        "                    baseinfos['feature'].extend(feats)\n",
        "                else:\n",
        "                    paths = sorted(glob(dpath + \"/*.json\"))\n",
        "                    baseinfos['annot_path'].extend(paths)\n",
        "        return baseinfos\n",
        "    \n",
        "    def imageNmask(self, image_path, mask_shape):\n",
        "        image = cv.imread(image_path)\n",
        "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)    # [H, W, C]\n",
        "        # mask = np.zeros(image.shape, dtype=np.uint8)    # [H, W, C] >> [N+1, H, W, C] >> [N, H, W]\n",
        "        masks = np.zeros(mask_shape, dtype = np.uint8)  \n",
        "        # 굳이 3채널 정보가 필요하지 않다면, (H, W) 만 받아도, instance 공간 할당은 이미 해놓음\n",
        "        # mask 생성 함수보고 고쳐야 할 듯\n",
        "        image = Image.fromarray(image)  # cv to PIL\n",
        "        # image = torchvision.transforms(image)    # [C, H, W] \n",
        "        return image, masks\n",
        "\n",
        "    def preprocess(self, img):\n",
        "        image = img\n",
        "        import torchvision.transforms as T\n",
        "        m, s = np.mean(image, axis = (0, 1)), np.std(image, axis = (0, 1))        \n",
        "        if self.mode == 'train':\n",
        "            transform = T.Compose([\n",
        "                                   T.ToTensor(),\n",
        "                                   T.Normalize(mean = m, std = s),\n",
        "            ])\n",
        "            image = transform(image)\n",
        "        else:\n",
        "            transform = T.Compose([\n",
        "                                   T.Resize(256),\n",
        "                                   T.ToTensor(),\n",
        "                                   T.Normalize(mean = m, std = s),\n",
        "            ])\n",
        "            image = transform(image)\n",
        "        return image\n",
        "    \n",
        "    def json2annots(self, annot_path, idx):\n",
        "        with open(annot_path, 'r') as f:\n",
        "            annot = json.loads(f.read())\n",
        "\n",
        "        n_objects = len(annot['annotations'])\n",
        "        W, H = int(annot['images'][0]['width']), int(annot['images'][0]['height'])\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            # target = {\"boxes\" : [], \"labels\" : [], \"masks\" : [np.zeros((W, H), dtype = np.uint8)], \"image_id\" : [], \"area\" : [], \"iscrowd\" : []}\n",
        "            target = {\"boxes\" : [], \"labels\" : [], \"masks\" : [], \"image_id\" : [], \"area\" : [], \"iscrowd\" : []}\n",
        "\n",
        "            # bbox ground bbox\n",
        "            gxmin, gymin, gxmax, gymax = 0, 0, 0, 0\n",
        "            for i in range(n_objects):\n",
        "                bbox = annot['annotations'][i]['bbox']\n",
        "                xmin, ymin, width, height = bbox[0],bbox[1],bbox[2],bbox[3]\n",
        "                xmax, ymax = xmin + width, ymin + height\n",
        "                target['boxes'].append([xmin, ymin, xmax, ymax])\n",
        "                # ########## just-in-case(ground_bbox) ##########\n",
        "                # gxin = xmin if xmin > gxmin else gxmin\n",
        "                # gymin = ymin if ymin > gymin else gymin\n",
        "                # gmax = xmax if xmax > gxmax else gxmax\n",
        "                # gmax = ymax if ymax > gymax else gymax\n",
        "                # target['boxes'].append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "                label = annot['annotations'][i]['category_id']\n",
        "                # torch.ones로 하지 않은 이유는 클래스 정보가 들어가야된다고 판단했기 때문\n",
        "                target['labels'].append([label])\n",
        "\n",
        "                target['masks'].append(np.zeros((W, H), dtype=np.uint8))\n",
        "                # mask 처리 전, 공간 할당\n",
        "\n",
        "                area = torch.tensor(annot['annotations'][i]['area'], dtype = torch.float32)\n",
        "                target['area'].append([area])\n",
        "\n",
        "                iscrowd = torch.tensor(annot['annotations'][i]['iscrowd'], dtype = torch.int64)\n",
        "                target['iscrowd'].append([iscrowd])\n",
        "                # ignore = torch.tensor(annot['annotations'][i]['ignore'], dtype = torch.int64)\n",
        "                # target['ignore'].append([ignore])\n",
        "\n",
        "            ########################################################## instance id, 필요 시 주석해제 ##########################################################    \n",
        "            # segmentation에 DataLoader 내에서 어떻게 선언되어 있는지 확인해야 함\n",
        "            # target['id'] = torch.stack([torch.tensor([i+1]) for i in range(n_objects)]) # shape : (3, 1)\n",
        "            # ValueError: Expected target boxes to be a tensorof shape [N, 4], got torch.Size([1, 3, 4]).\n",
        "            ########################################################## type은 맞으나, shape이 맞는지 확신이 없는 부분 ##########################################################\n",
        "            # torch.Size([3, 2048, 2048])\n",
        "            # target['boxes'] = torch.as_tensor(target['boxes'], dtype = torch.float32)   \n",
        "            # shape : (3, 4)\n",
        "            target['boxes'] = torch.stack([torch.tensor(bbox, dtype = torch.float32) for bbox in target['boxes']])\n",
        "\n",
        "            # shape : torch.Size([1, 3, 1])\n",
        "            target['labels'] = torch.as_tensor((target['labels'], ), dtype = torch.int64).squeeze()\n",
        "            \n",
        "            # shape : torch.Size([1, 4, 2048, 2048])\n",
        "            target['masks'] = torch.as_tensor(target['masks'], dtype = torch.uint8)\n",
        "            # mssk 처리 전, 공간 할당\n",
        "\n",
        "            ########################################################## below ~~ torch.Size([3, 1]) : 각각의 shape도 type도 맞는 것 같음 ##########################################################\n",
        "            # image_path[index] -> 이미지 자체의 아이디\n",
        "            target['image_id'] = torch.stack([torch.tensor([idx]) for i in range(n_objects)])   # shape : (3, 1)\n",
        "            target['area'] = torch.as_tensor(target['area'], dtype = torch.float32) # shape : (1, 3) >> (3, 1)\n",
        "            target['iscrowd'] = torch.as_tensor(target['iscrowd'], dtype = torch.int64)\n",
        "            # target['ignore'] = torch.as_tensor(target['ignore'], dtype = torch.int64)\n",
        "\n",
        "\n",
        "\n",
        "        else:\n",
        "            target = {}\n",
        "            try:\n",
        "                target['image_id'] = torch.stack([torch.tensor([idx]) for i in range(n_objects)], dtype = torch.int64)\n",
        "            except:\n",
        "                target['image_id'] = torch.tensor([idx], dtype=torch.int)\n",
        "        return (n_objects, W, H), target\n",
        "\n",
        "    def createMask(self, background, mask):\n",
        "        # bbox\n",
        "\n",
        "        # segmentation\n",
        "\n",
        "        return mask\n",
        "\n",
        "    \n",
        "    def _labels2category(self, label):\n",
        "        category = {1 : \"PET\", 2 : \"PS\", 3 : \"PP\", 4 : \"PE\"}\n",
        "        return category(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    # ########## just-in-case ##########\n",
        "    # # when we need more edas\n",
        "    # def additional_infos(self):\n",
        "    #     with open(self.annots[0], 'r') as f:\n",
        "    #         annot = json.loads(f.read())\n",
        "    #     metainfo = annot['metainfo']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9tAnzXD_whQ",
        "outputId": "182ad4b2-7875-4ded-9a75-9bcc0d3c680a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 18.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2142번째 train 이미지\n",
            "\t 이미지 shape : torch.Size([3, 2048, 2048])\n",
            "\t 이미지 type : torch.FloatTensor\n",
            "\t\tdict_keys == 'boxes' :  \n",
            "\t\t\tshape : torch.Size([3, 4])\n",
            "\t\t\ttype : torch.FloatTensor\n",
            "\t\t\ttensor([[ 824.,  457., 1204.,  852.],\n",
            "        [ 394.,  979.,  778., 1364.],\n",
            "        [1009., 1128., 1354., 1469.]])\n",
            "\t\tdict_keys == 'labels' :  \n",
            "\t\t\tshape : torch.Size([3])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([3, 3, 3])\n",
            "\t\tdict_keys == 'masks' :  \n",
            "\t\t\tshape : torch.Size([3, 2048, 2048])\n",
            "\t\t\ttype : torch.ByteTensor\n",
            "\t\tdict_keys == 'image_id' :  \n",
            "\t\t\tshape : torch.Size([3, 1])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([[2142],\n",
            "        [2142],\n",
            "        [2142]])\n",
            "\t\tdict_keys == 'area' :  \n",
            "\t\t\tshape : torch.Size([3, 1])\n",
            "\t\t\ttype : torch.FloatTensor\n",
            "\t\t\ttensor([[117983.5000],\n",
            "        [116196.5000],\n",
            "        [ 91739.0000]])\n",
            "\t\tdict_keys == 'iscrowd' :  \n",
            "\t\t\tshape : torch.Size([3, 1])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([[0],\n",
            "        [0],\n",
            "        [0]])\n"
          ]
        }
      ],
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "dataset = CustomDataset(PATH)\n",
        "\n",
        "idx = random.randint(0, dataset.__len__())\n",
        "\n",
        "print(f\"{idx}번째 train 이미지\")\n",
        "\n",
        "# C, W, H, N\n",
        "# \n",
        "# num_classes, W, H, Batch(num_instance + 1) : mask\n",
        "print(f\"\\t 이미지 shape : {dataset[idx][0].shape}\\n\\t 이미지 type : {dataset[idx][0].type()}\")\n",
        "for k in list(dataset[idx][1].keys()):\n",
        "     print(f\"\\t\\tdict_keys == '{k}' :  \\n\\t\\t\\tshape : {dataset[idx][1][k].shape}\\n\\t\\t\\ttype : {dataset[idx][1][k].type()}\")\n",
        "     if k is not \"masks\":\n",
        "         print(f\"\\t\\t\\t{dataset[idx][1][k]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydDuYh3M0JsA",
        "outputId": "7d8ba96d-8084-4250-a759-1f12e720f7ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 19.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2998번째 train 이미지\n",
            "\t 이미지 shape : torch.Size([3, 2048, 2048])\n",
            "\t 이미지 type : torch.FloatTensor\n",
            "\t\tdict_keys == 'boxes' :  \n",
            "\t\t\tshape : torch.Size([2, 4])\n",
            "\t\t\ttype : torch.FloatTensor\n",
            "\t\t\ttensor([[ 425.,  638.,  902., 1261.],\n",
            "        [1278.,  538., 1670., 1218.]])\n",
            "\t\tdict_keys == 'labels' :  \n",
            "\t\t\tshape : torch.Size([2])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([3, 3])\n",
            "\t\tdict_keys == 'masks' :  \n",
            "\t\t\tshape : torch.Size([2, 2048, 2048])\n",
            "\t\t\ttype : torch.ByteTensor\n",
            "\t\tdict_keys == 'image_id' :  \n",
            "\t\t\tshape : torch.Size([2, 1])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([[2998],\n",
            "        [2998]])\n",
            "\t\tdict_keys == 'area' :  \n",
            "\t\t\tshape : torch.Size([2, 1])\n",
            "\t\t\ttype : torch.FloatTensor\n",
            "\t\t\ttensor([[172804.],\n",
            "        [220773.]])\n",
            "\t\tdict_keys == 'iscrowd' :  \n",
            "\t\t\tshape : torch.Size([2, 1])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([[0],\n",
            "        [0]])\n"
          ]
        }
      ],
      "source": [
        "%timeit\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "dataset = CustomDataset(PATH)\n",
        "\n",
        "idx = random.randint(0, dataset.__len__())\n",
        "\n",
        "print(f\"{idx}번째 train 이미지\")\n",
        "\n",
        "# C, W, H, N\n",
        "# \n",
        "# num_classes, W, H, Batch(num_instance + 1) : mask\n",
        "print(f\"\\t 이미지 shape : {dataset[idx][0].shape}\\n\\t 이미지 type : {dataset[idx][0].type()}\")\n",
        "for k in list(dataset[idx][1].keys()):\n",
        "     print(f\"\\t\\tdict_keys == '{k}' :  \\n\\t\\t\\tshape : {dataset[idx][1][k].shape}\\n\\t\\t\\ttype : {dataset[idx][1][k].type()}\")\n",
        "     if k is not \"masks\":\n",
        "         print(f\"\\t\\t\\t{dataset[idx][1][k]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoGFZFStoV3o"
      },
      "source": [
        "## DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "qfQJOcqfbrXQ"
      },
      "outputs": [],
      "source": [
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=utils.collate_fn)\n",
        "\n",
        "# train\n",
        "images,targets = next(iter(data_loader))\n",
        "images = list(image for image in images)\n",
        "targets = [{k: v for k, v in t.items()} for t in targets]\n",
        "output = model(images,targets)   # Returns losses and detections\n",
        "\n",
        "# inference\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)           # Returns predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nskPrwrxoYqE",
        "outputId": "10d75ee2-9626-470b-db7d-65e3a9a50aef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 18.32it/s]\n"
          ]
        }
      ],
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "dataset = CustomDataset(PATH)\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=utils.collate_fn)\n",
        "\n",
        "# train\n",
        "images,targets = next(iter(data_loader))\n",
        "images = list(image for image in images)\n",
        "targets = [{k: v for k, v in t.items()} for t in targets]\n",
        "output = model(images,targets)   # Returns losses and detections\n",
        "\n",
        "# inference\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)           # Returns predictions\n",
        "\n",
        "# /usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
        "#   return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIErSrcwdHhp",
        "outputId": "a0efc204-a373-484b-e1f0-f31bc6c907e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([], dtype=torch.int64),\n",
              "  'scores': tensor([], grad_fn=<IndexBackward0>)},\n",
              " {'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([], dtype=torch.int64),\n",
              "  'scores': tensor([], grad_fn=<IndexBackward0>)}]"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XljAkERh8ycX"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "h-ssFJSNonw6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "\n",
        "def quick_check(dataset, idx):\n",
        "    print(f\"{idx}번째 {dataset.mode} 이미지\")\n",
        "    print(f\"\\t 이미지 shape : {dataset[idx][0].shape}\\n\\t 이미지 type : {dataset[idx][0].type()}\")\n",
        "    for k in list(dataset[idx][1].keys()):\n",
        "        print(f\"\\t\\tdict_keys == '{k}' :  \\n\\t\\t\\tshape : {dataset[idx][1][k].shape}\\n\\t\\t\\ttype : {dataset[idx][1][k].type()}\")\n",
        "        if k is not \"masks\":\n",
        "            print(f\"\\t\\t\\t{dataset[idx][1][k]}\\n\\n\")\n",
        "\n",
        "def uni_inference(precision, idx):\n",
        "    print(f\"inference no.{idx} : \")\n",
        "    for k in list(precision[0].keys()):\n",
        "        print(f\"\\t{k} : \\n\\t\\t shape : {precision[0][k].shape}\\n\")\n",
        "\n",
        "# fastrcnn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# unet ; torch.hub\n",
        "\n",
        "# fasterrcnn\n",
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "# %%writefile ./CaliberDetector.py    # customize 끝내고 run\n",
        "\n",
        "# detection\n",
        "from collections import OrderedDict\n",
        "from torch import nn\n",
        "from torchvision.ops import MultiScaleRoIAlign\n",
        "from torchvision._internally_replaced_utils import load_state_dict_from_url\n",
        "from torchvision.models.detection._utils import overwrite_eps\n",
        "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone, _validate_trainable_layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQaZC_unzKX2"
      },
      "source": [
        "## briefly check models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5_cZc_96kJW"
      },
      "source": [
        "### * fastrcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "k3Ncq75MYFD3"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True)\n",
        "\n",
        "num_classes = 5 # number_of_class + background\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6ej_R1v1loS",
        "outputId": "86e76a83-cf0b-470b-a184-59962b5b3c5a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=20, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgWAY6VWz-gF",
        "outputId": "5891bd25-e985-4cc3-982a-159e3a3854f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 17.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "416번째 train 이미지\n",
            "\t 이미지 shape : torch.Size([3, 2048, 2048])\n",
            "\t 이미지 type : torch.FloatTensor\n",
            "\t\tdict_keys == 'boxes' :  \n",
            "\t\t\tshape : torch.Size([3, 4])\n",
            "\t\t\ttype : torch.FloatTensor\n",
            "\t\t\ttensor([[1321.,  953., 1538., 1245.],\n",
            "        [ 614.,  872.,  845., 1097.],\n",
            "        [1154.,  438., 1363.,  632.]])\n",
            "\t\tdict_keys == 'labels' :  \n",
            "\t\t\tshape : torch.Size([3])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([4, 4, 4])\n",
            "\t\tdict_keys == 'masks' :  \n",
            "\t\t\tshape : torch.Size([3, 2048, 2048])\n",
            "\t\t\ttype : torch.ByteTensor\n",
            "\t\tdict_keys == 'image_id' :  \n",
            "\t\t\tshape : torch.Size([3, 1])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([[416],\n",
            "        [416],\n",
            "        [416]])\n",
            "\t\tdict_keys == 'area' :  \n",
            "\t\t\tshape : torch.Size([3, 1])\n",
            "\t\t\ttype : torch.FloatTensor\n",
            "\t\t\ttensor([[48670.5000],\n",
            "        [32862.0000],\n",
            "        [34117.0000]])\n",
            "\t\tdict_keys == 'iscrowd' :  \n",
            "\t\t\tshape : torch.Size([3, 1])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([[0],\n",
            "        [0],\n",
            "        [0]])\n"
          ]
        }
      ],
      "source": [
        "dataset = CustomDataset(PATH)\n",
        "\n",
        "idx = random.randint(0, dataset.__len__())\n",
        "\n",
        "print(f\"\\n\\n{idx}번째 train 이미지\")\n",
        "\n",
        "# C, W, H, N\n",
        "# \n",
        "# num_classes, W, H, Batch(num_instance + 1) : mask\n",
        "print(f\"\\t 이미지 shape : {dataset[idx][0].shape}\\n\\t 이미지 type : {dataset[idx][0].type()}\")\n",
        "for k in list(dataset[idx][1].keys()):\n",
        "     print(f\"\\t\\tdict_keys == '{k}' :  \\n\\t\\t\\tshape : {dataset[idx][1][k].shape}\\n\\t\\t\\ttype : {dataset[idx][1][k].type()}\")\n",
        "     if k is not \"masks\":\n",
        "         print(f\"\\t\\t\\t{dataset[idx][1][k]}\")\n",
        "\n",
        "img, _ = dataset.__getitem__(idx)\n",
        "input_batch = img.unsqueeze(0)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "precision = model(input_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU5R5_oD321i",
        "outputId": "9eaa3fde-6a0c-4878-e390-9bb885cafb0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train 데이터[416]로 inference한 결과 : \n",
            "\tboxes : \n",
            "\t\t shape : torch.Size([100, 4])\n",
            "\tlabels : \n",
            "\t\t shape : torch.Size([100])\n",
            "\tscores : \n",
            "\t\t shape : torch.Size([100])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([100, 4])"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"\\n\\nTrain 데이터[{idx}]로 inference한 결과 : \")\n",
        "for k in list(precision[0].keys()):\n",
        "    print(f\"\\t{k} : \\n\\t\\t shape : {precision[0][k].shape}\\n\\t\\t mean : {precision[0][k].mean\")\n",
        "\n",
        "precision[0]['boxes'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyjqHOKY6dwg"
      },
      "source": [
        "### * unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-BQ5fZR4qw4",
        "outputId": "b8977d62-ff41-43c7-887e-e45180c78e0e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/mateuszbuda/brain-segmentation-pytorch/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "Downloading: \"https://github.com/mateuszbuda/brain-segmentation-pytorch/releases/download/v1.0/unet-e012d006.pt\" to /root/.cache/torch/hub/checkpoints/unet-e012d006.pt\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "\n",
        "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
        "    in_channels=3, out_channels=1, init_features=32, pretrained=True)\n",
        "# model = torch.hub._load_local()   # local dire에서 hubconf에 따라 사용이 가능하다고 함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzYsvQhIBS7g",
        "outputId": "818c5dbf-4b0b-46d2-ac8a-9695b602132c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (encoder1): Sequential(\n",
              "    (enc1conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc1relu1): ReLU(inplace=True)\n",
              "    (enc1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc1relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (encoder2): Sequential(\n",
              "    (enc2conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc2relu1): ReLU(inplace=True)\n",
              "    (enc2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc2relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (encoder3): Sequential(\n",
              "    (enc3conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc3relu1): ReLU(inplace=True)\n",
              "    (enc3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc3relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (encoder4): Sequential(\n",
              "    (enc4conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc4relu1): ReLU(inplace=True)\n",
              "    (enc4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc4relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (bottleneck): Sequential(\n",
              "    (bottleneckconv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bottlenecknorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bottleneckrelu1): ReLU(inplace=True)\n",
              "    (bottleneckconv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bottlenecknorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bottleneckrelu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (upconv4): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (decoder4): Sequential(\n",
              "    (dec4conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec4relu1): ReLU(inplace=True)\n",
              "    (dec4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec4relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (decoder3): Sequential(\n",
              "    (dec3conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec3relu1): ReLU(inplace=True)\n",
              "    (dec3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec3relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (upconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (decoder2): Sequential(\n",
              "    (dec2conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec2relu1): ReLU(inplace=True)\n",
              "    (dec2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec2relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (upconv1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (decoder1): Sequential(\n",
              "    (dec1conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec1relu1): ReLU(inplace=True)\n",
              "    (dec1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec1relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOYIk_sZru0R",
        "outputId": "0567dc6b-49cf-48a6-f863-3ce457eb6804"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 18.93it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 107.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Datasets\n",
            "232번째 train 이미지\n",
            "\t 이미지 shape : torch.Size([3, 2048, 2048])\n",
            "\t 이미지 type : torch.FloatTensor\n",
            "\t\tdict_keys == 'boxes' :  \n",
            "\t\t\tshape : torch.Size([3, 4])\n",
            "\t\t\ttype : torch.FloatTensor\n",
            "\t\t\ttensor([[1064.,  496., 1285.,  676.],\n",
            "        [1389.,  897., 1550., 1107.],\n",
            "        [ 679.,  799.,  835., 1020.]])\n",
            "\n",
            "\n",
            "\t\tdict_keys == 'labels' :  \n",
            "\t\t\tshape : torch.Size([3])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([4, 4, 4])\n",
            "\n",
            "\n",
            "\t\tdict_keys == 'masks' :  \n",
            "\t\t\tshape : torch.Size([3, 2048, 2048])\n",
            "\t\t\ttype : torch.ByteTensor\n",
            "\t\tdict_keys == 'image_id' :  \n",
            "\t\t\tshape : torch.Size([3, 1])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([[232],\n",
            "        [232],\n",
            "        [232]])\n",
            "\n",
            "\n",
            "\t\tdict_keys == 'area' :  \n",
            "\t\t\tshape : torch.Size([3, 1])\n",
            "\t\t\ttype : torch.FloatTensor\n",
            "\t\t\ttensor([[25640.5000],\n",
            "        [25074.5000],\n",
            "        [29585.0000]])\n",
            "\n",
            "\n",
            "\t\tdict_keys == 'iscrowd' :  \n",
            "\t\t\tshape : torch.Size([3, 1])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([[0],\n",
            "        [0],\n",
            "        [0]])\n",
            "\n",
            "\n",
            "\n",
            "Test Datasets\n",
            "393번째 test 이미지\n",
            "\t 이미지 shape : torch.Size([3, 256, 256])\n",
            "\t 이미지 type : torch.FloatTensor\n",
            "\t\tdict_keys == 'image_id' :  \n",
            "\t\t\tshape : torch.Size([1])\n",
            "\t\t\ttype : torch.IntTensor\n",
            "\t\t\ttensor([393], dtype=torch.int32)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_dataset = CustomDataset(PATH, mode='train')\n",
        "test_dataset = CustomDataset(PATH, mode='test')\n",
        "\n",
        "train_idx = random.randint(0, train_dataset.__len__())\n",
        "test_idx = random.randint(0, test_dataset.__len__())\n",
        "\n",
        "print(\"\\nTrain Datasets\")\n",
        "quick_check(train_dataset, train_idx)\n",
        "print(\"\\nTest Datasets\")\n",
        "quick_check(test_dataset, test_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "HNHMZVJuocSJ"
      },
      "outputs": [],
      "source": [
        "img, _ = train_dataset.__getitem__(train_idx)\n",
        "input_batch = img.unsqueeze(0)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_9-SYkNss3M",
        "outputId": "28cad630-719a-413a-db87-e9b4c53c6575"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[1.6307e-05, 1.5145e-05, 1.5356e-05,  ..., 1.6836e-05,\n",
              "           1.5657e-05, 1.4536e-05],\n",
              "          [2.0911e-05, 2.0291e-05, 2.0425e-05,  ..., 1.6934e-05,\n",
              "           1.1150e-05, 1.5441e-05],\n",
              "          [1.9988e-05, 3.1703e-05, 2.4566e-05,  ..., 1.7147e-05,\n",
              "           1.6356e-05, 1.2977e-05],\n",
              "          ...,\n",
              "          [2.0111e-05, 2.4908e-05, 2.0760e-05,  ..., 1.2605e-05,\n",
              "           1.1867e-05, 1.0306e-05],\n",
              "          [1.9120e-05, 2.7806e-05, 2.3741e-05,  ..., 1.6201e-05,\n",
              "           1.1835e-05, 1.3853e-05],\n",
              "          [1.2632e-05, 1.8156e-05, 2.1674e-05,  ..., 1.8309e-05,\n",
              "           1.4875e-05, 1.0430e-05]]]], device='cuda:0')"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpuwfmRns1m4",
        "outputId": "5cb80b42-e899-4366-f4b8-94f8f5ea56f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(torch.round(output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71VH86Ais4R6",
        "outputId": "aef2de4e-fb16-4ddf-92d3-55b00e47af2b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 2048, 2048])"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "0NVHSd7WtB9K"
      },
      "outputs": [],
      "source": [
        "img, _ = test_dataset.__getitem__(test_idx)\n",
        "\n",
        "input_batch = img.unsqueeze(0)\n",
        "\n",
        "model.eval()\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "confidences = model(input_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsbEFJfIu-14",
        "outputId": "ea0c640c-72e8-46cc-f16f-6f159e393d4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[1.5116e-05, 1.5655e-05, 1.6071e-05,  ..., 1.5135e-05,\n",
              "           1.5036e-05, 1.6059e-05],\n",
              "          [1.4148e-05, 1.7100e-05, 1.7359e-05,  ..., 1.8232e-05,\n",
              "           1.0345e-05, 1.6023e-05],\n",
              "          [1.2134e-05, 1.9303e-05, 1.4963e-05,  ..., 1.4009e-05,\n",
              "           1.1067e-05, 1.2937e-05],\n",
              "          ...,\n",
              "          [1.3299e-05, 1.9279e-05, 1.4391e-05,  ..., 1.0745e-05,\n",
              "           9.5473e-06, 1.0863e-05],\n",
              "          [1.4198e-05, 1.7930e-05, 1.9462e-05,  ..., 1.2362e-05,\n",
              "           1.0356e-05, 1.2597e-05],\n",
              "          [1.0847e-05, 1.3987e-05, 1.5398e-05,  ..., 1.4463e-05,\n",
              "           1.2472e-05, 1.0189e-05]]]], device='cuda:0',\n",
              "       grad_fn=<SigmoidBackward0>)"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "confidences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHETGD0uvHBD",
        "outputId": "88ad1fbf-e4a6-4ade-f087-92479af4d6ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 256, 256])"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "confidences.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgVV7T2FvEAX",
        "outputId": "5824b343-6e8f-45cb-aba1-e719f0d7d0d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 256, 256])"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "confidences[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmnMj7sfuIqn",
        "outputId": "c39a8290-1542-450c-e30c-710b6d4020cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([256, 256])"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "confidences[0][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3DWCMqBt25y",
        "outputId": "4fd249d0-22db-4dec-cb09-b85e1d1fad34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0., device='cuda:0', grad_fn=<RoundBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(torch.round(confidences[0][0][0][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d72Zbon7vZ09"
      },
      "source": [
        "마스크 부분을 수정해야 제대로 된 결과가 나올 것으로 예상됨."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vad4WC856tYk"
      },
      "source": [
        "### * fasterrcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "31f08a689bf24b6d83db4da1902fa79c",
            "0d63decfc2914604ac3471ba0937b561",
            "06cc812aed094bf99d1e29e6dafb2fa3",
            "97dab76860eb48878a20cd2e045c9ee9",
            "23cff78dba2347d5884cc5cf574af714",
            "4074970e56c34bb2903a5b3bc6577844",
            "8ce40bc391ea4631b15268dd2bf14835",
            "da233f6f3df744768e7fe4217da70252",
            "ae481db850a345b2900d56a48d4f9247",
            "cb8b52c5aff54d8ea4487c3d1a2230cd",
            "3db037d77c6d407680d2722bb2e315c2"
          ]
        },
        "id": "KXUlDmt9KfSF",
        "outputId": "39f92443-dd7f-4e8f-f02e-912c35056d6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31f08a689bf24b6d83db4da1902fa79c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): Sequential(\n",
              "    (0): ConvNormActivation(\n",
              "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "    (1): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (2): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (3): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (4): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (8): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (9): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (10): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (11): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (12): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (13): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (14): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (15): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (16): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (17): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (18): ConvNormActivation(\n",
              "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (cls_logits): Conv2d(1280, 15, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(1280, 60, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=62720, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=20, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 183,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "backbone.out_channels = 1280\n",
        "\n",
        "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "                                                output_size=7,\n",
        "                                                sampling_ratio=2)\n",
        "\n",
        "model = FasterRCNN(backbone,\n",
        "                   num_classes=5,\n",
        "                   rpn_anchor_generator=anchor_generator,\n",
        "                   box_roi_pool=roi_pooler)\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLq9MVNezjNR",
        "outputId": "5c0a54a6-d629-4664-a67b-d9e8edb8d604"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 18.95it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 114.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Datasets\n",
            "1227번째 train 이미지\n",
            "\t 이미지 shape : torch.Size([3, 2048, 2048])\n",
            "\t 이미지 type : torch.FloatTensor\n",
            "\t\tdict_keys == 'boxes' :  \n",
            "\t\t\tshape : torch.Size([3, 4])\n",
            "\t\t\ttype : torch.FloatTensor\n",
            "\t\t\ttensor([[ 712.,  886., 1198., 1160.],\n",
            "        [ 229.,  740.,  634., 1014.],\n",
            "        [ 628.,  411.,  844.,  829.]])\n",
            "\n",
            "\n",
            "\t\tdict_keys == 'labels' :  \n",
            "\t\t\tshape : torch.Size([3])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([1, 1, 1])\n",
            "\n",
            "\n",
            "\t\tdict_keys == 'masks' :  \n",
            "\t\t\tshape : torch.Size([3, 2048, 2048])\n",
            "\t\t\ttype : torch.ByteTensor\n",
            "\t\tdict_keys == 'image_id' :  \n",
            "\t\t\tshape : torch.Size([3, 1])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([[1227],\n",
            "        [1227],\n",
            "        [1227]])\n",
            "\n",
            "\n",
            "\t\tdict_keys == 'area' :  \n",
            "\t\t\tshape : torch.Size([3, 1])\n",
            "\t\t\ttype : torch.FloatTensor\n",
            "\t\t\ttensor([[78723.5000],\n",
            "        [65639.0000],\n",
            "        [64169.5000]])\n",
            "\n",
            "\n",
            "\t\tdict_keys == 'iscrowd' :  \n",
            "\t\t\tshape : torch.Size([3, 1])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([[0],\n",
            "        [0],\n",
            "        [0]])\n",
            "\n",
            "\n",
            "\n",
            "Test Datasets\n",
            "231번째 test 이미지\n",
            "\t 이미지 shape : torch.Size([3, 256, 256])\n",
            "\t 이미지 type : torch.FloatTensor\n",
            "\t\tdict_keys == 'image_id' :  \n",
            "\t\t\tshape : torch.Size([1])\n",
            "\t\t\ttype : torch.IntTensor\n",
            "\t\t\ttensor([231], dtype=torch.int32)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_dataset = CustomDataset(PATH, mode='train')\n",
        "test_dataset = CustomDataset(PATH, mode='test')\n",
        "\n",
        "train_idx = random.randint(0, train_dataset.__len__())\n",
        "test_idx = random.randint(0, test_dataset.__len__())\n",
        "\n",
        "print(\"\\nTrain Datasets\")\n",
        "quick_check(train_dataset, train_idx)\n",
        "print(\"\\nTest Datasets\")\n",
        "quick_check(test_dataset, test_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "yoXhC7ElzrBa"
      },
      "outputs": [],
      "source": [
        "img, _ = train_dataset.__getitem__(train_idx)\n",
        "input_batch = img.unsqueeze(0)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqctuMYUU5J5",
        "outputId": "89dcbde3-11eb-4916-8d60-33d57470c1e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'boxes': tensor([[2.1186e+01, 1.3385e+03, 1.0081e+03, 2.0480e+03],\n",
              "          [2.6927e+02, 1.6911e+03, 3.8648e+02, 1.9292e+03],\n",
              "          [1.8649e+02, 1.7027e+03, 3.0089e+02, 1.9160e+03],\n",
              "          [3.4875e+02, 1.6951e+03, 4.6421e+02, 1.9312e+03],\n",
              "          [1.2511e+03, 1.6949e+03, 1.3652e+03, 1.9278e+03],\n",
              "          [1.1692e+03, 1.6949e+03, 1.2833e+03, 1.9277e+03],\n",
              "          [1.0873e+03, 1.6948e+03, 1.2014e+03, 1.9277e+03],\n",
              "          [1.0053e+03, 1.6948e+03, 1.1194e+03, 1.9277e+03],\n",
              "          [9.2341e+02, 1.6948e+03, 1.0375e+03, 1.9277e+03],\n",
              "          [1.3331e+03, 1.6949e+03, 1.4471e+03, 1.9278e+03],\n",
              "          [8.4149e+02, 1.6948e+03, 9.5555e+02, 1.9277e+03],\n",
              "          [4.3179e+02, 1.6953e+03, 5.4591e+02, 1.9284e+03],\n",
              "          [1.4150e+03, 1.6949e+03, 1.5292e+03, 1.9278e+03],\n",
              "          [7.5957e+02, 1.6948e+03, 8.7363e+02, 1.9277e+03],\n",
              "          [5.9574e+02, 1.6948e+03, 7.0979e+02, 1.9277e+03],\n",
              "          [6.7765e+02, 1.6948e+03, 7.9171e+02, 1.9277e+03],\n",
              "          [5.1389e+02, 1.6947e+03, 6.2782e+02, 1.9275e+03],\n",
              "          [1.4967e+03, 1.6940e+03, 1.6116e+03, 1.9265e+03],\n",
              "          [1.5754e+03, 1.6897e+03, 1.6922e+03, 1.9236e+03],\n",
              "          [2.6244e+02, 1.8092e+03, 3.7615e+02, 2.0480e+03],\n",
              "          [1.5687e+03, 1.8160e+03, 1.6837e+03, 2.0480e+03],\n",
              "          [1.4897e+03, 1.8119e+03, 1.6052e+03, 2.0480e+03],\n",
              "          [1.3265e+03, 1.8119e+03, 1.4417e+03, 2.0480e+03],\n",
              "          [1.2446e+03, 1.8119e+03, 1.3598e+03, 2.0480e+03],\n",
              "          [1.1627e+03, 1.8119e+03, 1.2779e+03, 2.0480e+03],\n",
              "          [1.4084e+03, 1.8119e+03, 1.5236e+03, 2.0480e+03],\n",
              "          [1.0808e+03, 1.8119e+03, 1.1960e+03, 2.0480e+03],\n",
              "          [9.9884e+02, 1.8119e+03, 1.1141e+03, 2.0480e+03],\n",
              "          [9.1693e+02, 1.8120e+03, 1.0321e+03, 2.0480e+03],\n",
              "          [8.3502e+02, 1.8119e+03, 9.5024e+02, 2.0480e+03],\n",
              "          [7.5310e+02, 1.8119e+03, 8.6832e+02, 2.0480e+03],\n",
              "          [6.7119e+02, 1.8119e+03, 7.8641e+02, 2.0480e+03],\n",
              "          [5.8927e+02, 1.8119e+03, 7.0449e+02, 2.0480e+03],\n",
              "          [5.0733e+02, 1.8120e+03, 6.2255e+02, 2.0480e+03],\n",
              "          [3.4296e+02, 1.8105e+03, 4.5785e+02, 2.0480e+03],\n",
              "          [4.2516e+02, 1.8119e+03, 5.4034e+02, 2.0480e+03],\n",
              "          [9.5598e+01, 1.7425e+03, 2.1783e+02, 1.9233e+03],\n",
              "          [3.0683e+01, 1.0596e+03, 1.4283e+03, 2.0480e+03],\n",
              "          [1.6588e+03, 1.6920e+03, 1.7750e+03, 1.9258e+03],\n",
              "          [1.8305e+03, 2.4682e-01, 1.9599e+03, 1.6020e+02],\n",
              "          [1.8251e+03, 1.3744e-01, 1.9692e+03, 4.2021e+01],\n",
              "          [1.7763e+02, 1.8164e+03, 2.8716e+02, 2.0380e+03],\n",
              "          [5.0040e+01, 1.6515e+00, 1.3242e+02, 1.8912e+02],\n",
              "          [6.5449e+00, 1.7597e+03, 1.2559e+02, 1.9157e+03],\n",
              "          [1.4460e+02, 1.0539e+00, 2.5332e+02, 1.4213e+02],\n",
              "          [1.4390e+01, 7.9529e+02, 7.7134e+02, 2.0480e+03],\n",
              "          [4.7981e+01, 7.9370e+01, 1.3127e+02, 2.6886e+02],\n",
              "          [1.0138e+00, 1.3640e+02, 5.7608e+01, 3.1988e+02],\n",
              "          [1.7771e+03, 0.0000e+00, 1.9026e+03, 1.5483e+02],\n",
              "          [6.6365e-01, 2.2784e+02, 5.0891e+01, 4.1500e+02],\n",
              "          [1.8642e+02, 2.3819e-01, 3.2214e+02, 2.8972e+01],\n",
              "          [6.9026e-01, 1.6899e+03, 5.5133e+01, 1.8849e+03],\n",
              "          [2.2625e+02, 1.0224e+00, 3.3686e+02, 1.0117e+02],\n",
              "          [4.5352e+01, 1.7518e+03, 2.5860e+02, 1.8108e+03],\n",
              "          [2.0968e+02, 1.6079e-01, 3.3621e+02, 1.8374e+01],\n",
              "          [2.3011e+02, 1.3075e+03, 3.3027e+02, 1.5391e+03],\n",
              "          [1.2556e+02, 1.4290e+03, 3.1635e+02, 1.4835e+03],\n",
              "          [1.2635e+02, 1.3469e+03, 3.1703e+02, 1.4015e+03],\n",
              "          [2.2967e+02, 1.2258e+03, 3.2997e+02, 1.4562e+03],\n",
              "          [2.2755e+02, 1.4752e+03, 3.2776e+02, 1.7161e+03],\n",
              "          [2.3107e+02, 1.3822e+03, 3.2820e+02, 1.6228e+03],\n",
              "          [1.2656e+02, 1.2647e+03, 3.1813e+02, 1.3191e+03],\n",
              "          [2.2643e+02, 2.2401e+00, 3.3831e+02, 1.9742e+02],\n",
              "          [1.2493e+02, 1.1823e+03, 3.1766e+02, 1.2362e+03],\n",
              "          [4.6261e+01, 1.7460e+02, 1.2417e+02, 3.5137e+02],\n",
              "          [2.1522e+02, 1.5906e+03, 3.1724e+02, 1.8011e+03],\n",
              "          [2.3020e+02, 1.1438e+03, 3.3021e+02, 1.3740e+03],\n",
              "          [2.3280e+02, 4.0247e+02, 3.3354e+02, 6.3726e+02],\n",
              "          [1.2569e+02, 5.2806e+02, 3.1634e+02, 5.8258e+02],\n",
              "          [2.2996e+02, 1.0619e+03, 3.2999e+02, 1.2919e+03],\n",
              "          [2.3086e+02, 3.2099e+02, 3.3109e+02, 5.5138e+02],\n",
              "          [1.2367e+02, 1.5109e+03, 3.1506e+02, 1.5655e+03],\n",
              "          [1.1790e+02, 1.6751e+03, 3.1272e+02, 1.7329e+03],\n",
              "          [2.2988e+02, 9.8124e+02, 3.2990e+02, 1.2105e+03],\n",
              "          [1.7558e+03, 6.0754e-02, 1.9044e+03, 6.0530e+01],\n",
              "          [1.2458e+02, 1.1878e+03, 4.1311e+02, 1.6920e+03],\n",
              "          [1.3060e+02, 5.2536e+02, 4.1978e+02, 1.0439e+03],\n",
              "          [3.0304e+02, 1.5636e+03, 4.0565e+02, 1.7958e+03],\n",
              "          [2.2992e+02, 9.0037e+02, 3.3059e+02, 1.1289e+03],\n",
              "          [2.3026e+02, 2.4125e+02, 3.3019e+02, 4.6947e+02],\n",
              "          [3.3834e+02, 5.0294e+02, 1.9858e+03, 2.0480e+03],\n",
              "          [2.2923e+02, 4.9106e+02, 3.3127e+02, 7.2190e+02],\n",
              "          [3.8090e+02, 1.5719e+03, 4.8474e+02, 1.8022e+03],\n",
              "          [1.0372e+03, 1.5723e+03, 1.1409e+03, 1.8024e+03],\n",
              "          [9.5530e+02, 1.5723e+03, 1.0590e+03, 1.8024e+03],\n",
              "          [1.1191e+03, 1.5723e+03, 1.2228e+03, 1.8024e+03],\n",
              "          [1.2011e+03, 1.5723e+03, 1.3047e+03, 1.8024e+03],\n",
              "          [8.7339e+02, 1.5722e+03, 9.7705e+02, 1.8024e+03],\n",
              "          [4.6373e+02, 1.5731e+03, 5.6714e+02, 1.8029e+03],\n",
              "          [1.2830e+03, 1.5723e+03, 1.3866e+03, 1.8024e+03],\n",
              "          [7.9148e+02, 1.5722e+03, 8.9513e+02, 1.8024e+03],\n",
              "          [1.4467e+03, 1.5724e+03, 1.5505e+03, 1.8024e+03],\n",
              "          [1.3648e+03, 1.5723e+03, 1.4685e+03, 1.8024e+03],\n",
              "          [7.0957e+02, 1.5722e+03, 8.1321e+02, 1.8024e+03],\n",
              "          [6.2765e+02, 1.5722e+03, 7.3129e+02, 1.8023e+03],\n",
              "          [5.4571e+02, 1.5723e+03, 6.4939e+02, 1.8021e+03],\n",
              "          [1.2382e+02, 1.6388e+03, 2.2968e+02, 1.8201e+03],\n",
              "          [2.6668e+02, 2.4085e-01, 4.0732e+02, 2.4820e+01],\n",
              "          [4.5261e+01, 2.5620e+02, 1.2507e+02, 4.3169e+02],\n",
              "          [1.2505e+02, 8.5627e+02, 3.2079e+02, 9.1543e+02]], device='cuda:0'),\n",
              "  'labels': tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
              "          4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4, 4, 4, 4, 4,\n",
              "          2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
              "          4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
              "          4, 4, 4, 4], device='cuda:0'),\n",
              "  'scores': tensor([0.2335, 0.2334, 0.2330, 0.2321, 0.2318, 0.2318, 0.2318, 0.2318, 0.2318,\n",
              "          0.2318, 0.2318, 0.2318, 0.2318, 0.2318, 0.2318, 0.2318, 0.2317, 0.2316,\n",
              "          0.2313, 0.2311, 0.2309, 0.2307, 0.2305, 0.2305, 0.2305, 0.2305, 0.2305,\n",
              "          0.2305, 0.2305, 0.2304, 0.2304, 0.2304, 0.2304, 0.2304, 0.2303, 0.2303,\n",
              "          0.2300, 0.2298, 0.2293, 0.2287, 0.2274, 0.2274, 0.2269, 0.2266, 0.2258,\n",
              "          0.2258, 0.2252, 0.2249, 0.2242, 0.2241, 0.2239, 0.2237, 0.2236, 0.2235,\n",
              "          0.2234, 0.2233, 0.2233, 0.2233, 0.2233, 0.2232, 0.2232, 0.2232, 0.2232,\n",
              "          0.2231, 0.2230, 0.2230, 0.2229, 0.2229, 0.2228, 0.2226, 0.2226, 0.2226,\n",
              "          0.2225, 0.2225, 0.2224, 0.2224, 0.2223, 0.2223, 0.2220, 0.2220, 0.2218,\n",
              "          0.2217, 0.2217, 0.2217, 0.2217, 0.2217, 0.2217, 0.2217, 0.2217, 0.2217,\n",
              "          0.2217, 0.2217, 0.2217, 0.2217, 0.2217, 0.2216, 0.2215, 0.2215, 0.2214,\n",
              "          0.2214], device='cuda:0')}]"
            ]
          },
          "execution_count": 186,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzGOUJHp0E_-",
        "outputId": "0f29585e-d488-44a4-deb9-93f4f124f881"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Data Output\n",
            "inference no.1227 : \n",
            "\tboxes : \n",
            "\t\t shape : torch.Size([100, 4])\n",
            "\n",
            "\tlabels : \n",
            "\t\t shape : torch.Size([100])\n",
            "\n",
            "\tscores : \n",
            "\t\t shape : torch.Size([100])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Train Data Output\")\n",
        "uni_inference(output, train_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "lh7oeXsNH7lD"
      },
      "outputs": [],
      "source": [
        "img, _ = test_dataset.__getitem__(test_idx)\n",
        "\n",
        "input_batch = img.unsqueeze(0)\n",
        "\n",
        "model.eval()\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "predictions = model(input_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmi3NgBs0kG4",
        "outputId": "f81566d6-a9fd-4dbc-b0ee-d68588258d1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'boxes': tensor([[ 22.6541,  22.6714,  36.2568,  43.2240],\n",
              "          [123.2007,   3.5830, 162.3059,  82.9571],\n",
              "          [102.6815,   3.5628, 141.8291,  82.9354],\n",
              "          [143.6196,   3.9919, 182.7745,  83.3374],\n",
              "          [ 82.1691,   3.6955, 121.2818,  82.9451],\n",
              "          [ 61.7119,   3.7490, 100.8140,  82.9393],\n",
              "          [164.1181,   4.6507, 203.4202,  85.1843],\n",
              "          [ 41.3099,   3.8039,  80.3901,  83.0411],\n",
              "          [ 12.5812,   4.9828,  49.8068,  85.4255],\n",
              "          [ 22.4761,  34.1992,  35.8650,  55.2395],\n",
              "          [ 21.2685,  25.4661,  60.0878, 104.9288],\n",
              "          [ 16.0646,  33.6116,  57.6379,  41.9813],\n",
              "          [159.7459,  33.4850, 199.6217,  41.6733],\n",
              "          [ 67.5610,  33.5238, 107.4440,  41.7073],\n",
              "          [128.9939,  33.5610, 168.8706,  41.7357],\n",
              "          [ 88.0315,  33.5099, 127.9200,  41.6989],\n",
              "          [108.4872,  33.5417, 148.3909,  41.7241],\n",
              "          [ 22.4386,  55.1692,  35.7900,  76.1828],\n",
              "          [ 22.4528, 147.2960,  35.8036, 168.3235],\n",
              "          [ 22.4517, 157.5170,  35.8081, 178.5559],\n",
              "          [ 22.4499, 137.0544,  35.8001, 158.0743],\n",
              "          [163.9399,  35.4296, 203.7325, 115.6107],\n",
              "          [ 22.4503,  96.1053,  35.8118, 117.1071],\n",
              "          [ 22.4570, 106.3286,  35.8135, 127.3425],\n",
              "          [ 22.4509, 126.8079,  35.8004, 147.8265],\n",
              "          [ 22.4407,  85.8786,  35.8028, 106.8760],\n",
              "          [ 22.4546, 116.5600,  35.8050, 137.5810],\n",
              "          [ 22.4444,  65.3815,  35.8037,  86.4065],\n",
              "          [ 22.4490,  44.8947,  35.8437,  66.0157],\n",
              "          [ 22.4382,  75.6302,  35.7999,  96.6393],\n",
              "          [ 24.8966,  41.6884,  64.5355,  50.5630],\n",
              "          [135.4895,  34.1239, 147.2654,  56.0859],\n",
              "          [158.4653,  41.5820, 197.5198,  50.4740],\n",
              "          [125.2426,  34.1313, 137.0202,  56.0887],\n",
              "          [145.7359,  34.1076, 157.5052,  56.0814],\n",
              "          [ 22.4519, 167.5860,  35.8460, 188.6535],\n",
              "          [115.0014,  34.1218, 126.7712,  56.0792],\n",
              "          [155.9809,  34.0794, 167.7418,  56.0643],\n",
              "          [127.7714,  41.6880, 166.7428,  50.5622],\n",
              "          [104.7654,  34.1021, 116.5247,  56.0743],\n",
              "          [ 74.0539,  34.0594,  85.8025,  56.0543],\n",
              "          [166.2209,  34.0403, 177.9739,  56.0925],\n",
              "          [ 94.5281,  34.0750, 106.2778,  56.0600],\n",
              "          [ 84.2923,  34.0636,  96.0376,  56.0545],\n",
              "          [176.4025,  34.1121, 188.1718,  56.1684],\n",
              "          [ 63.8009,  34.0114,  75.5628,  55.9583],\n",
              "          [107.2936,  41.6807, 146.2746,  50.5592],\n",
              "          [ 47.0313,  33.5952,  86.9264,  41.7426],\n",
              "          [ 66.3368,  41.6697, 105.3169,  50.5504],\n",
              "          [ 43.3409,  34.2624,  55.1933,  56.0612],\n",
              "          [ 86.8247,  41.6715, 125.8087,  50.5536],\n",
              "          [ 53.4865,  34.1371,  65.2560,  56.0897],\n",
              "          [159.2988, 113.0749, 196.5976, 121.9900],\n",
              "          [ 21.3313,  86.5332,  60.0661, 166.0888],\n",
              "          [159.3399, 102.8498, 196.6698, 111.7527],\n",
              "          [ 22.9137, 195.4424,  37.5847, 214.5587],\n",
              "          [166.2559, 106.3273, 177.9209, 128.4485],\n",
              "          [ 15.5748, 164.3102,  55.5302, 173.2506],\n",
              "          [ 25.7838,  61.8534,  63.7724,  70.6472],\n",
              "          [ 15.5670, 154.0669,  55.5183, 163.0125],\n",
              "          [ 25.9574,  92.5749,  63.7242, 101.3968],\n",
              "          [ 25.9675, 102.8114,  63.7359, 111.6336],\n",
              "          [ 25.9632,  82.3324,  63.7228,  91.1516],\n",
              "          [ 15.5630, 143.8197,  55.5113, 152.7717],\n",
              "          [ 25.9897, 113.0488,  63.7481, 121.8690],\n",
              "          [ 22.4848, 177.4787,  35.9998, 198.4676],\n",
              "          [ 26.0050,  72.0481,  63.7225,  80.8848],\n",
              "          [ 15.5598, 174.5563,  55.5225, 183.4934],\n",
              "          [ 26.0054, 133.5296,  63.7589, 142.3442],\n",
              "          [ 26.0036, 123.2866,  63.7563, 132.1044],\n",
              "          [159.2968, 123.2705, 196.5487, 132.2328],\n",
              "          [ 22.7623, 187.6742,  36.6430, 207.9779],\n",
              "          [145.8033, 106.4426, 157.4118, 128.4483],\n",
              "          [159.2791, 133.4518, 196.4812, 142.4440],\n",
              "          [176.4028,  96.1628, 188.1284, 118.3032],\n",
              "          [125.2633,  55.1771, 136.9570,  77.2752],\n",
              "          [135.5875, 106.4243, 147.1862, 128.4342],\n",
              "          [128.4547, 113.1388, 165.7914, 122.1091],\n",
              "          [156.0404, 106.4290, 167.6692, 128.4650],\n",
              "          [125.3493,  44.8936, 137.0098,  67.0715],\n",
              "          [135.5962,  44.8985, 147.2434,  67.0709],\n",
              "          [159.3390, 143.6882, 196.5510, 152.6922],\n",
              "          [135.5119,  55.1953, 147.1948,  77.2781],\n",
              "          [166.2188,  96.1190, 177.9049, 118.2555],\n",
              "          [159.4226,  92.6047, 196.7119, 101.5027],\n",
              "          [135.5879,  95.9826, 147.1881, 118.0633],\n",
              "          [125.2710,  65.2567, 136.9637,  87.3817],\n",
              "          [135.5642, 116.7388, 147.1964, 138.7407],\n",
              "          [118.2579,  61.9934, 155.7130,  70.8262],\n",
              "          [145.8039, 116.7249, 157.4269, 138.7297],\n",
              "          [128.5525, 133.5898, 165.8592, 142.5650],\n",
              "          [125.3882,  95.9465, 136.9957, 118.0866],\n",
              "          [145.7943,  96.0319, 157.4009, 118.0972],\n",
              "          [125.3620, 106.4041, 136.9738, 128.4457],\n",
              "          [118.4697,  72.1942, 155.6596,  81.0743],\n",
              "          [128.4802, 123.3918, 165.8306, 132.3660],\n",
              "          [135.5021,  65.3012, 147.1876,  87.3918],\n",
              "          [125.3232, 116.7460, 136.9702, 138.7695],\n",
              "          [115.1233,  44.8877, 126.7711,  67.0683],\n",
              "          [156.0600, 116.6467, 167.6845, 138.7063]], device='cuda:0',\n",
              "         grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "          2, 2, 2, 2], device='cuda:0'),\n",
              "  'scores': tensor([0.2345, 0.2344, 0.2343, 0.2341, 0.2341, 0.2340, 0.2339, 0.2337, 0.2335,\n",
              "          0.2333, 0.2333, 0.2330, 0.2326, 0.2325, 0.2325, 0.2325, 0.2325, 0.2324,\n",
              "          0.2324, 0.2324, 0.2323, 0.2323, 0.2323, 0.2323, 0.2323, 0.2323, 0.2323,\n",
              "          0.2323, 0.2323, 0.2323, 0.2323, 0.2322, 0.2322, 0.2322, 0.2322, 0.2322,\n",
              "          0.2322, 0.2322, 0.2322, 0.2322, 0.2322, 0.2321, 0.2321, 0.2321, 0.2321,\n",
              "          0.2321, 0.2321, 0.2321, 0.2320, 0.2320, 0.2320, 0.2319, 0.2319, 0.2319,\n",
              "          0.2318, 0.2318, 0.2318, 0.2318, 0.2318, 0.2318, 0.2318, 0.2318, 0.2318,\n",
              "          0.2318, 0.2318, 0.2318, 0.2318, 0.2318, 0.2318, 0.2318, 0.2318, 0.2318,\n",
              "          0.2318, 0.2318, 0.2318, 0.2318, 0.2318, 0.2318, 0.2317, 0.2317, 0.2317,\n",
              "          0.2317, 0.2317, 0.2317, 0.2317, 0.2317, 0.2317, 0.2317, 0.2317, 0.2317,\n",
              "          0.2317, 0.2317, 0.2317, 0.2317, 0.2317, 0.2317, 0.2317, 0.2317, 0.2317,\n",
              "          0.2317], device='cuda:0', grad_fn=<IndexBackward0>)}]"
            ]
          },
          "execution_count": 194,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8WGq8s3UmcZ",
        "outputId": "6ec9b598-8812-41e9-d29a-98f18a8fa0f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Data Output\n",
            "inference no.231 : \n",
            "\tboxes : \n",
            "\t\t shape : torch.Size([100, 4])\n",
            "\n",
            "\tlabels : \n",
            "\t\t shape : torch.Size([100])\n",
            "\n",
            "\tscores : \n",
            "\t\t shape : torch.Size([100])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Test Data Output\")\n",
        "uni_inference(output, test_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5JE0-hzHgsT"
      },
      "source": [
        "### * maskrcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "3n5b7u1705oU"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "# MaskRCNN\n",
        "# MaskRCNNHeads\n",
        "\n",
        "def get_model_instance_segmentation(num_classes):\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "82d85325dac8460ebf9a5a4afee5b0d4",
            "756e552563c649ca8566a6b1fe053634",
            "391875e5a644469ca840eeb994aa3218",
            "0031618e7ce248e099bbc9bd174c46ad",
            "5a329e6d789b4916ac72b31cf8d5aed9",
            "fd0b40f45aeb4cdea3339f342bc6f724",
            "6bb6ffd2d00342d09c9d45c3ff5bc24c",
            "bc681dc64df9422c984cc3b60967a202",
            "d4abd4777ed24a02811c25656be9d4ba",
            "12c43f1f217644a6ae8030e3af447965",
            "1959a4b17ae64b14bb57f4e512f50e32"
          ]
        },
        "id": "m-0xKHh_1mfR",
        "outputId": "afc3433c-94d5-48f3-88a3-cc8b36adaf39"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82d85325dac8460ebf9a5a4afee5b0d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/170M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "MaskRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=20, bias=True)\n",
              "    )\n",
              "    (mask_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n",
              "    (mask_head): MaskRCNNHeads(\n",
              "      (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu2): ReLU(inplace=True)\n",
              "      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu3): ReLU(inplace=True)\n",
              "      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (relu4): ReLU(inplace=True)\n",
              "    )\n",
              "    (mask_predictor): MaskRCNNPredictor(\n",
              "      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (mask_fcn_logits): Conv2d(256, 5, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 197,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = get_model_instance_segmentation(num_classes=5)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QltQN5lN1EpP",
        "outputId": "ca0dcc26-ec1c-4aaf-b8d1-4fbad8910574"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 16.03it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 94.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train Datasets\n",
            "1206번째 train 이미지\n",
            "\t 이미지 shape : torch.Size([3, 2048, 2048])\n",
            "\t 이미지 type : torch.FloatTensor\n",
            "\t\tdict_keys == 'boxes' :  \n",
            "\t\t\tshape : torch.Size([3, 4])\n",
            "\t\t\ttype : torch.FloatTensor\n",
            "\t\t\ttensor([[ 832.,  665., 1191., 1009.],\n",
            "        [ 395.,  729.,  763., 1082.],\n",
            "        [ 716., 1129.,  961., 1612.]])\n",
            "\n",
            "\n",
            "\t\tdict_keys == 'labels' :  \n",
            "\t\t\tshape : torch.Size([3])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([1, 1, 1])\n",
            "\n",
            "\n",
            "\t\tdict_keys == 'masks' :  \n",
            "\t\t\tshape : torch.Size([3, 2048, 2048])\n",
            "\t\t\ttype : torch.ByteTensor\n",
            "\t\tdict_keys == 'image_id' :  \n",
            "\t\t\tshape : torch.Size([3, 1])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([[1206],\n",
            "        [1206],\n",
            "        [1206]])\n",
            "\n",
            "\n",
            "\t\tdict_keys == 'area' :  \n",
            "\t\t\tshape : torch.Size([3, 1])\n",
            "\t\t\ttype : torch.FloatTensor\n",
            "\t\t\ttensor([[52780.],\n",
            "        [51960.],\n",
            "        [62293.]])\n",
            "\n",
            "\n",
            "\t\tdict_keys == 'iscrowd' :  \n",
            "\t\t\tshape : torch.Size([3, 1])\n",
            "\t\t\ttype : torch.LongTensor\n",
            "\t\t\ttensor([[0],\n",
            "        [0],\n",
            "        [0]])\n",
            "\n",
            "\n",
            "\n",
            "Test Datasets\n",
            "241번째 test 이미지\n",
            "\t 이미지 shape : torch.Size([3, 256, 256])\n",
            "\t 이미지 type : torch.FloatTensor\n",
            "\t\tdict_keys == 'image_id' :  \n",
            "\t\t\tshape : torch.Size([1])\n",
            "\t\t\ttype : torch.IntTensor\n",
            "\t\t\ttensor([241], dtype=torch.int32)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_dataset = CustomDataset(PATH, mode='train')\n",
        "test_dataset = CustomDataset(PATH, mode='test')\n",
        "\n",
        "train_idx = random.randint(0, train_dataset.__len__())\n",
        "test_idx = random.randint(0, test_dataset.__len__())\n",
        "\n",
        "print(\"\\nTrain Datasets\")\n",
        "quick_check(train_dataset, train_idx)\n",
        "print(\"\\nTest Datasets\")\n",
        "quick_check(test_dataset, test_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "aMbSkfVo1Tna"
      },
      "outputs": [],
      "source": [
        "img, _ = train_dataset.__getitem__(train_idx)\n",
        "input_batch = img.unsqueeze(0)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC05Nnbe1lEI",
        "outputId": "f55035ca-e887-4c25-9d86-dadd81942511"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'boxes': tensor([[5.7536e+00, 2.8919e+02, 6.8827e+01, 3.5940e+02],\n",
              "          [5.5641e+00, 3.7109e+02, 6.9109e+01, 4.4187e+02],\n",
              "          [5.5232e+00, 9.8553e+02, 6.9152e+01, 1.0564e+03],\n",
              "          [5.5240e+00, 1.3337e+03, 6.9160e+01, 1.4046e+03],\n",
              "          [5.5208e+00, 4.1208e+02, 6.9157e+01, 4.8296e+02],\n",
              "          [5.5237e+00, 1.5385e+03, 6.9160e+01, 1.6094e+03],\n",
              "          [5.5254e+00, 1.0265e+03, 6.9157e+01, 1.0974e+03],\n",
              "          [5.5199e+00, 1.1084e+03, 6.9159e+01, 1.1793e+03],\n",
              "          [5.5227e+00, 1.3951e+03, 6.9162e+01, 1.4660e+03],\n",
              "          [5.5220e+00, 1.3644e+03, 6.9158e+01, 1.4353e+03],\n",
              "          [5.5399e+00, 5.1449e+02, 6.9143e+01, 5.8536e+02],\n",
              "          [5.5307e+00, 9.1385e+02, 6.9159e+01, 9.8473e+02],\n",
              "          [5.5225e+00, 1.2313e+03, 6.9165e+01, 1.3022e+03],\n",
              "          [5.5231e+00, 1.2927e+03, 6.9165e+01, 1.3636e+03],\n",
              "          [5.5290e+00, 7.5001e+02, 6.9162e+01, 8.2089e+02],\n",
              "          [5.5224e+00, 5.8617e+02, 6.9162e+01, 6.5704e+02],\n",
              "          [5.5011e+00, 1.5999e+03, 6.9178e+01, 1.6708e+03],\n",
              "          [5.5199e+00, 1.4259e+03, 6.9163e+01, 1.4967e+03],\n",
              "          [5.5240e+00, 9.4457e+02, 6.9162e+01, 1.0155e+03],\n",
              "          [5.5253e+00, 1.0674e+03, 6.9165e+01, 1.1383e+03],\n",
              "          [5.5248e+00, 1.1698e+03, 6.9163e+01, 1.2407e+03],\n",
              "          [5.5186e+00, 1.4668e+03, 6.9161e+01, 1.5377e+03],\n",
              "          [5.5253e+00, 6.7832e+02, 6.9163e+01, 7.4920e+02],\n",
              "          [5.5278e+00, 8.5241e+02, 6.9160e+01, 9.2330e+02],\n",
              "          [5.5191e+00, 1.4975e+03, 6.9163e+01, 1.5684e+03],\n",
              "          [5.5207e+00, 4.5305e+02, 6.9168e+01, 5.2393e+02],\n",
              "          [5.5227e+00, 7.0905e+02, 6.9157e+01, 7.7993e+02],\n",
              "          [5.5249e+00, 6.1690e+02, 6.9162e+01, 6.8777e+02],\n",
              "          [5.5240e+00, 8.1146e+02, 6.9161e+01, 8.8234e+02],\n",
              "          [5.6076e+00, 3.3023e+02, 6.8993e+01, 4.0082e+02],\n",
              "          [5.4873e+00, 1.6307e+03, 6.9162e+01, 1.7015e+03],\n",
              "          [1.4995e+03, 1.1081e+03, 2.0262e+03, 1.9867e+03],\n",
              "          [5.5145e+00, 1.6716e+03, 6.9275e+01, 1.7424e+03],\n",
              "          [6.0803e+00, 2.1814e+02, 6.8035e+01, 2.8670e+02],\n",
              "          [6.0964e+00, 1.7957e+03, 6.8755e+01, 1.8652e+03],\n",
              "          [5.9178e+00, 1.7540e+03, 6.9144e+01, 1.8241e+03],\n",
              "          [5.4676e+00, 1.7125e+03, 6.9513e+01, 1.7837e+03],\n",
              "          [1.2351e+03, 7.8766e+02, 2.0094e+03, 1.9634e+03],\n",
              "          [0.0000e+00, 1.1743e+01, 6.6687e+01, 1.0993e+02],\n",
              "          [3.6984e+02, 3.5798e+02, 1.1432e+03, 1.9261e+03],\n",
              "          [1.9720e+03, 2.4344e+01, 2.0317e+03, 7.8485e+01],\n",
              "          [6.8592e+02, 3.5859e+02, 1.4572e+03, 1.9266e+03],\n",
              "          [1.2344e+01, 1.2600e+03, 8.2278e+01, 1.3351e+03],\n",
              "          [1.2345e+01, 1.1474e+03, 8.2276e+01, 1.2224e+03],\n",
              "          [1.2348e+01, 7.7876e+02, 8.2276e+01, 8.5381e+02],\n",
              "          [1.2346e+01, 4.3061e+02, 8.2281e+01, 5.0563e+02],\n",
              "          [1.2343e+01, 1.1986e+03, 8.2274e+01, 1.2737e+03],\n",
              "          [1.2346e+01, 8.8116e+02, 8.2287e+01, 9.5622e+02],\n",
              "          [1.2346e+01, 1.5673e+03, 8.2268e+01, 1.6423e+03],\n",
              "          [1.2338e+01, 5.5349e+02, 8.2283e+01, 6.2852e+02],\n",
              "          [1.2362e+01, 4.8182e+02, 8.2269e+01, 5.5684e+02],\n",
              "          [1.2350e+01, 6.4564e+02, 8.2286e+01, 7.2069e+02],\n",
              "          [1.2930e+01, 2.5720e+02, 8.1557e+01, 3.3042e+02],\n",
              "          [9.5076e+02, 1.2997e+03, 2.0036e+03, 1.9980e+03],\n",
              "          [1.2364e-01, 8.2009e+00, 1.0555e+02, 1.7818e+02],\n",
              "          [0.0000e+00, 1.4835e+00, 1.7223e+03, 1.4610e+02],\n",
              "          [2.8515e+02, 1.2122e+03, 1.3215e+03, 1.9300e+03],\n",
              "          [0.0000e+00, 8.1090e+01, 2.0679e+02, 1.8675e+03],\n",
              "          [1.0082e+01, 1.3922e+01, 1.1220e+02, 9.2490e+01],\n",
              "          [0.0000e+00, 3.5138e+02, 6.6733e+02, 1.9200e+03],\n",
              "          [1.5920e+03, 1.4204e+02, 2.0305e+03, 1.8247e+03],\n",
              "          [5.8990e+01, 2.2767e+00, 1.0098e+03, 8.5839e+01],\n",
              "          [5.6025e+00, 2.8094e+01, 1.4003e+02, 2.0195e+02],\n",
              "          [1.1637e+01, 0.0000e+00, 4.0368e+02, 1.0737e+02],\n",
              "          [0.0000e+00, 1.3124e+03, 9.5203e+02, 1.9925e+03],\n",
              "          [4.9280e+02, 1.7830e+00, 1.8566e+03, 9.0113e+01],\n",
              "          [2.3954e+01, 1.7316e+03, 9.4543e+01, 1.8063e+03],\n",
              "          [2.3797e+01, 1.6906e+03, 9.4503e+01, 1.7653e+03],\n",
              "          [0.0000e+00, 1.1802e+03, 5.1609e+02, 1.9807e+03],\n",
              "          [1.3598e+03, 7.3097e+02, 2.0210e+03, 1.4261e+03],\n",
              "          [1.3598e+03, 4.0324e+02, 2.0210e+03, 1.0984e+03],\n",
              "          [9.9200e+02, 5.1719e+02, 2.0044e+03, 1.3022e+03],\n",
              "          [5.8982e+02, 1.4145e+03, 1.5078e+03, 1.9991e+03],\n",
              "          [5.7793e+02, 8.6871e+02, 1.9865e+03, 1.2082e+03],\n",
              "          [5.7875e+02, 1.1962e+03, 1.9868e+03, 1.5357e+03],\n",
              "          [5.7770e+02, 4.5918e+02, 1.9862e+03, 7.9861e+02],\n",
              "          [7.8460e+01, 5.9216e+02, 1.2135e+03, 1.3841e+03],\n",
              "          [5.7835e+02, 1.0324e+03, 1.9862e+03, 1.3720e+03],\n",
              "          [5.7886e+02, 7.0479e+02, 1.9873e+03, 1.0445e+03],\n",
              "          [4.8974e+02, 5.9206e+02, 1.6211e+03, 1.3834e+03],\n",
              "          [5.7590e+02, 3.4786e+02, 1.6942e+03, 1.1363e+03],\n",
              "          [1.3630e+03, 1.5719e+02, 2.0172e+03, 8.5420e+02],\n",
              "          [1.0113e+03, 2.0305e+02, 1.7685e+03, 1.8105e+03],\n",
              "          [2.3734e+02, 8.3836e+02, 1.3698e+03, 1.6293e+03],\n",
              "          [0.0000e+00, 2.6480e+02, 1.1254e+03, 1.0530e+03],\n",
              "          [5.7886e+02, 2.9450e+02, 1.9835e+03, 6.3390e+02],\n",
              "          [4.7985e+02, 1.6421e+03, 2.0073e+03, 1.9313e+03],\n",
              "          [9.3666e+02, 1.3627e+03, 2.0021e+03, 1.6825e+03],\n",
              "          [1.4358e+02, 6.2062e+02, 1.9689e+03, 9.6417e+02],\n",
              "          [6.0368e+02, 1.1126e+03, 1.6719e+03, 1.8593e+03],\n",
              "          [3.3695e+02, 1.0025e+02, 1.4400e+03, 8.9434e+02],\n",
              "          [7.4623e+02, 8.3813e+02, 1.8609e+03, 1.6282e+03],\n",
              "          [0.0000e+00, 1.5078e+03, 1.9623e+03, 1.9429e+03],\n",
              "          [9.6450e+00, 2.3017e+02, 1.3261e+02, 4.0199e+02],\n",
              "          [0.0000e+00, 3.7509e+02, 1.9371e+03, 7.1577e+02],\n",
              "          [0.0000e+00, 9.4820e+02, 1.9521e+03, 1.2894e+03],\n",
              "          [9.8194e+00, 2.9288e+02, 1.3187e+02, 4.6304e+02],\n",
              "          [1.9428e+03, 2.3091e+01, 2.0191e+03, 8.6118e+01],\n",
              "          [0.0000e+00, 1.1129e+03, 1.9557e+03, 1.4537e+03],\n",
              "          [0.0000e+00, 7.8542e+02, 1.9477e+03, 1.1278e+03]], device='cuda:0'),\n",
              "  'labels': tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
              "          4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 3, 1, 3, 4, 3, 4, 4, 4, 4, 4, 4,\n",
              "          4, 4, 4, 4, 4, 3, 1, 4, 3, 3, 4, 3, 3, 4, 4, 4, 3, 4, 4, 4, 3, 3, 3, 3,\n",
              "          3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3,\n",
              "          4, 4, 3, 3], device='cuda:0'),\n",
              "  'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            ...,\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "  \n",
              "  \n",
              "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            ...,\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "  \n",
              "  \n",
              "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            ...,\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "  \n",
              "  \n",
              "          ...,\n",
              "  \n",
              "  \n",
              "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            ...,\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "  \n",
              "  \n",
              "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            ...,\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "  \n",
              "  \n",
              "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            ...,\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "            [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0'),\n",
              "  'scores': tensor([0.3315, 0.3315, 0.3312, 0.3312, 0.3312, 0.3312, 0.3312, 0.3312, 0.3312,\n",
              "          0.3312, 0.3312, 0.3312, 0.3312, 0.3312, 0.3312, 0.3312, 0.3312, 0.3311,\n",
              "          0.3311, 0.3311, 0.3311, 0.3311, 0.3311, 0.3311, 0.3311, 0.3311, 0.3311,\n",
              "          0.3311, 0.3311, 0.3310, 0.3309, 0.3302, 0.3302, 0.3287, 0.3285, 0.3285,\n",
              "          0.3278, 0.3246, 0.3245, 0.3223, 0.3218, 0.3192, 0.3173, 0.3173, 0.3173,\n",
              "          0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3173, 0.3167, 0.3164,\n",
              "          0.3151, 0.3138, 0.3132, 0.3094, 0.3092, 0.3091, 0.3085, 0.3058, 0.3040,\n",
              "          0.3027, 0.3019, 0.3017, 0.3003, 0.3002, 0.2993, 0.2977, 0.2977, 0.2954,\n",
              "          0.2952, 0.2950, 0.2947, 0.2946, 0.2943, 0.2940, 0.2937, 0.2931, 0.2927,\n",
              "          0.2927, 0.2925, 0.2921, 0.2919, 0.2912, 0.2907, 0.2905, 0.2900, 0.2899,\n",
              "          0.2898, 0.2898, 0.2896, 0.2891, 0.2888, 0.2887, 0.2887, 0.2887, 0.2883,\n",
              "          0.2880], device='cuda:0')}]"
            ]
          },
          "execution_count": 200,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUeSL2x52H7L",
        "outputId": "29962fd6-c77e-44ff-e18d-176840032cf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Data Output\n",
            "inference no.1206 : \n",
            "\tboxes : \n",
            "\t\t shape : torch.Size([100, 4])\n",
            "\n",
            "\tlabels : \n",
            "\t\t shape : torch.Size([100])\n",
            "\n",
            "\tscores : \n",
            "\t\t shape : torch.Size([100])\n",
            "\n",
            "\tmasks : \n",
            "\t\t shape : torch.Size([100, 1, 2048, 2048])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Train Data Output\")\n",
        "uni_inference(output, train_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "BfXGH_1X2xiY"
      },
      "outputs": [],
      "source": [
        "img, _ = test_dataset.__getitem__(test_idx)\n",
        "\n",
        "input_batch = img.unsqueeze(0)\n",
        "\n",
        "model.eval()\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "predictions = model(input_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiX7INpU24Yd",
        "outputId": "7600ef3a-24e6-42b6-b749-11d6779d0ef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Data Output\n",
            "inference no.241 : \n",
            "\tboxes : \n",
            "\t\t shape : torch.Size([100, 4])\n",
            "\n",
            "\tlabels : \n",
            "\t\t shape : torch.Size([100])\n",
            "\n",
            "\tscores : \n",
            "\t\t shape : torch.Size([100])\n",
            "\n",
            "\tmasks : \n",
            "\t\t shape : torch.Size([100, 1, 2048, 2048])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Test Data Output\")\n",
        "uni_inference(output, test_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjnVJRjh8ttS"
      },
      "source": [
        "## :construction: CustomModel :construction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew1C12kHtkEt",
        "outputId": "238a4904-1314-4a38-952d-98205d025066"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 4000\n",
              "    Root location: /content/drive/MyDrive/Task/plastic-segmentation/Data/train/image\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
              "               RandomCrop(size=(224, 224), padding=None)\n",
              "               RandomHorizontalFlip(p=0.5)\n",
              "               ToTensor()\n",
              "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
              "           )"
            ]
          },
          "execution_count": 206,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.io import read_image\n",
        "from torchvision import transforms as T\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "\n",
        "transform = T.Compose([\n",
        "        T.Resize(256),\n",
        "        T.RandomCrop(224),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) \n",
        "\n",
        "train_transform = T.Compose([\n",
        "        T.Resize(256),\n",
        "        T.RandomCrop(224),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) \n",
        "\n",
        "transform = T.Compose([\n",
        "        T.Resize(256),\n",
        "        T.RandomCrop(224),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) \n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=PATH+'/train/image', transform = train_transform)\n",
        "\n",
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "H6pP6Hx8nzzG",
        "outputId": "13ad42cf-16a3-4041-cbdc-30e48cdf7f8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 19.09it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 115.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1 of 5\n",
            "<class 'dict'> 4\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-215-76510a58c6fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (100) at non-singleton dimension 1"
          ]
        }
      ],
      "source": [
        "# No changes tried with the optimizer yet.\n",
        "MOMENTUM = 0.9\n",
        "LEARNING_RATE = 0.001\n",
        "WEIGHT_DECAY = 0.0005\n",
        "\n",
        "from typing import Any, Callable, cast, Dict, List, Optional, Tuple\n",
        "\n",
        "train_dataset = CustomDataset(root=PATH, mode = 'train')\n",
        "test_dataset = CustomDataset(root=PATH, mode='test')\n",
        "\n",
        "# pe_trainset = torchvision.datasets.ImageFolder(root = f'{PATH}/image/PE', transform=)\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "#optimizer = torch.optim.Adam(params, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "dl_train = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "dl_val = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "n_batches, n_batches_val = len(train_dataset), len(test_dataset)\n",
        "\n",
        "validation_mask_losses = []\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    print(f\"Starting epoch {epoch} of {NUM_EPOCHS}\")\n",
        "\n",
        "    time_start = time.time()\n",
        "    loss_accum = 0.0\n",
        "    loss_mask_accum = 0.0\n",
        "    loss_classifier_accum = 0.0\n",
        "    # for batch_idx, (images, targets) in enumerate(dl_train, 1):\n",
        "    for batch_idx, (images, targets) in enumerate(dl_train):\n",
        "    \n",
        "        # Predict\n",
        "        images = list(image.to(DEVICE) for image in images)\n",
        "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)[0]\n",
        "        print(type(loss_dict), len(loss_dict))\n",
        "        loss = sum(loss for loss in loss_dict.values())\n",
        "        \n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Logging\n",
        "        loss_mask = loss_dict['loss_mask'].item()\n",
        "        loss_accum += loss.item()\n",
        "        loss_mask_accum += loss_mask\n",
        "        loss_classifier_accum += loss_dict['loss_classifier'].item()\n",
        "        \n",
        "        if batch_idx % 500 == 0:\n",
        "            print(f\"    [Batch {batch_idx:3d} / {n_batches:3d}] Batch train loss: {loss.item():7.3f}. Mask-only loss: {loss_mask:7.3f}.\")\n",
        "                        \n",
        "    if USE_SCHEDULER:\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    # Train losses\n",
        "    train_loss = loss_accum / n_batches\n",
        "    train_loss_mask = loss_mask_accum / n_batches\n",
        "    train_loss_classifier = loss_classifier_accum / n_batches\n",
        "\n",
        "    # Validation\n",
        "    val_loss_accum = 0\n",
        "    val_loss_mask_accum = 0\n",
        "    val_loss_classifier_accum = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, targets) in enumerate(dl_val, 1):\n",
        "            images = list(image.to(DEVICE) for image in images)\n",
        "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            val_loss_dict = model(images, targets)\n",
        "            val_batch_loss = sum(loss for loss in val_loss_dict.values())\n",
        "            val_loss_accum += val_batch_loss.item()\n",
        "            val_loss_mask_accum += val_loss_dict['loss_mask'].item()\n",
        "            val_loss_classifier_accum += val_loss_dict['loss_classifier'].item()\n",
        "\n",
        "    # Validation losses\n",
        "    val_loss = val_loss_accum / n_batches_val\n",
        "    val_loss_mask = val_loss_mask_accum / n_batches_val\n",
        "    val_loss_classifier = val_loss_classifier_accum / n_batches_val\n",
        "    elapsed = time.time() - time_start\n",
        "\n",
        "    validation_mask_losses.append(val_loss_mask)\n",
        "\n",
        "    torch.save(model.state_dict(), f\"pytorch_model-e{epoch}.bin\")\n",
        "    prefix = f\"[Epoch {epoch:2d} / {NUM_EPOCHS:2d}]\"\n",
        "    print(prefix)\n",
        "    print(f\"{prefix} Train mask-only loss: {train_loss_mask:7.3f}, classifier loss {train_loss_classifier:7.3f}\")\n",
        "    print(f\"{prefix} Val mask-only loss  : {val_loss_mask:7.3f}, classifier loss {val_loss_classifier:7.3f}\")\n",
        "    print(prefix)\n",
        "    print(f\"{prefix} Train loss: {train_loss:7.3f}. Val loss: {val_loss:7.3f} [{elapsed:.0f} secs]\")\n",
        "    print(prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2maBa_NxlA3P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.transforms.functional as F\n",
        "from pathlib import Path    # sys가 인식할 수 있도록 경로화 시킴???\n",
        "\n",
        "\n",
        "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
        "\n",
        "\n",
        "def show(imgs):\n",
        "    if not isinstance(imgs, list):\n",
        "        imgs = [imgs]\n",
        "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
        "    for i, img in enumerate(imgs):\n",
        "        img = img.detach()\n",
        "        img = F.to_pil_image(img)\n",
        "        axs[0, i].imshow(np.asarray(img))\n",
        "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cERPRR_xjHuI"
      },
      "source": [
        "# additional refs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku5LaQvW8GnM"
      },
      "source": [
        "## utilites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "kygamPn-tKFQ"
      },
      "outputs": [],
      "source": [
        "# ref: https://www.kaggle.com/inversion/run-length-decoding-quick-start\n",
        "def rle_decode(mask_rle, shape, color=1):\n",
        "    '''\n",
        "    mask_rle: run-length as string formated (start length)\n",
        "    shape: (height, width, channels) of array to return\n",
        "    color: color for the mask\n",
        "    Returns numpy array (mask)\n",
        "\n",
        "    '''\n",
        "    s = mask_rle.split()\n",
        "\n",
        "    starts = list(map(lambda x: int(x) - 1, s[0::2]))\n",
        "    lengths = list(map(int, s[1::2]))\n",
        "    ends = [x + y for x, y in zip(starts, lengths)]\n",
        "    if len(shape)==3:\n",
        "        img = np.zeros((shape[0] * shape[1], shape[2]), dtype=np.float32)\n",
        "    else:\n",
        "        img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n",
        "    for start, end in zip(starts, ends):\n",
        "        img[start : end] = color\n",
        "\n",
        "    return img.reshape(shape)\n",
        "\n",
        "\n",
        "def rle_encoding(x):\n",
        "    dots = np.where(x.flatten() == 1)[0]\n",
        "    run_lengths = []\n",
        "    prev = -2\n",
        "    for b in dots:\n",
        "        if (b>prev+1): run_lengths.extend((b + 1, 0))\n",
        "        run_lengths[-1] += 1\n",
        "        prev = b\n",
        "    return ' '.join(map(str, run_lengths))\n",
        "\n",
        "\n",
        "def remove_overlapping_pixels(mask, other_masks):\n",
        "    for other_mask in other_masks:\n",
        "        if np.sum(np.logical_and(mask, other_mask)) > 0:\n",
        "            mask[np.logical_and(mask, other_mask)] = 0\n",
        "    return mask\n",
        "\n",
        "def combine_masks(masks, mask_threshold):\n",
        "    \"\"\"\n",
        "    combine masks into one image\n",
        "    \"\"\"\n",
        "    maskimg = np.zeros((HEIGHT, WIDTH))\n",
        "    # print(len(masks.shape), masks.shape)\n",
        "    for m, mask in enumerate(masks,1):\n",
        "        maskimg[mask>mask_threshold] = m\n",
        "    return maskimg\n",
        "\n",
        "\n",
        "def get_filtered_masks(pred):\n",
        "    \"\"\"\n",
        "    filter masks using MIN_SCORE for mask and MAX_THRESHOLD for pixels\n",
        "    \"\"\"\n",
        "    use_masks = []   \n",
        "    for i, mask in enumerate(pred[\"masks\"]):\n",
        "\n",
        "        # Filter-out low-scoring results. Not tried yet.\n",
        "        scr = pred[\"scores\"][i].cpu().item()\n",
        "        label = pred[\"labels\"][i].cpu().item()\n",
        "        if scr > min_score_dict[label]:\n",
        "            mask = mask.cpu().numpy().squeeze()\n",
        "            # Keep only highly likely pixels\n",
        "            binary_mask = mask > mask_threshold_dict[label]\n",
        "            binary_mask = remove_overlapping_pixels(binary_mask, use_masks)\n",
        "            use_masks.append(binary_mask)\n",
        "\n",
        "    return use_masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt8JcfoR8On7"
      },
      "source": [
        "## Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSJSlhn18Mkx"
      },
      "outputs": [],
      "source": [
        "# Metric: mean of the precision values at each IoU threshold?\n",
        "# Ref: https://www.kaggle.com/theoviel/competition-metric-map-iou\n",
        "\n",
        "def compute_iou(labels, y_pred, verbose=0):\n",
        "    \"\"\"\n",
        "    Computes the IoU for instance labels and predictions.\n",
        "\n",
        "    Args:\n",
        "        labels (np array): Labels.\n",
        "        y_pred (np array): predictions\n",
        "\n",
        "    Returns:\n",
        "        np array: IoU matrix, of size true_objects x pred_objects.\n",
        "    \"\"\"\n",
        "\n",
        "    true_objects = len(np.unique(labels))\n",
        "    pred_objects = len(np.unique(y_pred))\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Number of true objects: {}\".format(true_objects))\n",
        "        print(\"Number of predicted objects: {}\".format(pred_objects))\n",
        "\n",
        "    # Compute intersection between all objects\n",
        "    intersection = np.histogram2d(\n",
        "        labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects)\n",
        "    )[0]\n",
        "\n",
        "    # Compute areas (needed for finding the union between all objects)\n",
        "    area_true = np.histogram(labels, bins=true_objects)[0]\n",
        "    area_pred = np.histogram(y_pred, bins=pred_objects)[0]\n",
        "    area_true = np.expand_dims(area_true, -1)\n",
        "    area_pred = np.expand_dims(area_pred, 0)\n",
        "\n",
        "    # Compute union\n",
        "    union = area_true + area_pred - intersection\n",
        "    intersection = intersection[1:, 1:] # exclude background\n",
        "    union = union[1:, 1:]\n",
        "    union[union == 0] = 1e-9\n",
        "    iou = intersection / union\n",
        "    \n",
        "    return iou  \n",
        "\n",
        "def precision_at(threshold, iou):\n",
        "    \"\"\"\n",
        "    Computes the precision at a given threshold.\n",
        "\n",
        "    Args:\n",
        "        threshold (float): Threshold.\n",
        "        iou (np array): IoU matrix.\n",
        "\n",
        "    Returns:\n",
        "        int: Number of true positives,\n",
        "        int: Number of false positives,\n",
        "        int: Number of false negatives.\n",
        "    \"\"\"\n",
        "    matches = iou > threshold\n",
        "    true_positives = np.sum(matches, axis=1) == 1  # Correct objects\n",
        "    false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
        "    false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
        "    tp, fp, fn = (\n",
        "        np.sum(true_positives),\n",
        "        np.sum(false_positives),\n",
        "        np.sum(false_negatives),\n",
        "    )\n",
        "    return tp, fp, fn\n",
        "\n",
        "def iou_map(truths, preds, verbose=0):\n",
        "    \"\"\"\n",
        "    Computes the metric for the competition.\n",
        "    Masks contain the segmented pixels where each object has one value associated,\n",
        "    and 0 is the background.\n",
        "\n",
        "    Args:\n",
        "        truths (list of masks): Ground truths.\n",
        "        preds (list of masks): Predictions.\n",
        "        verbose (int, optional): Whether to print infos. Defaults to 0.\n",
        "\n",
        "    Returns:\n",
        "        float: mAP.\n",
        "    \"\"\"\n",
        "    ious = [compute_iou(truth, pred, verbose) for truth, pred in zip(truths, preds)]\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
        "\n",
        "    prec = []\n",
        "    for t in np.arange(0.5, 1.0, 0.05):\n",
        "        tps, fps, fns = 0, 0, 0\n",
        "        for iou in ious:\n",
        "            tp, fp, fn = precision_at(t, iou)\n",
        "            tps += tp\n",
        "            fps += fp\n",
        "            fns += fn\n",
        "\n",
        "        p = tps / (tps + fps + fns)\n",
        "        prec.append(p)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tps, fps, fns, p))\n",
        "\n",
        "    if verbose:\n",
        "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
        "\n",
        "    return np.mean(prec)\n",
        "\n",
        "\n",
        "def get_score(ds, mdl):\n",
        "    \"\"\"\n",
        "    Get average IOU mAP score for a dataset\n",
        "    \"\"\"\n",
        "    mdl.eval()\n",
        "    iouscore = 0\n",
        "    for i in tqdm(range(len(ds))):\n",
        "        img, targets = ds[i]\n",
        "        with torch.no_grad():\n",
        "            result = mdl([img.to(DEVICE)])[0]\n",
        "            \n",
        "        masks = combine_masks(targets['masks'], 0.5)\n",
        "        labels = pd.Series(result['labels'].cpu().numpy()).value_counts()\n",
        "\n",
        "        mask_threshold = mask_threshold_dict[labels.sort_values().index[-1]]\n",
        "        pred_masks = combine_masks(get_filtered_masks(result), mask_threshold)\n",
        "        iouscore += iou_map([masks],[pred_masks])\n",
        "    return iouscore / len(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCHlgi-48R38"
      },
      "source": [
        "## augmentation_ver1\n",
        "\n",
        "* These are slight redefinitions of torch.transformation classes\n",
        "* The difference is that they handle the target and the mask\n",
        "* Copied from Abishek, added new ones\n",
        "* ref : https://www.kaggle.com/rluethy/sartorius-torch-mask-r-cnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "HKkFlnmIthmU"
      },
      "outputs": [],
      "source": [
        "class Compose:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "class VerticalFlip:\n",
        "    def __init__(self, prob):\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() < self.prob:\n",
        "            height, width = image.shape[-2:]\n",
        "            image = image.flip(-2)\n",
        "            bbox = target[\"boxes\"]\n",
        "            bbox[:, [1, 3]] = height - bbox[:, [3, 1]]\n",
        "            target[\"boxes\"] = bbox\n",
        "            target[\"masks\"] = target[\"masks\"].flip(-2)\n",
        "        return image, target\n",
        "\n",
        "class HorizontalFlip:\n",
        "    def __init__(self, prob):\n",
        "        self.prob = prob\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        if random.random() < self.prob:\n",
        "            height, width = image.shape[-2:]\n",
        "            image = image.flip(-1)\n",
        "            bbox = target[\"boxes\"]\n",
        "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
        "            target[\"boxes\"] = bbox\n",
        "            target[\"masks\"] = target[\"masks\"].flip(-1)\n",
        "        return image, target\n",
        "\n",
        "class Normalize:\n",
        "    def __call__(self, image, target):\n",
        "        image = F.normalize(image, RESNET_MEAN, RESNET_STD)\n",
        "        return image, target\n",
        "\n",
        "class ToTensor:\n",
        "    def __call__(self, image, target):\n",
        "        image = F.to_tensor(image)\n",
        "        return image, target\n",
        "    \n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = [ToTensor()]\n",
        "    if NORMALIZE:\n",
        "        transforms.append(Normalize())\n",
        "    \n",
        "    # Data augmentation for train\n",
        "    if train: \n",
        "        transforms.append(HorizontalFlip(0.5))\n",
        "        transforms.append(VerticalFlip(0.5))\n",
        "\n",
        "    return Compose(transforms)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "nCqJ5Eqm9eNG",
        "bTQpoJtE9hEs",
        "94ppeImr_vhX",
        "i6RrxAFLKzKP",
        "C9t3CnT-gkR8",
        "5wbbcgOl0hLG",
        "awosI9-Sm-Oz",
        "ku5LaQvW8GnM",
        "Xt8JcfoR8On7",
        "lCHlgi-48R38"
      ],
      "machine_shape": "hm",
      "name": "Ensemble.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0031618e7ce248e099bbc9bd174c46ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4abd4777ed24a02811c25656be9d4ba",
            "max": 178090079,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc681dc64df9422c984cc3b60967a202",
            "value": 178090079
          }
        },
        "06cc812aed094bf99d1e29e6dafb2fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ce40bc391ea4631b15268dd2bf14835",
            "placeholder": "​",
            "style": "IPY_MODEL_4074970e56c34bb2903a5b3bc6577844",
            "value": "100%"
          }
        },
        "0d63decfc2914604ac3471ba0937b561": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1111615f04104ee79a439a0d87a7c867": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "12a66ff7d7a44b2c9306c86dbb63ed7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12c43f1f217644a6ae8030e3af447965": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1959a4b17ae64b14bb57f4e512f50e32": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23cff78dba2347d5884cc5cf574af714": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3db037d77c6d407680d2722bb2e315c2",
            "placeholder": "​",
            "style": "IPY_MODEL_cb8b52c5aff54d8ea4487c3d1a2230cd",
            "value": " 13.6M/13.6M [00:00&lt;00:00, 17.3MB/s]"
          }
        },
        "24b5fc4d2f9849f3bd8fbb63ac2b217c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25d806d99eb744f9b8cc5fada96fd6db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31f08a689bf24b6d83db4da1902fa79c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06cc812aed094bf99d1e29e6dafb2fa3",
              "IPY_MODEL_97dab76860eb48878a20cd2e045c9ee9",
              "IPY_MODEL_23cff78dba2347d5884cc5cf574af714"
            ],
            "layout": "IPY_MODEL_0d63decfc2914604ac3471ba0937b561"
          }
        },
        "391875e5a644469ca840eeb994aa3218": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bb6ffd2d00342d09c9d45c3ff5bc24c",
            "placeholder": "​",
            "style": "IPY_MODEL_fd0b40f45aeb4cdea3339f342bc6f724",
            "value": "100%"
          }
        },
        "3db037d77c6d407680d2722bb2e315c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3df1de1be286450182d84d34e52ea5a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c900504bc034a89b2a031bba849bdef",
            "placeholder": "​",
            "style": "IPY_MODEL_12a66ff7d7a44b2c9306c86dbb63ed7a",
            "value": " 170M/170M [00:01&lt;00:00, 121MB/s]"
          }
        },
        "4074970e56c34bb2903a5b3bc6577844": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c900504bc034a89b2a031bba849bdef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a329e6d789b4916ac72b31cf8d5aed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1959a4b17ae64b14bb57f4e512f50e32",
            "placeholder": "​",
            "style": "IPY_MODEL_12c43f1f217644a6ae8030e3af447965",
            "value": " 170M/170M [00:07&lt;00:00, 24.9MB/s]"
          }
        },
        "621a3f0d353441d9a1f9b5317366da60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bb6ffd2d00342d09c9d45c3ff5bc24c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "756e552563c649ca8566a6b1fe053634": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82d85325dac8460ebf9a5a4afee5b0d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_391875e5a644469ca840eeb994aa3218",
              "IPY_MODEL_0031618e7ce248e099bbc9bd174c46ad",
              "IPY_MODEL_5a329e6d789b4916ac72b31cf8d5aed9"
            ],
            "layout": "IPY_MODEL_756e552563c649ca8566a6b1fe053634"
          }
        },
        "8ce40bc391ea4631b15268dd2bf14835": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fd8ec36da0d4ad3b47e4f329468a6b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24b5fc4d2f9849f3bd8fbb63ac2b217c",
            "max": 178090079,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1111615f04104ee79a439a0d87a7c867",
            "value": 178090079
          }
        },
        "97dab76860eb48878a20cd2e045c9ee9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae481db850a345b2900d56a48d4f9247",
            "max": 14212972,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da233f6f3df744768e7fe4217da70252",
            "value": 14212972
          }
        },
        "ae481db850a345b2900d56a48d4f9247": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc681dc64df9422c984cc3b60967a202": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c683d5910bf2427a918ef6918291fc5e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb8b52c5aff54d8ea4487c3d1a2230cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf7978919cbf48488745cbf4c0048bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fdb0330c8cb648c1b966fb67de915ebf",
              "IPY_MODEL_8fd8ec36da0d4ad3b47e4f329468a6b4",
              "IPY_MODEL_3df1de1be286450182d84d34e52ea5a8"
            ],
            "layout": "IPY_MODEL_c683d5910bf2427a918ef6918291fc5e"
          }
        },
        "d4abd4777ed24a02811c25656be9d4ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da233f6f3df744768e7fe4217da70252": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd0b40f45aeb4cdea3339f342bc6f724": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdb0330c8cb648c1b966fb67de915ebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_621a3f0d353441d9a1f9b5317366da60",
            "placeholder": "​",
            "style": "IPY_MODEL_25d806d99eb744f9b8cc5fada96fd6db",
            "value": "100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
