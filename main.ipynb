{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPKObLFPiybFbvxZf8sbWMs"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "498bfcf5d9244b32afb4fc54853cdf6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6b85a50f0ce24d6bab604c0e43f8c7e1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_82934452cc4c44c2a0df4a8273a9978f",
              "IPY_MODEL_03a545ac26524b90806c3fc1794eed33",
              "IPY_MODEL_418c20d344d247eb8b704413422e4c4f"
            ]
          }
        },
        "6b85a50f0ce24d6bab604c0e43f8c7e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "82934452cc4c44c2a0df4a8273a9978f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eac9c3f04f634cd5a8e473c5a24d8304",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a8972f7961e149f0b3275fa223802d00"
          }
        },
        "03a545ac26524b90806c3fc1794eed33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_030cde431aa749af93d27d08e5fa7d29",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 167502836,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 167502836,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d5a8b6e8cde0496aabf9b87bcfd395e6"
          }
        },
        "418c20d344d247eb8b704413422e4c4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c701665cd58b444b98355410936dc46d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 160M/160M [00:02&lt;00:00, 78.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cd9a4e21757841be960631fcc345588b"
          }
        },
        "eac9c3f04f634cd5a8e473c5a24d8304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a8972f7961e149f0b3275fa223802d00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "030cde431aa749af93d27d08e5fa7d29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d5a8b6e8cde0496aabf9b87bcfd395e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c701665cd58b444b98355410936dc46d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cd9a4e21757841be960631fcc345588b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyyjXDjeMY3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e0e92bc-fdea-489e-a2df-7489f3db5915"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "from os.path import join as jn\n",
        "import shutil\n",
        "\n",
        "# 구글 드라이브 접근\n",
        "ROOT = \"/content/drive\"\n",
        "try:\n",
        "  drive.mount(ROOT, force_remount=True)\n",
        "except:\n",
        "  drive.mount(ROOT)\n",
        "\n",
        "# 본인 경로에 맞게 수정하면 됨.\n",
        "PATH = jn(ROOT, \"MyDrive/Task/plastic-segmentation/Data\")\n",
        "# PATH = jn(ROOT, \"MyDrive/Task/plastic-segmentation/Sample_data\")\n",
        "\n",
        "# 구글 드라이브 경로에서 '/content/sample_data/data\"로 복사 시\n",
        "# shutil.copytree(PATH +\"/train\", \"./sample_data/data/train\")\n",
        "# shutil.copytree(PATH +\"/test\", \"./sample_data/data/test\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmU1FEV5Pb9W"
      },
      "source": [
        "# init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7DuTJMEKYWI"
      },
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "\n",
        "import numpy as np\n",
        "# import pandas as pd\n",
        "import cv2 as cv\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch  import nn, Tensor\n",
        "\n",
        "BASE = os.getcwd()\n",
        "PATH = PATH # gdrive\n",
        "# PATH = f\"{BASE}/sample_data/data\"   # colab\n",
        "# PATH = f\"{BASE}/assets/data\"        # github"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U27Zh1SIRqaL",
        "outputId": "628949d1-2447-44bb-dc74-53d57143bb77"
      },
      "source": [
        "# version 1 : @qkrwjdduf159\n",
        "\n",
        "## train\n",
        "trn_path = f\"{PATH}/train/annotation\"\n",
        "file_list = os.listdir(trn_path)\n",
        "\n",
        "train_files = []\n",
        "for file in tqdm(file_list):\n",
        "    dir = trn_path + '/' + file\n",
        "    json_list = glob(dir + '/' + '*.json')\n",
        "    train_files.append(json_list)\n",
        "\n",
        "train_json_list = []\n",
        "for files in tqdm(train_files):\n",
        "    for json_file in tqdm(files):\n",
        "        train_json_list.append(json_file)\n",
        "\n",
        "## test\n",
        "tst_PATH = f\"{PATH}/test/annotations\"\n",
        "file_list = os.listdir(tst_PATH)\n",
        "\n",
        "test_files = []\n",
        "for file in tqdm(file_list):\n",
        "    dir = tst_PATH + '/' + file\n",
        "    json_list = glob(dir + '/' + '*.json')\n",
        "    test_files.append(json_list)\n",
        "\n",
        "test_json_list = []\n",
        "for files in tqdm(test_files):\n",
        "    for json_file in tqdm(files):\n",
        "        test_json_list.append(json_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 85.26it/s]\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 1985939.39it/s]\n",
            "\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 1808669.25it/s]\n",
            "\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 1076290.48it/s]\n",
            "\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 1889326.13it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00, 164.44it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00, 382.56it/s]\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\n",
            "100%|██████████| 100/100 [00:00<00:00, 594094.05it/s]\n",
            "\n",
            "100%|██████████| 100/100 [00:00<00:00, 884874.26it/s]\n",
            "\n",
            "100%|██████████| 100/100 [00:00<00:00, 651289.44it/s]\n",
            "\n",
            "100%|██████████| 100/100 [00:00<00:00, 701388.63it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00, 189.45it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAwIck4sT5G4",
        "outputId": "74b11e3c-5fd3-4ea9-9786-a9fab33e4df3"
      },
      "source": [
        "# version 2 @AshbeeKim\n",
        "trn = os.path.join(PATH, \"train\")\n",
        "tst = os.path.join(PATH, \"test\")\n",
        "\n",
        "baseDF = {\"kind\" : [], \"label\" : [], \"metainfo_id\" : [], \"feature\" : [], \"image_path\" : [], \"annot_path\": []}\n",
        "trn_num, tst_num = 0, 0\n",
        "for fpath in tqdm([trn, tst]):\n",
        "    kind = os.path.basename(fpath)\n",
        "    BDIR = sorted(os.listdir(fpath))    # image, annotation(s)\n",
        "    for bdir in BDIR:\n",
        "        for dirs in sorted(os.listdir(jn(fpath, bdir))):\n",
        "            dpath = jn(fpath, bdir, dirs)\n",
        "            if bdir.lower()=='image':\n",
        "                paths = sorted(glob(dpath + \"/*.jpg\"))\n",
        "                baseDF['image_path'].extend(paths)\n",
        "                kinds = [kind for cnt in range(len(paths))]\n",
        "                baseDF['kind'].extend(kinds)\n",
        "                if kind=='train' : trn_num += len(paths)\n",
        "                elif kind=='test' : tst_num += len(paths)\n",
        "                fnames = [os.path.basename(fname) for fname in paths]\n",
        "                labels = list(map(lambda x: (x.split('_')[0]), fnames))\n",
        "                baseDF['label'].extend(labels)\n",
        "                metaIds = list(map(lambda x: int(x.split('_')[1]), fnames))\n",
        "                baseDF['metainfo_id'].extend(metaIds)\n",
        "                feats = list(map(lambda x: int(x.split('_')[-1][:-4]), fnames))\n",
        "                baseDF['feature'].extend(feats)\n",
        "            else:\n",
        "                paths = sorted(glob(dpath + \"/*.json\"))\n",
        "                baseDF['annot_path'].extend(paths)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:17<00:00,  8.51s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-sChDpFof_v"
      },
      "source": [
        "# 수정 point_1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV0fT9zqWbFi"
      },
      "source": [
        "# cotributed by @qkrwjdduf156, @AshbeeKim\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# 추가 수정 중\n",
        "# from torchvision import datasets\n",
        "# torch.utils.data.Dataset\n",
        "class CustomDataset:\n",
        "    def __init__(self, base_info, transforms = None, mode = 'train', train_size = None, test_size = None):\n",
        "        self.mode = mode.lower()\n",
        "        self.infos = base_info\n",
        "        self.transforms = transforms\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self.images = (self.infos['image_path'])[:train_size]\n",
        "            self.annots = (self.infos['annot_path'])[:train_size]\n",
        "\n",
        "        elif self.mode == 'test':\n",
        "            self.images = (self.infos['image_path'])[train_size:train_size+test_size]\n",
        "            self.annots = (self.infos['annot_path'])[train_size:train_size+test_size]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        json_path = self.annots[idx]\n",
        "        image_path = self.images[idx]\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            image = Image.open(image_path).convert('RGB')   # (C, H, W)\n",
        "            mask = np.zeros(image.copy())   # (C, H, W) # BK\n",
        "            \n",
        "            with open(json_path, 'r') as f:\n",
        "                annot = json.loads(f.read())\n",
        "\n",
        "            boxes = []\n",
        "            segmentations = []\n",
        "            labels = []\n",
        "            for i in range(len(annot['annotations'])):\n",
        "\n",
        "                segmentation = annot['annotations'][i]['segmentation'][0]\n",
        "                bbox = annot['annotations'][i]['bbox']\n",
        "                label = annot['annotations'][i]['category_id']\n",
        "                xmin, ymin, width, height = bbox[0],bbox[1],bbox[2],bbox[3]\n",
        "                xmin, ymin, xmax, ymax = xmin, ymin, xmin + width, ymin + height\n",
        "                \n",
        "                boxes.append([xmin, ymin, xmax, ymax])\n",
        "                segmentations.append([segmentation])\n",
        "                labels.append(label)\n",
        "\n",
        "            target = {}\n",
        "            target['boxes'] = torch.as_tensor(boxes, dtype = torch.float32)\n",
        "            target['labels'] = torch.ones((labels, ), dtype = torch.int64)\n",
        "            target['segmentation'] = segmentations\n",
        "            target['image_id'] = image_path.split('/')[-1]\n",
        "\n",
        "            if self.transforms is not None:\n",
        "                image, target = self.transforms(image, target)\n",
        "\n",
        "            return image, target\n",
        "\n",
        "        elif self.model == 'test':\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            with open(json_path, 'r') as f:\n",
        "                annot = json.loads(f.read())\n",
        "            \n",
        "            target = {}\n",
        "            target['image_id'] = image_path.split('/')[-1]\n",
        "            return image, target\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5_cZc_96kJW"
      },
      "source": [
        "* fastrcnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "498bfcf5d9244b32afb4fc54853cdf6f",
            "6b85a50f0ce24d6bab604c0e43f8c7e1",
            "82934452cc4c44c2a0df4a8273a9978f",
            "03a545ac26524b90806c3fc1794eed33",
            "418c20d344d247eb8b704413422e4c4f",
            "eac9c3f04f634cd5a8e473c5a24d8304",
            "a8972f7961e149f0b3275fa223802d00",
            "030cde431aa749af93d27d08e5fa7d29",
            "d5a8b6e8cde0496aabf9b87bcfd395e6",
            "c701665cd58b444b98355410936dc46d",
            "cd9a4e21757841be960631fcc345588b"
          ]
        },
        "id": "k3Ncq75MYFD3",
        "outputId": "aa068897-274f-4983-8485-d75cbcac672c"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True)\n",
        "\n",
        "num_classes = 5 # 4 class + background\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "498bfcf5d9244b32afb4fc54853cdf6f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/160M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6ej_R1v1loS",
        "outputId": "dc50a34d-a8a6-4c8d-e3fa-801e2bff5429"
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=20, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgWAY6VWz-gF"
      },
      "source": [
        "from torchvision import transforms as T\n",
        "\n",
        "train_dataset = CustomDataset(baseDF, mode = 'train', train_size=trn_num, test_size=tst_num)\n",
        "input_image = train_dataset.__getitem__(23)[0]\n",
        "preprocess = T.Compose([\n",
        "                        T.Resize(256),\n",
        "                        T.ToTensor(),\n",
        "                        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0) # torch.Size([1, 3, 2048, 2048]) >>> (resize to 256*256) >>> torch.Size([1, 3, 256, 256])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ65FG7v2B1f"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     output = model(input_batch)[0]['out'][0]\n",
        "# output_predictions = output.argmax(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9oP71oR2lgt",
        "outputId": "88990555-0a89-4b4e-999d-a3c20ba480c6"
      },
      "source": [
        "model(input_batch)[0]   # dict_keys(['boxes', 'labels', 'scores'])\n",
        "model(input_batch)[0]['scores']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5513, 0.4587, 0.4472, 0.4444, 0.4397, 0.4277, 0.4276, 0.4151, 0.4149,\n",
              "        0.4126, 0.4120, 0.4078, 0.4017, 0.4017, 0.4007, 0.3998, 0.3976, 0.3965,\n",
              "        0.3963, 0.3960, 0.3958, 0.3923, 0.3875, 0.3839, 0.3839, 0.3820, 0.3797,\n",
              "        0.3789, 0.3783, 0.3759, 0.3736, 0.3726, 0.3724, 0.3722, 0.3718, 0.3717,\n",
              "        0.3709, 0.3704, 0.3673, 0.3659, 0.3648, 0.3642, 0.3631, 0.3614, 0.3596,\n",
              "        0.3594, 0.3581, 0.3549, 0.3534, 0.3532, 0.3499, 0.3498, 0.3497, 0.3480,\n",
              "        0.3475, 0.3445, 0.3434, 0.3428, 0.3425, 0.3421, 0.3411, 0.3409, 0.3401,\n",
              "        0.3392, 0.3376, 0.3362, 0.3360, 0.3353, 0.3339, 0.3338, 0.3326, 0.3325,\n",
              "        0.3325, 0.3324, 0.3323, 0.3312, 0.3307, 0.3303, 0.3299, 0.3284, 0.3274,\n",
              "        0.3271, 0.3267, 0.3264, 0.3256, 0.3245, 0.3236, 0.3232, 0.3231, 0.3224,\n",
              "        0.3209, 0.3207, 0.3199, 0.3197, 0.3191, 0.3191, 0.3190, 0.3186, 0.3184,\n",
              "        0.3183], device='cuda:0', grad_fn=<IndexBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyjqHOKY6dwg"
      },
      "source": [
        "* unet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-BQ5fZR4qw4",
        "outputId": "691abd86-aba9-4e32-fa90-d14694b999fe"
      },
      "source": [
        "import torch\n",
        "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
        "    in_channels=3, out_channels=1, init_features=32, pretrained=True)\n",
        "\n",
        "\n",
        "from torchvision import transforms as T\n",
        "\n",
        "train_dataset = CustomDataset(baseDF, mode = 'train', train_size=trn_num, test_size=tst_num)\n",
        "input_image = train_dataset.__getitem__(23)\n",
        "m, s = np.mean(input_image[0], axis=(0, 1)), np.std(input_image[0], axis=(0, 1))\n",
        "preprocess = T.Compose([\n",
        "                        # T.Resize(256),\n",
        "                        T.ToTensor(),\n",
        "                        T.Normalize(mean=m, std=s),\n",
        "])\n",
        "input_tensor = preprocess(input_image[0])\n",
        "input_batch = input_tensor.unsqueeze(0) # torch.Size([1, 3, 2048, 2048]) >>> (resize to 256*256) >>> torch.Size([1, 3, 256, 256])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8BPRP955pOW"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model = model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AghOiG186BoQ",
        "outputId": "13c68142-dd85-4e0e-a25d-22b224a708ca"
      },
      "source": [
        "print(torch.round(output))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M16wyt-I6om_",
        "outputId": "76a7b52e-f7ee-4c39-9ea6-3fa38c5b4d61"
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (encoder1): Sequential(\n",
              "    (enc1conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc1relu1): ReLU(inplace=True)\n",
              "    (enc1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc1relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (encoder2): Sequential(\n",
              "    (enc2conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc2relu1): ReLU(inplace=True)\n",
              "    (enc2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc2relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (encoder3): Sequential(\n",
              "    (enc3conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc3relu1): ReLU(inplace=True)\n",
              "    (enc3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc3relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (encoder4): Sequential(\n",
              "    (enc4conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc4relu1): ReLU(inplace=True)\n",
              "    (enc4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (enc4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (enc4relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (bottleneck): Sequential(\n",
              "    (bottleneckconv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bottlenecknorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bottleneckrelu1): ReLU(inplace=True)\n",
              "    (bottleneckconv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bottlenecknorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bottleneckrelu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (upconv4): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (decoder4): Sequential(\n",
              "    (dec4conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec4relu1): ReLU(inplace=True)\n",
              "    (dec4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec4relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (decoder3): Sequential(\n",
              "    (dec3conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec3relu1): ReLU(inplace=True)\n",
              "    (dec3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec3relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (upconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (decoder2): Sequential(\n",
              "    (dec2conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec2relu1): ReLU(inplace=True)\n",
              "    (dec2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec2relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (upconv1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
              "  (decoder1): Sequential(\n",
              "    (dec1conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec1relu1): ReLU(inplace=True)\n",
              "    (dec1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (dec1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (dec1relu2): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vad4WC856tYk"
      },
      "source": [
        "* fasterrcnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXUlDmt9KfSF"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "backbone.out_channels = 1280\n",
        "\n",
        "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "                                                output_size=7,\n",
        "                                                sampling_ratio=2)\n",
        "\n",
        "model = FasterRCNN(backbone,\n",
        "                   num_classes=5,\n",
        "                   rpn_anchor_generator=anchor_generator,\n",
        "                   box_roi_pool=roi_pooler)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U_TNy1YHpby"
      },
      "source": [
        "from torchvision import transforms as T\n",
        "\n",
        "train_dataset = CustomDataset(baseDF, mode = 'train', train_size=trn_num, test_size=tst_num)\n",
        "input_image = train_dataset.__getitem__(23)[0]\n",
        "m, s = np.mean(input_image, axis=(0, 1)), np.std(input_image, axis=(0, 1))\n",
        "preprocess = T.Compose([\n",
        "                        # T.Resize(256),\n",
        "                        T.ToTensor(),\n",
        "                        T.Normalize(mean=m, std=s),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0) # torch.Size([1, 3, 2048, 2048]) >>> (resize to 256*256) >>> torch.Size([1, 3, 256, 256])"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjnVJRjh8ttS"
      },
      "source": [
        "*** 현재 수정 중 ***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh7oeXsNH7lD"
      },
      "source": [
        "# if torch.cuda.is_available():\n",
        "#     input_batch = input_batch.to('cuda')\n",
        "#     model = model.to('cuda')\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     output = model(input_batch)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5JE0-hzHgsT"
      },
      "source": [
        "* maskrcnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShLv6LCaLtjN"
      },
      "source": [
        "# %%writefile ./CaliberDetector.py    # customize 끝내고 run\n",
        "\n",
        "# detection\n",
        "from collection import OrderedDict\n",
        "from torch import nn\n",
        "from torchvision.ops import MultiScaleRoIAlign\n",
        "from torchvision._internally_replaced_utils import load_state_dict_from_url\n",
        "from torchvision.models.detection._utils import overwrite_eps\n",
        "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone, _validate_trainable_layers\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2OEzmOxKvOc"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "# MaskRCNN\n",
        "# MaskRCNNHeads\n",
        "\n",
        "def get_model_instance_segmentation(num_classes):\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWiv1tKe7_82",
        "outputId": "8f4a28f7-e0cb-42e6-e0e5-9568f5608e64"
      },
      "source": [
        "!pip install -U torchvision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYtUes5nLC_N"
      },
      "source": [
        "from torchvision import utils\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "dataset = CustomDataset(baseDF, mode = 'train', train_size=trn_num, test_size=tst_num)\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        " dataset, batch_size=5, shuffle=True, num_workers=4,\n",
        " collate_fn=utils.data._utils.collate)\n",
        "# For Training\n",
        "images,targets = next(iter(data_loader))\n",
        "images = list(image for image in images)\n",
        "targets = [{k: v for k, v in t.items()} for t in targets]\n",
        "output = model(images,targets)   # Returns losses and detections\n",
        "# For inference\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)           # Returns predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWQX5XMbtPq-"
      },
      "source": [
        "from torchvision iumport datasets\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=PATH+'/train', trainsform = train_trainsform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZthpwS8JaJQ"
      },
      "source": [
        "from torchvision import transforms as T\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "\n",
        "transform = T.Compose([\n",
        "        T.Resize(256),\n",
        "        T.RandomCrop(224),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) \n",
        "\n",
        "train_transform = T.Compose([\n",
        "        T.Resize(256),\n",
        "        T.RandomCrop(224),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) \n",
        "\n",
        "transform = T.Compose([\n",
        "        T.Resize(256),\n",
        "        T.RandomCrop(224),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7Vhlh_1DerZ"
      },
      "source": [
        "import torch\n",
        "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
        "    in_channels=3, out_channels=1, init_features=32, pretrained=True)\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "# print(type(train_dataset))\n",
        "input_image = train_dataset.__getitem__(23)[0]\n",
        "\n",
        "input_image = np.squeeze(input_image.numpy())\n",
        "# print(type(input_image[0]))\n",
        "m, s = np.mean(input_image[0], axis=(0, 1)), np.std(input_image[0], axis=(0, 1))\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=m, std=s),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model = model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)\n",
        "\n",
        "print(torch.round(output[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMBWmkAAEIU-",
        "outputId": "00ae5179-c451-4482-9d4d-a2095f89355a"
      },
      "source": [
        "input_image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PIL.Image.Image image mode=RGB size=2048x2048 at 0x7F003D7937D0>,\n",
              " {'boxes': [[1134, 765, 1322, 882],\n",
              "   [1359, 1086, 1548, 1394],\n",
              "   [768, 1044, 925, 1265]],\n",
              "  'image_id': 'PE_074_121.jpg',\n",
              "  'labels': [tensor(4), tensor(4), tensor(4)],\n",
              "  'segmentation': [[[1167,\n",
              "     765,\n",
              "     1160,\n",
              "     766,\n",
              "     1153,\n",
              "     766,\n",
              "     1146,\n",
              "     767,\n",
              "     1139,\n",
              "     770,\n",
              "     1135,\n",
              "     777,\n",
              "     1135,\n",
              "     784,\n",
              "     1135,\n",
              "     791,\n",
              "     1135,\n",
              "     798,\n",
              "     1135,\n",
              "     805,\n",
              "     1135,\n",
              "     812,\n",
              "     1134,\n",
              "     819,\n",
              "     1134,\n",
              "     826,\n",
              "     1134,\n",
              "     833,\n",
              "     1134,\n",
              "     840,\n",
              "     1134,\n",
              "     847,\n",
              "     1134,\n",
              "     854,\n",
              "     1134,\n",
              "     861,\n",
              "     1135,\n",
              "     868,\n",
              "     1142,\n",
              "     873,\n",
              "     1149,\n",
              "     874,\n",
              "     1156,\n",
              "     875,\n",
              "     1163,\n",
              "     876,\n",
              "     1170,\n",
              "     877,\n",
              "     1177,\n",
              "     877,\n",
              "     1184,\n",
              "     878,\n",
              "     1191,\n",
              "     878,\n",
              "     1198,\n",
              "     878,\n",
              "     1205,\n",
              "     879,\n",
              "     1212,\n",
              "     879,\n",
              "     1219,\n",
              "     880,\n",
              "     1226,\n",
              "     881,\n",
              "     1233,\n",
              "     881,\n",
              "     1240,\n",
              "     881,\n",
              "     1247,\n",
              "     882,\n",
              "     1254,\n",
              "     882,\n",
              "     1261,\n",
              "     882,\n",
              "     1268,\n",
              "     881,\n",
              "     1275,\n",
              "     881,\n",
              "     1282,\n",
              "     882,\n",
              "     1289,\n",
              "     882,\n",
              "     1296,\n",
              "     882,\n",
              "     1303,\n",
              "     881,\n",
              "     1310,\n",
              "     877,\n",
              "     1315,\n",
              "     870,\n",
              "     1316,\n",
              "     863,\n",
              "     1317,\n",
              "     856,\n",
              "     1318,\n",
              "     849,\n",
              "     1319,\n",
              "     842,\n",
              "     1321,\n",
              "     835,\n",
              "     1322,\n",
              "     828,\n",
              "     1322,\n",
              "     821,\n",
              "     1322,\n",
              "     814,\n",
              "     1322,\n",
              "     807,\n",
              "     1322,\n",
              "     800,\n",
              "     1322,\n",
              "     793,\n",
              "     1322,\n",
              "     786,\n",
              "     1320,\n",
              "     779,\n",
              "     1313,\n",
              "     773,\n",
              "     1306,\n",
              "     771,\n",
              "     1299,\n",
              "     770,\n",
              "     1292,\n",
              "     769,\n",
              "     1285,\n",
              "     770,\n",
              "     1278,\n",
              "     770,\n",
              "     1271,\n",
              "     769,\n",
              "     1264,\n",
              "     769,\n",
              "     1257,\n",
              "     769,\n",
              "     1250,\n",
              "     769,\n",
              "     1243,\n",
              "     768,\n",
              "     1236,\n",
              "     768,\n",
              "     1229,\n",
              "     767,\n",
              "     1222,\n",
              "     767,\n",
              "     1215,\n",
              "     766,\n",
              "     1208,\n",
              "     766,\n",
              "     1201,\n",
              "     766,\n",
              "     1194,\n",
              "     765,\n",
              "     1187,\n",
              "     765,\n",
              "     1180,\n",
              "     765,\n",
              "     1173,\n",
              "     765]],\n",
              "   [[1455,\n",
              "     1086,\n",
              "     1445,\n",
              "     1089,\n",
              "     1435,\n",
              "     1090,\n",
              "     1425,\n",
              "     1090,\n",
              "     1415,\n",
              "     1091,\n",
              "     1405,\n",
              "     1092,\n",
              "     1395,\n",
              "     1097,\n",
              "     1385,\n",
              "     1101,\n",
              "     1375,\n",
              "     1102,\n",
              "     1365,\n",
              "     1106,\n",
              "     1360,\n",
              "     1116,\n",
              "     1361,\n",
              "     1126,\n",
              "     1366,\n",
              "     1136,\n",
              "     1370,\n",
              "     1146,\n",
              "     1371,\n",
              "     1156,\n",
              "     1365,\n",
              "     1166,\n",
              "     1359,\n",
              "     1176,\n",
              "     1359,\n",
              "     1186,\n",
              "     1360,\n",
              "     1196,\n",
              "     1363,\n",
              "     1206,\n",
              "     1366,\n",
              "     1216,\n",
              "     1366,\n",
              "     1226,\n",
              "     1368,\n",
              "     1236,\n",
              "     1371,\n",
              "     1246,\n",
              "     1373,\n",
              "     1256,\n",
              "     1374,\n",
              "     1266,\n",
              "     1378,\n",
              "     1276,\n",
              "     1380,\n",
              "     1286,\n",
              "     1381,\n",
              "     1296,\n",
              "     1385,\n",
              "     1306,\n",
              "     1387,\n",
              "     1316,\n",
              "     1388,\n",
              "     1326,\n",
              "     1392,\n",
              "     1336,\n",
              "     1394,\n",
              "     1346,\n",
              "     1395,\n",
              "     1356,\n",
              "     1400,\n",
              "     1366,\n",
              "     1403,\n",
              "     1376,\n",
              "     1411,\n",
              "     1383,\n",
              "     1420,\n",
              "     1391,\n",
              "     1430,\n",
              "     1393,\n",
              "     1440,\n",
              "     1394,\n",
              "     1450,\n",
              "     1394,\n",
              "     1460,\n",
              "     1394,\n",
              "     1470,\n",
              "     1393,\n",
              "     1480,\n",
              "     1392,\n",
              "     1490,\n",
              "     1386,\n",
              "     1500,\n",
              "     1383,\n",
              "     1510,\n",
              "     1381,\n",
              "     1520,\n",
              "     1374,\n",
              "     1530,\n",
              "     1371,\n",
              "     1538,\n",
              "     1363,\n",
              "     1544,\n",
              "     1353,\n",
              "     1548,\n",
              "     1343,\n",
              "     1548,\n",
              "     1333,\n",
              "     1544,\n",
              "     1323,\n",
              "     1542,\n",
              "     1313,\n",
              "     1539,\n",
              "     1303,\n",
              "     1536,\n",
              "     1293,\n",
              "     1534,\n",
              "     1283,\n",
              "     1529,\n",
              "     1273,\n",
              "     1527,\n",
              "     1263,\n",
              "     1524,\n",
              "     1253,\n",
              "     1521,\n",
              "     1243,\n",
              "     1520,\n",
              "     1233,\n",
              "     1519,\n",
              "     1223,\n",
              "     1517,\n",
              "     1213,\n",
              "     1515,\n",
              "     1203,\n",
              "     1513,\n",
              "     1193,\n",
              "     1512,\n",
              "     1183,\n",
              "     1510,\n",
              "     1173,\n",
              "     1508,\n",
              "     1163,\n",
              "     1506,\n",
              "     1153,\n",
              "     1499,\n",
              "     1145,\n",
              "     1491,\n",
              "     1137,\n",
              "     1482,\n",
              "     1129,\n",
              "     1479,\n",
              "     1119,\n",
              "     1478,\n",
              "     1109,\n",
              "     1476,\n",
              "     1099,\n",
              "     1469,\n",
              "     1089,\n",
              "     1459,\n",
              "     1087]],\n",
              "   [[856,\n",
              "     1044,\n",
              "     849,\n",
              "     1046,\n",
              "     842,\n",
              "     1049,\n",
              "     835,\n",
              "     1051,\n",
              "     828,\n",
              "     1051,\n",
              "     821,\n",
              "     1052,\n",
              "     814,\n",
              "     1054,\n",
              "     807,\n",
              "     1059,\n",
              "     800,\n",
              "     1059,\n",
              "     793,\n",
              "     1062,\n",
              "     786,\n",
              "     1067,\n",
              "     779,\n",
              "     1071,\n",
              "     774,\n",
              "     1077,\n",
              "     769,\n",
              "     1084,\n",
              "     768,\n",
              "     1091,\n",
              "     768,\n",
              "     1098,\n",
              "     769,\n",
              "     1105,\n",
              "     772,\n",
              "     1112,\n",
              "     773,\n",
              "     1119,\n",
              "     774,\n",
              "     1126,\n",
              "     778,\n",
              "     1133,\n",
              "     779,\n",
              "     1140,\n",
              "     782,\n",
              "     1147,\n",
              "     784,\n",
              "     1154,\n",
              "     785,\n",
              "     1161,\n",
              "     786,\n",
              "     1168,\n",
              "     789,\n",
              "     1175,\n",
              "     790,\n",
              "     1182,\n",
              "     791,\n",
              "     1189,\n",
              "     795,\n",
              "     1196,\n",
              "     797,\n",
              "     1203,\n",
              "     802,\n",
              "     1209,\n",
              "     808,\n",
              "     1216,\n",
              "     812,\n",
              "     1223,\n",
              "     814,\n",
              "     1230,\n",
              "     814,\n",
              "     1237,\n",
              "     815,\n",
              "     1244,\n",
              "     818,\n",
              "     1251,\n",
              "     823,\n",
              "     1257,\n",
              "     830,\n",
              "     1260,\n",
              "     837,\n",
              "     1264,\n",
              "     844,\n",
              "     1265,\n",
              "     851,\n",
              "     1265,\n",
              "     858,\n",
              "     1265,\n",
              "     865,\n",
              "     1263,\n",
              "     872,\n",
              "     1260,\n",
              "     879,\n",
              "     1258,\n",
              "     886,\n",
              "     1257,\n",
              "     893,\n",
              "     1254,\n",
              "     900,\n",
              "     1250,\n",
              "     907,\n",
              "     1249,\n",
              "     913,\n",
              "     1244,\n",
              "     920,\n",
              "     1239,\n",
              "     924,\n",
              "     1233,\n",
              "     925,\n",
              "     1226,\n",
              "     925,\n",
              "     1219,\n",
              "     924,\n",
              "     1212,\n",
              "     921,\n",
              "     1205,\n",
              "     918,\n",
              "     1198,\n",
              "     918,\n",
              "     1191,\n",
              "     920,\n",
              "     1184,\n",
              "     923,\n",
              "     1177,\n",
              "     924,\n",
              "     1170,\n",
              "     924,\n",
              "     1163,\n",
              "     923,\n",
              "     1156,\n",
              "     921,\n",
              "     1149,\n",
              "     919,\n",
              "     1142,\n",
              "     918,\n",
              "     1135,\n",
              "     914,\n",
              "     1128,\n",
              "     913,\n",
              "     1121,\n",
              "     912,\n",
              "     1114,\n",
              "     910,\n",
              "     1107,\n",
              "     908,\n",
              "     1100,\n",
              "     907,\n",
              "     1093,\n",
              "     906,\n",
              "     1086,\n",
              "     904,\n",
              "     1079,\n",
              "     902,\n",
              "     1072,\n",
              "     901,\n",
              "     1065,\n",
              "     896,\n",
              "     1058,\n",
              "     890,\n",
              "     1052,\n",
              "     883,\n",
              "     1051,\n",
              "     876,\n",
              "     1048,\n",
              "     869,\n",
              "     1045,\n",
              "     862,\n",
              "     1044]]]})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC9egw-TGXIy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}