{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPSfVpsOuw8A9wLKB7Z4DgD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d7071cc21f244942906c846230cbd496": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5db32722521344868969ab3d2cf9ca82",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cd6159f4e17a4bc7bb535990e9df859c",
              "IPY_MODEL_280d57f90313410085f381ee94fcafa8",
              "IPY_MODEL_6864000625404f408eb2a63ae4ca4081"
            ]
          }
        },
        "5db32722521344868969ab3d2cf9ca82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd6159f4e17a4bc7bb535990e9df859c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c4e4ea86814749ada1f7c525593c7127",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a8d0112f979e4e8bbf5ae41198fd3284"
          }
        },
        "280d57f90313410085f381ee94fcafa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_72aebdc2dd4944e1884a8e7274a8eaaa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 14212972,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 14212972,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1356962ed9db411dbe85bdec9d12b75e"
          }
        },
        "6864000625404f408eb2a63ae4ca4081": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c4850ae11c664b729377de63a48ccfd4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13.6M/13.6M [00:00&lt;00:00, 49.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_28e7c8a3d7d647bb8efd231175aa3202"
          }
        },
        "c4e4ea86814749ada1f7c525593c7127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a8d0112f979e4e8bbf5ae41198fd3284": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72aebdc2dd4944e1884a8e7274a8eaaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1356962ed9db411dbe85bdec9d12b75e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c4850ae11c664b729377de63a48ccfd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "28e7c8a3d7d647bb8efd231175aa3202": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyyjXDjeMY3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57006941-9e0c-473b-8fb9-6d53b57af238"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "from os.path import join as jn\n",
        "import shutil\n",
        "\n",
        "# 구글 드라이브 접근\n",
        "ROOT = \"/content/drive\"\n",
        "try:\n",
        "  drive.mount(ROOT, force_remount=True)\n",
        "except:\n",
        "  drive.mount(ROOT)\n",
        "\n",
        "# 본인 경로에 맞게 수정하면 됨.\n",
        "PATH = jn(ROOT, \"MyDrive/Task/plastic-segmentation/Data\")\n",
        "# PATH = jn(ROOT, \"MyDrive/Task/plastic-segmentation/Sample_data\")\n",
        "\n",
        "# 구글 드라이브 경로에서 '/content/sample_data/data\"로 복사 시\n",
        "# shutil.copytree(PATH +\"/train\", \"./sample_data/data/train\")\n",
        "# shutil.copytree(PATH +\"/test\", \"./sample_data/data/test\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmU1FEV5Pb9W"
      },
      "source": [
        "# init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7DuTJMEKYWI"
      },
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "\n",
        "import numpy as np\n",
        "# import pandas as pd\n",
        "import cv2 as cv\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch  import nn, Tensor\n",
        "\n",
        "BASE = os.getcwd()\n",
        "PATH = PATH # gdrive\n",
        "# PATH = f\"{BASE}/sample_data/data\"   # colab\n",
        "# PATH = f\"{BASE}/assets/data\"        # github"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U27Zh1SIRqaL",
        "outputId": "628949d1-2447-44bb-dc74-53d57143bb77"
      },
      "source": [
        "# version 1 : @qkrwjdduf159\n",
        "\n",
        "## train\n",
        "trn_path = f\"{PATH}/train/annotation\"\n",
        "file_list = os.listdir(trn_path)\n",
        "\n",
        "train_files = []\n",
        "for file in tqdm(file_list):\n",
        "    dir = trn_path + '/' + file\n",
        "    json_list = glob(dir + '/' + '*.json')\n",
        "    train_files.append(json_list)\n",
        "\n",
        "train_json_list = []\n",
        "for files in tqdm(train_files):\n",
        "    for json_file in tqdm(files):\n",
        "        train_json_list.append(json_file)\n",
        "\n",
        "## test\n",
        "tst_PATH = f\"{PATH}/test/annotations\"\n",
        "file_list = os.listdir(tst_PATH)\n",
        "\n",
        "test_files = []\n",
        "for file in tqdm(file_list):\n",
        "    dir = tst_PATH + '/' + file\n",
        "    json_list = glob(dir + '/' + '*.json')\n",
        "    test_files.append(json_list)\n",
        "\n",
        "test_json_list = []\n",
        "for files in tqdm(test_files):\n",
        "    for json_file in tqdm(files):\n",
        "        test_json_list.append(json_file)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 85.26it/s]\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 1985939.39it/s]\n",
            "\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 1808669.25it/s]\n",
            "\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 1076290.48it/s]\n",
            "\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 1889326.13it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00, 164.44it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00, 382.56it/s]\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\n",
            "100%|██████████| 100/100 [00:00<00:00, 594094.05it/s]\n",
            "\n",
            "100%|██████████| 100/100 [00:00<00:00, 884874.26it/s]\n",
            "\n",
            "100%|██████████| 100/100 [00:00<00:00, 651289.44it/s]\n",
            "\n",
            "100%|██████████| 100/100 [00:00<00:00, 701388.63it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00, 189.45it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAwIck4sT5G4",
        "outputId": "22a19e95-9527-4734-cc60-2caa3590bf5b"
      },
      "source": [
        "# version 2 @AshbeeKim\n",
        "trn = os.path.join(PATH, \"train\")\n",
        "tst = os.path.join(PATH, \"test\")\n",
        "\n",
        "baseDF = {\"kind\" : [], \"label\" : [], \"metainfo_id\" : [], \"feature\" : [], \"image_path\" : [], \"annot_path\": []}\n",
        "trn_num, tst_num = 0, 0\n",
        "for fpath in tqdm([trn, tst]):\n",
        "    kind = os.path.basename(fpath)\n",
        "    BDIR = sorted(os.listdir(fpath))    # image, annotation(s)\n",
        "    for bdir in BDIR:\n",
        "        for dirs in sorted(os.listdir(jn(fpath, bdir))):\n",
        "            dpath = jn(fpath, bdir, dirs)\n",
        "            if bdir.lower()=='image':\n",
        "                paths = sorted(glob(dpath + \"/*.jpg\"))\n",
        "                baseDF['image_path'].extend(paths)\n",
        "                kinds = [kind for cnt in range(len(paths))]\n",
        "                baseDF['kind'].extend(kinds)\n",
        "                if kind=='train' : trn_num += len(paths)\n",
        "                elif kind=='test' : tst_num += len(paths)\n",
        "                fnames = [os.path.basename(fname) for fname in paths]\n",
        "                labels = list(map(lambda x: (x.split('_')[0]), fnames))\n",
        "                baseDF['label'].extend(labels)\n",
        "                metaIds = list(map(lambda x: int(x.split('_')[1]), fnames))\n",
        "                baseDF['metainfo_id'].extend(metaIds)\n",
        "                feats = list(map(lambda x: int(x.split('_')[-1][:-4]), fnames))\n",
        "                baseDF['feature'].extend(feats)\n",
        "            else:\n",
        "                paths = sorted(glob(dpath + \"/*.json\"))\n",
        "                baseDF['annot_path'].extend(paths)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:15<00:00,  7.60s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV0fT9zqWbFi"
      },
      "source": [
        "# cotributed by @qkrwjdduf156, @AshbeeKim\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# 추가 수정 중\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_info, transforms = None, mode = 'train', train_size = None, test_size = None):\n",
        "        self.mode = mode\n",
        "        self.infos = base_info\n",
        "        self.transforms = transforms\n",
        "        self.train_size = train_size\n",
        "        self.test_size = test_size\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode == 'train':\n",
        "            json_path = (self.infos['annot_path'])[:self.train_size][idx]\n",
        "\n",
        "            with open(json_path, 'r') as f:\n",
        "                annot = json.loads(f.read())\n",
        "\n",
        "            image_path = (self.infos['image_path'])[:self.train_size][idx]\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "            boxes = []\n",
        "            segmentations = []\n",
        "            labels = []\n",
        "            for i in range(len(annot['annotations'])):\n",
        "\n",
        "                segmentation = annot['annotations'][i]['segmentation'][0]\n",
        "                bbox = annot['annotations'][i]['bbox']\n",
        "                label = torch.as_tensor(annot['annotations'][i]['category_id'], dtype = torch.int64)\n",
        "                xmin, ymin, width, height = bbox[0],bbox[1],bbox[2],bbox[3]\n",
        "                xmin, ymin, xmax, ymax = xmin, ymin, xmin + width, ymin + height\n",
        "                \n",
        "                boxes.append([xmin, ymin, xmax, ymax])\n",
        "                segmentations.append([segmentation])\n",
        "                labels.append(label)\n",
        "\n",
        "            target = {}\n",
        "            target['boxes'] = boxes\n",
        "            target['labels'] = labels\n",
        "            target['segmentation'] = segmentations\n",
        "            target['image_id'] = image_path.split('/')[-1]\n",
        "\n",
        "            if self.transforms is not None:\n",
        "                image, target = self.transforms(image, target)\n",
        "\n",
        "            return image, target\n",
        "\n",
        "        elif self.model == 'test':\n",
        "            st_num = self.train_size\n",
        "            ed_num = self.train_size + self.test_size\n",
        "\n",
        "            json_path = ((self.infos['annot_path'])[st_num:ed_num])[idx]\n",
        "            with open(json_path, 'r') as f:\n",
        "                annot = json.loads(f.read())\n",
        "\n",
        "            image_path = ((self.infos['image_path'])[st_num:ed_num])[idx]\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            \n",
        "            target = {}\n",
        "            target['image_id'] = image_path.split('/')[-1]\n",
        "            return image, target\n",
        "        \n",
        "    def __len__(self):\n",
        "        if self.mode == 'train':\n",
        "            return self.train_size\n",
        "        elif self.mode == 'test':\n",
        "            return self.test_size"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3Ncq75MYFD3"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "train_dataset = CustomDataset(baseDF, mode = 'train', train_size=trn_num, test_size=tst_num)\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True)\n",
        "\n",
        "num_classes = 5 # 4 class + background\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "d7071cc21f244942906c846230cbd496",
            "5db32722521344868969ab3d2cf9ca82",
            "cd6159f4e17a4bc7bb535990e9df859c",
            "280d57f90313410085f381ee94fcafa8",
            "6864000625404f408eb2a63ae4ca4081",
            "c4e4ea86814749ada1f7c525593c7127",
            "a8d0112f979e4e8bbf5ae41198fd3284",
            "72aebdc2dd4944e1884a8e7274a8eaaa",
            "1356962ed9db411dbe85bdec9d12b75e",
            "c4850ae11c664b729377de63a48ccfd4",
            "28e7c8a3d7d647bb8efd231175aa3202"
          ]
        },
        "id": "KXUlDmt9KfSF",
        "outputId": "57fedcd4-eb1d-4576-9e06-cc6734e07411"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "backbone.out_channels = 1280\n",
        "\n",
        "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "                                                output_size=7,\n",
        "                                                sampling_ratio=2)\n",
        "\n",
        "model = FasterRCNN(backbone,\n",
        "                   num_classes=5,\n",
        "                   rpn_anchor_generator=anchor_generator,\n",
        "                   box_roi_pool=roi_pooler)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7071cc21f244942906c846230cbd496",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2OEzmOxKvOc"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "\n",
        "def get_model_instance_segmentation(num_classes):\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjnVJRjh8ttS"
      },
      "source": [
        "*** 현재 수정 중 ***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "CYtUes5nLC_N",
        "outputId": "8ca233e0-4b06-4a17-d012-f9eb6375250e"
      },
      "source": [
        "from torchvision import utils\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "dataset = CustomDataset(baseDF, mode = 'train', train_size=trn_num, test_size=tst_num)\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        " dataset, batch_size=5, shuffle=True, num_workers=4,\n",
        " collate_fn=utils.collate_fn)\n",
        "# For Training\n",
        "images,targets = next(iter(data_loader))\n",
        "images = list(image for image in images)\n",
        "targets = [{k: v for k, v in t.items()} for t in targets]\n",
        "output = model(images,targets)   # Returns losses and detections\n",
        "# For inference\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)           # Returns predictions"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-fda0083e3480>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m data_loader = torch.utils.data.DataLoader(\n\u001b[1;32m      5\u001b[0m  \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m  collate_fn=utils.collate_fn)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# For Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torchvision.utils' has no attribute 'collate_fn'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZthpwS8JaJQ"
      },
      "source": [
        "from torchvision import transforms as T\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "\n",
        "# transform = T.Compose([\n",
        "#         T.Resize(256),\n",
        "#         T.RandomCrop(224),\n",
        "#         T.RandomHorizontalFlip(),\n",
        "#         T.ToTensor(),\n",
        "#         T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) "
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "T7Vhlh_1DerZ",
        "outputId": "5fc67253-1bc9-4010-f0c6-5d044fcd485c"
      },
      "source": [
        "import torch\n",
        "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
        "    in_channels=3, out_channels=1, init_features=32, pretrained=True)\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "# print(type(train_dataset))\n",
        "input_image = train_dataset.__getitem__(23)[0]\n",
        "\n",
        "input_image = np.squeeze(input_image.numpy())\n",
        "# print(type(input_image[0]))\n",
        "m, s = np.mean(input_image[0], axis=(0, 1)), np.std(input_image[0], axis=(0, 1))\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=m, std=s),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model = model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)\n",
        "\n",
        "print(torch.round(output[0]))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-0a5ed04a6300>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0minput_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0minput_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# print(type(input_image[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Image' object has no attribute 'numpy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMBWmkAAEIU-",
        "outputId": "00ae5179-c451-4482-9d4d-a2095f89355a"
      },
      "source": [
        "input_image"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PIL.Image.Image image mode=RGB size=2048x2048 at 0x7F003D7937D0>,\n",
              " {'boxes': [[1134, 765, 1322, 882],\n",
              "   [1359, 1086, 1548, 1394],\n",
              "   [768, 1044, 925, 1265]],\n",
              "  'image_id': 'PE_074_121.jpg',\n",
              "  'labels': [tensor(4), tensor(4), tensor(4)],\n",
              "  'segmentation': [[[1167,\n",
              "     765,\n",
              "     1160,\n",
              "     766,\n",
              "     1153,\n",
              "     766,\n",
              "     1146,\n",
              "     767,\n",
              "     1139,\n",
              "     770,\n",
              "     1135,\n",
              "     777,\n",
              "     1135,\n",
              "     784,\n",
              "     1135,\n",
              "     791,\n",
              "     1135,\n",
              "     798,\n",
              "     1135,\n",
              "     805,\n",
              "     1135,\n",
              "     812,\n",
              "     1134,\n",
              "     819,\n",
              "     1134,\n",
              "     826,\n",
              "     1134,\n",
              "     833,\n",
              "     1134,\n",
              "     840,\n",
              "     1134,\n",
              "     847,\n",
              "     1134,\n",
              "     854,\n",
              "     1134,\n",
              "     861,\n",
              "     1135,\n",
              "     868,\n",
              "     1142,\n",
              "     873,\n",
              "     1149,\n",
              "     874,\n",
              "     1156,\n",
              "     875,\n",
              "     1163,\n",
              "     876,\n",
              "     1170,\n",
              "     877,\n",
              "     1177,\n",
              "     877,\n",
              "     1184,\n",
              "     878,\n",
              "     1191,\n",
              "     878,\n",
              "     1198,\n",
              "     878,\n",
              "     1205,\n",
              "     879,\n",
              "     1212,\n",
              "     879,\n",
              "     1219,\n",
              "     880,\n",
              "     1226,\n",
              "     881,\n",
              "     1233,\n",
              "     881,\n",
              "     1240,\n",
              "     881,\n",
              "     1247,\n",
              "     882,\n",
              "     1254,\n",
              "     882,\n",
              "     1261,\n",
              "     882,\n",
              "     1268,\n",
              "     881,\n",
              "     1275,\n",
              "     881,\n",
              "     1282,\n",
              "     882,\n",
              "     1289,\n",
              "     882,\n",
              "     1296,\n",
              "     882,\n",
              "     1303,\n",
              "     881,\n",
              "     1310,\n",
              "     877,\n",
              "     1315,\n",
              "     870,\n",
              "     1316,\n",
              "     863,\n",
              "     1317,\n",
              "     856,\n",
              "     1318,\n",
              "     849,\n",
              "     1319,\n",
              "     842,\n",
              "     1321,\n",
              "     835,\n",
              "     1322,\n",
              "     828,\n",
              "     1322,\n",
              "     821,\n",
              "     1322,\n",
              "     814,\n",
              "     1322,\n",
              "     807,\n",
              "     1322,\n",
              "     800,\n",
              "     1322,\n",
              "     793,\n",
              "     1322,\n",
              "     786,\n",
              "     1320,\n",
              "     779,\n",
              "     1313,\n",
              "     773,\n",
              "     1306,\n",
              "     771,\n",
              "     1299,\n",
              "     770,\n",
              "     1292,\n",
              "     769,\n",
              "     1285,\n",
              "     770,\n",
              "     1278,\n",
              "     770,\n",
              "     1271,\n",
              "     769,\n",
              "     1264,\n",
              "     769,\n",
              "     1257,\n",
              "     769,\n",
              "     1250,\n",
              "     769,\n",
              "     1243,\n",
              "     768,\n",
              "     1236,\n",
              "     768,\n",
              "     1229,\n",
              "     767,\n",
              "     1222,\n",
              "     767,\n",
              "     1215,\n",
              "     766,\n",
              "     1208,\n",
              "     766,\n",
              "     1201,\n",
              "     766,\n",
              "     1194,\n",
              "     765,\n",
              "     1187,\n",
              "     765,\n",
              "     1180,\n",
              "     765,\n",
              "     1173,\n",
              "     765]],\n",
              "   [[1455,\n",
              "     1086,\n",
              "     1445,\n",
              "     1089,\n",
              "     1435,\n",
              "     1090,\n",
              "     1425,\n",
              "     1090,\n",
              "     1415,\n",
              "     1091,\n",
              "     1405,\n",
              "     1092,\n",
              "     1395,\n",
              "     1097,\n",
              "     1385,\n",
              "     1101,\n",
              "     1375,\n",
              "     1102,\n",
              "     1365,\n",
              "     1106,\n",
              "     1360,\n",
              "     1116,\n",
              "     1361,\n",
              "     1126,\n",
              "     1366,\n",
              "     1136,\n",
              "     1370,\n",
              "     1146,\n",
              "     1371,\n",
              "     1156,\n",
              "     1365,\n",
              "     1166,\n",
              "     1359,\n",
              "     1176,\n",
              "     1359,\n",
              "     1186,\n",
              "     1360,\n",
              "     1196,\n",
              "     1363,\n",
              "     1206,\n",
              "     1366,\n",
              "     1216,\n",
              "     1366,\n",
              "     1226,\n",
              "     1368,\n",
              "     1236,\n",
              "     1371,\n",
              "     1246,\n",
              "     1373,\n",
              "     1256,\n",
              "     1374,\n",
              "     1266,\n",
              "     1378,\n",
              "     1276,\n",
              "     1380,\n",
              "     1286,\n",
              "     1381,\n",
              "     1296,\n",
              "     1385,\n",
              "     1306,\n",
              "     1387,\n",
              "     1316,\n",
              "     1388,\n",
              "     1326,\n",
              "     1392,\n",
              "     1336,\n",
              "     1394,\n",
              "     1346,\n",
              "     1395,\n",
              "     1356,\n",
              "     1400,\n",
              "     1366,\n",
              "     1403,\n",
              "     1376,\n",
              "     1411,\n",
              "     1383,\n",
              "     1420,\n",
              "     1391,\n",
              "     1430,\n",
              "     1393,\n",
              "     1440,\n",
              "     1394,\n",
              "     1450,\n",
              "     1394,\n",
              "     1460,\n",
              "     1394,\n",
              "     1470,\n",
              "     1393,\n",
              "     1480,\n",
              "     1392,\n",
              "     1490,\n",
              "     1386,\n",
              "     1500,\n",
              "     1383,\n",
              "     1510,\n",
              "     1381,\n",
              "     1520,\n",
              "     1374,\n",
              "     1530,\n",
              "     1371,\n",
              "     1538,\n",
              "     1363,\n",
              "     1544,\n",
              "     1353,\n",
              "     1548,\n",
              "     1343,\n",
              "     1548,\n",
              "     1333,\n",
              "     1544,\n",
              "     1323,\n",
              "     1542,\n",
              "     1313,\n",
              "     1539,\n",
              "     1303,\n",
              "     1536,\n",
              "     1293,\n",
              "     1534,\n",
              "     1283,\n",
              "     1529,\n",
              "     1273,\n",
              "     1527,\n",
              "     1263,\n",
              "     1524,\n",
              "     1253,\n",
              "     1521,\n",
              "     1243,\n",
              "     1520,\n",
              "     1233,\n",
              "     1519,\n",
              "     1223,\n",
              "     1517,\n",
              "     1213,\n",
              "     1515,\n",
              "     1203,\n",
              "     1513,\n",
              "     1193,\n",
              "     1512,\n",
              "     1183,\n",
              "     1510,\n",
              "     1173,\n",
              "     1508,\n",
              "     1163,\n",
              "     1506,\n",
              "     1153,\n",
              "     1499,\n",
              "     1145,\n",
              "     1491,\n",
              "     1137,\n",
              "     1482,\n",
              "     1129,\n",
              "     1479,\n",
              "     1119,\n",
              "     1478,\n",
              "     1109,\n",
              "     1476,\n",
              "     1099,\n",
              "     1469,\n",
              "     1089,\n",
              "     1459,\n",
              "     1087]],\n",
              "   [[856,\n",
              "     1044,\n",
              "     849,\n",
              "     1046,\n",
              "     842,\n",
              "     1049,\n",
              "     835,\n",
              "     1051,\n",
              "     828,\n",
              "     1051,\n",
              "     821,\n",
              "     1052,\n",
              "     814,\n",
              "     1054,\n",
              "     807,\n",
              "     1059,\n",
              "     800,\n",
              "     1059,\n",
              "     793,\n",
              "     1062,\n",
              "     786,\n",
              "     1067,\n",
              "     779,\n",
              "     1071,\n",
              "     774,\n",
              "     1077,\n",
              "     769,\n",
              "     1084,\n",
              "     768,\n",
              "     1091,\n",
              "     768,\n",
              "     1098,\n",
              "     769,\n",
              "     1105,\n",
              "     772,\n",
              "     1112,\n",
              "     773,\n",
              "     1119,\n",
              "     774,\n",
              "     1126,\n",
              "     778,\n",
              "     1133,\n",
              "     779,\n",
              "     1140,\n",
              "     782,\n",
              "     1147,\n",
              "     784,\n",
              "     1154,\n",
              "     785,\n",
              "     1161,\n",
              "     786,\n",
              "     1168,\n",
              "     789,\n",
              "     1175,\n",
              "     790,\n",
              "     1182,\n",
              "     791,\n",
              "     1189,\n",
              "     795,\n",
              "     1196,\n",
              "     797,\n",
              "     1203,\n",
              "     802,\n",
              "     1209,\n",
              "     808,\n",
              "     1216,\n",
              "     812,\n",
              "     1223,\n",
              "     814,\n",
              "     1230,\n",
              "     814,\n",
              "     1237,\n",
              "     815,\n",
              "     1244,\n",
              "     818,\n",
              "     1251,\n",
              "     823,\n",
              "     1257,\n",
              "     830,\n",
              "     1260,\n",
              "     837,\n",
              "     1264,\n",
              "     844,\n",
              "     1265,\n",
              "     851,\n",
              "     1265,\n",
              "     858,\n",
              "     1265,\n",
              "     865,\n",
              "     1263,\n",
              "     872,\n",
              "     1260,\n",
              "     879,\n",
              "     1258,\n",
              "     886,\n",
              "     1257,\n",
              "     893,\n",
              "     1254,\n",
              "     900,\n",
              "     1250,\n",
              "     907,\n",
              "     1249,\n",
              "     913,\n",
              "     1244,\n",
              "     920,\n",
              "     1239,\n",
              "     924,\n",
              "     1233,\n",
              "     925,\n",
              "     1226,\n",
              "     925,\n",
              "     1219,\n",
              "     924,\n",
              "     1212,\n",
              "     921,\n",
              "     1205,\n",
              "     918,\n",
              "     1198,\n",
              "     918,\n",
              "     1191,\n",
              "     920,\n",
              "     1184,\n",
              "     923,\n",
              "     1177,\n",
              "     924,\n",
              "     1170,\n",
              "     924,\n",
              "     1163,\n",
              "     923,\n",
              "     1156,\n",
              "     921,\n",
              "     1149,\n",
              "     919,\n",
              "     1142,\n",
              "     918,\n",
              "     1135,\n",
              "     914,\n",
              "     1128,\n",
              "     913,\n",
              "     1121,\n",
              "     912,\n",
              "     1114,\n",
              "     910,\n",
              "     1107,\n",
              "     908,\n",
              "     1100,\n",
              "     907,\n",
              "     1093,\n",
              "     906,\n",
              "     1086,\n",
              "     904,\n",
              "     1079,\n",
              "     902,\n",
              "     1072,\n",
              "     901,\n",
              "     1065,\n",
              "     896,\n",
              "     1058,\n",
              "     890,\n",
              "     1052,\n",
              "     883,\n",
              "     1051,\n",
              "     876,\n",
              "     1048,\n",
              "     869,\n",
              "     1045,\n",
              "     862,\n",
              "     1044]]]})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC9egw-TGXIy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}